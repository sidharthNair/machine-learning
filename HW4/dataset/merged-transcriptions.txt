WEBVTT

1
00:05:40.410 --> 00:05:42.240
Inderjit Dhillon: Hey? Good afternoon.

2
00:06:02.520 --> 00:06:04.500
Inderjit Dhillon: Just setting my ipad up

3
00:06:45.490 --> 00:06:47.000
Inderjit Dhillon: testing, testing.

4
00:06:47.810 --> 00:06:48.510
Oh.

5
00:06:51.180 --> 00:06:52.310
Inderjit Dhillon: well.

6
00:06:53.310 --> 00:06:54.790
Inderjit Dhillon: testing, testing.

7
00:07:32.310 --> 00:07:33.880
Inderjit Dhillon: Okay.

8
00:07:35.210 --> 00:07:38.060
Inderjit Dhillon: just confirming. If you guys can all hear me

9
00:07:39.620 --> 00:07:42.039
Inderjit Dhillon: if you yeah. Okay. Good.

10
00:07:43.150 --> 00:07:45.560
Inderjit Dhillon: There's just a little echo that i'm getting.

11
00:07:48.700 --> 00:07:50.829
Inderjit Dhillon: Are you guys getting an echo or you not?

12
00:07:53.140 --> 00:07:54.679
I don't hear any echo.

13
00:07:57.270 --> 00:07:59.399
Inderjit Dhillon: Okay, let me just go to zoom and

14
00:08:25.550 --> 00:08:27.010
Inderjit Dhillon: just in testing.

15
00:08:28.650 --> 00:08:30.189
Inderjit Dhillon: Okay.

16
00:08:39.080 --> 00:08:41.690
Inderjit Dhillon: Okay. So welcome to the first

17
00:08:41.799 --> 00:08:43.190
Inderjit Dhillon: real lecture

18
00:08:45.160 --> 00:08:48.710
Inderjit Dhillon: today, i'll talk about linear regression.

19
00:08:48.800 --> 00:08:50.490
Inderjit Dhillon: So this is

20
00:08:55.420 --> 00:08:56.850
Inderjit Dhillon: like to one.

21
00:09:04.840 --> 00:09:06.330
Inderjit Dhillon: Well, let me be a regression.

22
00:09:06.630 --> 00:09:08.700
Inderjit Dhillon: Okay. So let's look at

23
00:09:09.170 --> 00:09:10.650
Inderjit Dhillon: a particular example.

24
00:09:14.660 --> 00:09:16.780
Inderjit Dhillon: Suppose we make

25
00:09:17.070 --> 00:09:19.960
Inderjit Dhillon: various measurements on, let's say a prostate.

26
00:09:20.240 --> 00:09:23.730
Inderjit Dhillon: and we want to predict levels of

27
00:09:23.910 --> 00:09:29.459
Inderjit Dhillon: Psa right, which is some particular levels of

28
00:09:29.580 --> 00:09:31.140
Inderjit Dhillon: you see. So

29
00:09:31.510 --> 00:09:32.430
Inderjit Dhillon: predict

30
00:09:36.350 --> 00:09:37.870
Inderjit Dhillon: levels of.

31
00:09:40.570 --> 00:09:41.890
Inderjit Dhillon: you say.

32
00:09:41.970 --> 00:09:44.570
Inderjit Dhillon: which is prostate something antigen

33
00:09:44.910 --> 00:09:47.780
Inderjit Dhillon: right? So this is an example that's in the

34
00:09:48.020 --> 00:09:52.280
Inderjit Dhillon: He's the tip Shirani textbook that I've mentioned.

35
00:09:52.440 --> 00:09:56.959
Inderjit Dhillon: along with other books, on various measurements

36
00:10:00.840 --> 00:10:02.129
Inderjit Dhillon: on plus it.

37
00:10:06.240 --> 00:10:09.529
Inderjit Dhillon: So in this case, what we can do is we can formalize the problem

38
00:10:10.030 --> 00:10:12.389
Inderjit Dhillon: to say that we have a training data

39
00:10:12.960 --> 00:10:15.880
Inderjit Dhillon: which is in these pairs, which is

40
00:10:16.430 --> 00:10:20.390
Inderjit Dhillon: first measurement along with the level of the Psa.

41
00:10:21.400 --> 00:10:26.069
Inderjit Dhillon: Second measurement along with y 2, which is the Psa.

42
00:10:26.920 --> 00:10:31.889
Inderjit Dhillon: And let's say we have n total measurement. Xn is the measurement.

43
00:10:32.320 --> 00:10:37.190
Inderjit Dhillon: and Y. M is the corresponding level of psi.

44
00:10:37.330 --> 00:10:39.389
Inderjit Dhillon: So this will be

45
00:10:39.470 --> 00:10:40.900
Inderjit Dhillon: the training data.

46
00:10:43.680 --> 00:10:44.330
Hmm.

47
00:10:58.330 --> 00:11:01.789
Inderjit Dhillon: This is the truth. I need to get rid of the screens. Actually, right?

48
00:11:13.750 --> 00:11:14.430
Hmm.

49
00:11:18.380 --> 00:11:19.080
Inderjit Dhillon: Okay.

50
00:11:20.610 --> 00:11:24.490
Inderjit Dhillon: Sorry for some of the glitches in this first lecture. So by one

51
00:11:25.270 --> 00:11:26.790
Inderjit Dhillon: y, 2,

52
00:11:26.880 --> 00:11:28.900
Inderjit Dhillon: 2 Ym.

53
00:11:29.660 --> 00:11:31.889
Inderjit Dhillon: These are the node levels

54
00:11:33.340 --> 00:11:34.750
Inderjit Dhillon: of Psa.

55
00:11:35.110 --> 00:11:37.869
Inderjit Dhillon: and these levels are some real numbers.

56
00:11:38.730 --> 00:11:41.999
Inderjit Dhillon: So each of the Yi's is a real number.

57
00:11:42.500 --> 00:11:46.080
Inderjit Dhillon: And then, like I said, we have x, one X 2

58
00:11:46.210 --> 00:11:47.680
Inderjit Dhillon: through. Xm.

59
00:11:48.330 --> 00:11:49.700
Inderjit Dhillon: These are

60
00:11:49.810 --> 00:11:51.130
Inderjit Dhillon: measurements

61
00:11:55.700 --> 00:11:57.329
Inderjit Dhillon: right on the prospect.

62
00:12:01.860 --> 00:12:03.410
Inderjit Dhillon: And so these are

63
00:12:03.510 --> 00:12:05.340
Inderjit Dhillon: X. I typically

64
00:12:05.450 --> 00:12:06.570
Inderjit Dhillon: is.

65
00:12:06.880 --> 00:12:08.539
Inderjit Dhillon: do you measurements?

66
00:12:08.910 --> 00:12:12.790
Inderjit Dhillon: And so this is a d dimensional. Vector

67
00:12:13.930 --> 00:12:16.379
Inderjit Dhillon: Okay. So that's one example which is

68
00:12:16.510 --> 00:12:18.720
Inderjit Dhillon: an example of a regression problem.

69
00:12:19.930 --> 00:12:21.260
Inderjit Dhillon: Another example

70
00:12:22.420 --> 00:12:23.930
Inderjit Dhillon: is something that

71
00:12:25.400 --> 00:12:28.060
Inderjit Dhillon: it's kind of more current and modern.

72
00:12:28.850 --> 00:12:30.360
Inderjit Dhillon: Suppose you have

73
00:12:31.940 --> 00:12:33.380
Inderjit Dhillon: Netflix.

74
00:12:34.160 --> 00:12:39.130
Inderjit Dhillon: and what you have on Netflix is people end up reading

75
00:12:39.330 --> 00:12:42.629
Inderjit Dhillon: different movies that they say.

76
00:12:43.490 --> 00:12:49.610
Inderjit Dhillon: Suppose these ratings are on a scale from one to 5, 5 being that they liked it the most.

77
00:12:49.660 --> 00:12:52.090
Inderjit Dhillon: and one being that they did not like it at all.

78
00:12:52.370 --> 00:12:54.290
Inderjit Dhillon: So in this case you have

79
00:12:55.250 --> 00:12:56.450
Inderjit Dhillon: user.

80
00:12:58.390 --> 00:13:00.470
Inderjit Dhillon: Movie or episode

81
00:13:01.380 --> 00:13:02.360
Inderjit Dhillon: and

82
00:13:03.420 --> 00:13:04.330
Inderjit Dhillon: rating.

83
00:13:04.940 --> 00:13:07.290
Inderjit Dhillon: So you have these 3 triplets

84
00:13:08.370 --> 00:13:11.070
Inderjit Dhillon: that various users provide

85
00:13:11.280 --> 00:13:14.150
Inderjit Dhillon: to the system. Right? So, for example.

86
00:13:14.470 --> 00:13:15.850
Inderjit Dhillon: it could be.

87
00:13:16.510 --> 00:13:19.090
Inderjit Dhillon: you know, for example myself.

88
00:13:22.230 --> 00:13:24.020
Inderjit Dhillon: Well, i'm looking at

89
00:13:24.740 --> 00:13:29.920
Inderjit Dhillon: a movie I don't know, or a breaking bad

90
00:13:30.630 --> 00:13:32.030
Inderjit Dhillon: episode one

91
00:13:37.510 --> 00:13:38.740
Inderjit Dhillon: season one

92
00:13:40.270 --> 00:13:43.439
Inderjit Dhillon: right, and suppose I gave it to reading of 4 stars.

93
00:13:47.300 --> 00:13:52.860
Inderjit Dhillon: So you have all these different ratings. And incidentally, actually, this was a competition that was run

94
00:13:53.070 --> 00:13:56.680
Inderjit Dhillon: now about 15 years ago, where Netflix

95
00:13:57.390 --> 00:14:04.290
Inderjit Dhillon: provided some data, and they said that if you can improve

96
00:14:04.370 --> 00:14:08.600
Inderjit Dhillon: their production system by 10%,

97
00:14:08.720 --> 00:14:13.019
Inderjit Dhillon: they would actually award 1 million dollars to the winner.

98
00:14:13.440 --> 00:14:33.319
Inderjit Dhillon: And at some point of time I can go into detail as to how that ended up happening. I think it. Somebody was able to claim 1 million dollars after 3 or 4 years, but it actually spawned off a lot of research in this area of recommender systems or collaborative filtering. That, of course, since then has become much more mature.

99
00:14:33.390 --> 00:14:35.719
Inderjit Dhillon: right? So there's you know.

100
00:14:36.950 --> 00:14:38.710
Inderjit Dhillon: 1 million dollar.

101
00:14:39.240 --> 00:14:41.040
Inderjit Dhillon: Netflix Price

102
00:14:44.030 --> 00:14:47.640
Inderjit Dhillon: and the goal over here. What was the price about the goal is

103
00:14:47.900 --> 00:14:48.910
Inderjit Dhillon: to predict

104
00:14:51.310 --> 00:14:52.410
Inderjit Dhillon: why.

105
00:14:53.360 --> 00:14:54.340
Inderjit Dhillon: for

106
00:14:54.770 --> 00:14:56.329
Inderjit Dhillon: on New X.

107
00:14:57.580 --> 00:15:02.689
Inderjit Dhillon: What is X. Well, this is the x, the sorry this is the y.

108
00:15:03.180 --> 00:15:05.100
Inderjit Dhillon: and this

109
00:15:05.300 --> 00:15:06.960
Inderjit Dhillon: kind of forms

110
00:15:07.940 --> 00:15:08.940
x.

111
00:15:10.060 --> 00:15:16.880
Inderjit Dhillon: So here it's not totally clear what X's. This could be features of the user features of the movie

112
00:15:17.040 --> 00:15:20.559
Inderjit Dhillon: right on the corresponding ratings. But over here

113
00:15:20.820 --> 00:15:23.729
Inderjit Dhillon: the problem is also that Y. I

114
00:15:23.780 --> 00:15:25.240
Inderjit Dhillon: belongs to our.

115
00:15:27.300 --> 00:15:32.649
Inderjit Dhillon: because there is a numerical scale, right 5. It's better than 4 is better than 3 or so on.

116
00:15:33.040 --> 00:15:41.949
Inderjit Dhillon: You could think about it as a classification problem which I'll come to next also. But there's a clear ordering. So this is also an example of a regression problem.

117
00:15:42.160 --> 00:15:56.080
Inderjit Dhillon: and here. The X's are actually not totally clear as to what should be used right. So that's why there was a price that you could try and use whatever you had at your disposal. You obviously did not know the users in this case.

118
00:15:58.050 --> 00:16:00.500
Inderjit Dhillon: but

119
00:16:00.760 --> 00:16:03.839
Inderjit Dhillon: you knew the movie

120
00:16:04.400 --> 00:16:07.879
Inderjit Dhillon: and you needed to possibly form some features.

121
00:16:09.920 --> 00:16:14.310
Inderjit Dhillon: Okay, somebody just sent a message saying, then you cannot hear me.

122
00:16:17.350 --> 00:16:18.800
Inderjit Dhillon: Is that true?

123
00:16:22.420 --> 00:16:24.019
Inderjit Dhillon: Then she sent.

124
00:16:26.760 --> 00:16:27.770
Okay.

125
00:16:29.110 --> 00:16:34.720
Inderjit Dhillon: Okay. So you know, just in in in case something like this happens, you know, if You' to reply to

126
00:16:34.820 --> 00:16:39.749
Inderjit Dhillon: the other people as to what the issue might be. So hopefully, then she can fill.

127
00:16:39.870 --> 00:16:41.729
Inderjit Dhillon: fix that audio issue.

128
00:16:42.980 --> 00:16:44.330
Are you there?

129
00:16:53.190 --> 00:16:53.970
Inderjit Dhillon: Okay.

130
00:16:55.440 --> 00:16:58.230
Inderjit Dhillon: Then we have.

131
00:17:00.360 --> 00:17:02.959
Inderjit Dhillon: This is also a regression problem.

132
00:17:05.630 --> 00:17:07.439
Inderjit Dhillon: This is regression.

133
00:17:09.500 --> 00:17:11.139
Inderjit Dhillon: This is regression.

134
00:17:14.010 --> 00:17:14.609
Okay?

135
00:17:14.910 --> 00:17:20.029
Inderjit Dhillon: Now, suppose I want to I'm: I'm. Going to talk about a different prediction problem, which is

136
00:17:20.400 --> 00:17:21.319
Inderjit Dhillon: predict

137
00:17:23.050 --> 00:17:24.190
Inderjit Dhillon: whether

138
00:17:26.119 --> 00:17:27.540
Inderjit Dhillon: an email

139
00:17:28.870 --> 00:17:30.330
Inderjit Dhillon: is Pam

140
00:17:32.230 --> 00:17:33.430
Inderjit Dhillon: or not.

141
00:17:34.980 --> 00:17:35.690
Inderjit Dhillon: Okay.

142
00:17:36.350 --> 00:17:38.470
Inderjit Dhillon: So in this case

143
00:17:39.020 --> 00:17:42.400
Inderjit Dhillon: you have X

144
00:17:42.990 --> 00:17:46.240
Inderjit Dhillon: is the set of emails.

145
00:17:54.460 --> 00:17:58.299
Inderjit Dhillon: Alright, so if we have an emails x, one

146
00:17:58.880 --> 00:18:00.320
Inderjit Dhillon: x, 2,

147
00:18:00.550 --> 00:18:01.790
Inderjit Dhillon: xm.

148
00:18:03.070 --> 00:18:04.510
Inderjit Dhillon: X. I.

149
00:18:04.550 --> 00:18:10.349
Inderjit Dhillon: Again, we' to some already where these you know there's some way of

150
00:18:10.640 --> 00:18:18.330
Inderjit Dhillon: getting a vector of the numbers associated with each email, and we can see later on how this might arise.

151
00:18:18.960 --> 00:18:22.340
Inderjit Dhillon: and why is the associated labels.

152
00:18:22.500 --> 00:18:25.580
Inderjit Dhillon: and these are whether this email is span

153
00:18:26.110 --> 00:18:27.570
Inderjit Dhillon: or no, no.

154
00:18:30.130 --> 00:18:35.139
Inderjit Dhillon: So remember that we are right. Now, in looking at supervised learning

155
00:18:35.240 --> 00:18:39.269
Inderjit Dhillon: in all these examples. You know you have a training data

156
00:18:39.480 --> 00:18:44.700
Inderjit Dhillon: where you know the wise. So there is a Y associated with it. Later on we look at

157
00:18:44.930 --> 00:18:50.410
Inderjit Dhillon: unsupervised learning where you just have these access, and there is no corresponding label.

158
00:18:50.740 --> 00:18:51.450
Okay.

159
00:18:51.740 --> 00:18:56.140
Inderjit Dhillon: So in this is an example of a classification problem.

160
00:18:59.740 --> 00:19:03.990
Inderjit Dhillon: And this is because this label Y is categorical.

161
00:19:04.540 --> 00:19:08.760
Inderjit Dhillon: it's not naturally like a real number. So when

162
00:19:09.880 --> 00:19:11.330
Inderjit Dhillon: why is

163
00:19:15.410 --> 00:19:16.670
Inderjit Dhillon: this? Is

164
00:19:17.050 --> 00:19:18.430
Inderjit Dhillon: classification.

165
00:19:21.730 --> 00:19:23.350
Inderjit Dhillon: and then, when why

166
00:19:24.890 --> 00:19:25.980
Inderjit Dhillon: is

167
00:19:26.230 --> 00:19:27.620
Inderjit Dhillon: real valued?

168
00:19:29.480 --> 00:19:31.230
Inderjit Dhillon: All continues.

169
00:19:34.700 --> 00:19:36.110
Inderjit Dhillon: then we have a

170
00:19:37.370 --> 00:19:38.530
Inderjit Dhillon: repression problem.

171
00:19:40.230 --> 00:19:41.030
Inderjit Dhillon: Okay.

172
00:19:41.430 --> 00:19:43.690
Inderjit Dhillon: So today we are focused on

173
00:19:43.710 --> 00:19:45.800
Inderjit Dhillon: the regression problem.

174
00:19:46.960 --> 00:19:49.029
Inderjit Dhillon: Okay. So now let's formalize it.

175
00:19:49.120 --> 00:19:50.320
Inderjit Dhillon: We have

176
00:19:52.350 --> 00:19:55.550
Inderjit Dhillon: this training data in the regression problem.

177
00:19:57.930 --> 00:20:00.409
Inderjit Dhillon: So what we are is we are given

178
00:20:02.710 --> 00:20:04.689
Inderjit Dhillon: sorry and go too fast.

179
00:20:10.400 --> 00:20:16.230
Inderjit Dhillon: Given end training points. Each training point is an X. Iy. I. Pair.

180
00:20:17.010 --> 00:20:20.640
Inderjit Dhillon: Xi is 3 dimensional vectors.

181
00:20:21.450 --> 00:20:26.679
Inderjit Dhillon: Y. I is just a single valued real number.

182
00:20:27.320 --> 00:20:28.690
Inderjit Dhillon: and I have

183
00:20:29.000 --> 00:20:30.760
Inderjit Dhillon: 1, 2,

184
00:20:31.870 --> 00:20:33.910
Inderjit Dhillon: and training data points.

185
00:20:34.990 --> 00:20:38.089
Inderjit Dhillon: And well, what does it mean for X to belong to R. D.

186
00:20:38.360 --> 00:20:40.560
Inderjit Dhillon: Right. Let me say that

187
00:20:40.830 --> 00:20:45.970
Inderjit Dhillon: you know the individual component of X, my X, one

188
00:20:46.410 --> 00:20:47.760
Inderjit Dhillon: x, 2

189
00:20:47.780 --> 00:20:49.190
Inderjit Dhillon: through X D:

190
00:20:50.470 --> 00:20:53.410
Inderjit Dhillon: So this is a vector of D numbers.

191
00:20:54.060 --> 00:20:54.760
Inderjit Dhillon: Okay.

192
00:20:55.370 --> 00:21:02.080
Inderjit Dhillon: Now, what is the model that I should use to try and predict why, from the X.

193
00:21:02.700 --> 00:21:04.300
Inderjit Dhillon: The simplest model

194
00:21:04.490 --> 00:21:07.419
Inderjit Dhillon: is to. So we want to do a prediction.

195
00:21:10.240 --> 00:21:13.989
Inderjit Dhillon: The prediction needs to go from Y. Sorry from X,

196
00:21:14.170 --> 00:21:16.250
Inderjit Dhillon: and wants to predict a particular one.

197
00:21:16.500 --> 00:21:20.249
Inderjit Dhillon: Okay, so the simplest thing I can do is do a linear prediction.

198
00:21:21.550 --> 00:21:24.460
Inderjit Dhillon: Okay? And the linear prediction will be that.

199
00:21:24.520 --> 00:21:26.110
Inderjit Dhillon: given the next.

200
00:21:26.880 --> 00:21:28.790
Inderjit Dhillon: I will predict for it, or why

201
00:21:28.850 --> 00:21:30.070
Inderjit Dhillon: substant?

202
00:21:30.680 --> 00:21:32.510
Inderjit Dhillon: Why is

203
00:21:32.600 --> 00:21:33.640
Inderjit Dhillon: linear

204
00:21:33.660 --> 00:21:35.909
Inderjit Dhillon: in X, so I can have

205
00:21:37.840 --> 00:21:40.459
Inderjit Dhillon: this as W. One W, not

206
00:21:40.930 --> 00:21:42.739
Inderjit Dhillon: those W. One

207
00:21:42.780 --> 00:21:44.070
Inderjit Dhillon: x, one

208
00:21:45.470 --> 00:21:46.930
Inderjit Dhillon: W, 2,

209
00:21:47.130 --> 00:21:48.580
Inderjit Dhillon: x, 2

210
00:21:50.000 --> 00:21:52.800
Inderjit Dhillon: plus W. D.

211
00:21:56.670 --> 00:22:01.140
Inderjit Dhillon: Okay, I can write this more compactly as W. Not

212
00:22:01.350 --> 00:22:02.420
Inderjit Dhillon: plus

213
00:22:02.800 --> 00:22:04.110
Inderjit Dhillon: W.

214
00:22:04.230 --> 00:22:05.810
Inderjit Dhillon: Transpose

215
00:22:06.560 --> 00:22:07.720
X:

216
00:22:09.200 --> 00:22:12.760
Inderjit Dhillon: Okay, where my W. Is equal to

217
00:22:13.540 --> 00:22:16.479
Inderjit Dhillon: W, one W, 2,

218
00:22:17.290 --> 00:22:19.149
Inderjit Dhillon: W. D.

219
00:22:21.530 --> 00:22:22.260
Inderjit Dhillon: Okay.

220
00:22:24.080 --> 00:22:28.720
Inderjit Dhillon: that note that I have not included W: Not so, actually. I'm. Going to call this W. Bar.

221
00:22:30.200 --> 00:22:33.200
Inderjit Dhillon: Okay, because right now, I have 2 separate terms.

222
00:22:33.760 --> 00:22:37.440
Inderjit Dhillon: It's actually makes it easier to write it as

223
00:22:37.520 --> 00:22:39.930
Inderjit Dhillon: W. Transpose

224
00:22:40.410 --> 00:22:41.820
Inderjit Dhillon: X bar.

225
00:22:42.870 --> 00:22:44.230
Inderjit Dhillon: So over here

226
00:22:44.410 --> 00:22:47.129
Inderjit Dhillon: X is the same as this X over here.

227
00:22:48.170 --> 00:22:51.690
Inderjit Dhillon: Okay. So I guess there's a cursor here.

228
00:22:52.260 --> 00:22:53.590
Inderjit Dhillon: I think.

229
00:22:57.380 --> 00:22:59.490
Inderjit Dhillon: Okay. So

230
00:22:59.930 --> 00:23:04.130
the X that I've written up above is W. Not plus w

231
00:23:04.260 --> 00:23:07.639
Inderjit Dhillon: trans bar transpose X is the same as the X

232
00:23:07.920 --> 00:23:09.280
Inderjit Dhillon: on the left side.

233
00:23:09.430 --> 00:23:12.419
Inderjit Dhillon: But now what I can do is I can just augment

234
00:23:12.750 --> 00:23:13.720
Inderjit Dhillon: 2

235
00:23:13.980 --> 00:23:15.080
Inderjit Dhillon: x

236
00:23:15.400 --> 00:23:17.860
Inderjit Dhillon: the number one to make it export

237
00:23:18.560 --> 00:23:22.660
Inderjit Dhillon: right? So I can write like this where

238
00:23:25.720 --> 00:23:26.910
Inderjit Dhillon: W.

239
00:23:27.610 --> 00:23:29.289
Inderjit Dhillon: Includes W. Not

240
00:23:29.480 --> 00:23:30.780
Inderjit Dhillon: W. One

241
00:23:31.790 --> 00:23:34.740
Inderjit Dhillon: and all the way to the

242
00:23:35.590 --> 00:23:37.129
Inderjit Dhillon: and X bar

243
00:23:38.320 --> 00:23:40.230
Inderjit Dhillon: is essentially

244
00:23:41.040 --> 00:23:44.080
Inderjit Dhillon: one, and then the rest of us.

245
00:23:44.820 --> 00:23:47.950
Inderjit Dhillon: because this will allow us to write things more completely.

246
00:23:50.230 --> 00:23:52.470
Inderjit Dhillon: Okay. So now both W.

247
00:23:53.050 --> 00:23:54.570
Inderjit Dhillon: And X Bar

248
00:23:54.640 --> 00:23:56.010
Inderjit Dhillon: belongs to.

249
00:23:56.450 --> 00:24:03.730
Inderjit Dhillon: or D plus one right? So that's a convention of saying that it is a vector of size t plus one.

250
00:24:04.780 --> 00:24:05.950
Inderjit Dhillon: Okay.

251
00:24:06.300 --> 00:24:18.160
Inderjit Dhillon: Now, these are the parameters of the model, right? I'm trying to do a linear prediction. But right now I don't know what W. Not W. One through. Wd should be

252
00:24:18.840 --> 00:24:19.590
Inderjit Dhillon: okay.

253
00:24:19.760 --> 00:24:23.070
Inderjit Dhillon: But i'm given the training data. I know that.

254
00:24:23.100 --> 00:24:24.220
Inderjit Dhillon: Why

255
00:24:24.290 --> 00:24:25.750
Inderjit Dhillon: of x one

256
00:24:25.960 --> 00:24:28.910
Inderjit Dhillon: should approach y one right?

257
00:24:30.290 --> 00:24:32.349
Inderjit Dhillon: So why

258
00:24:32.730 --> 00:24:34.180
Inderjit Dhillon: of x one

259
00:24:34.510 --> 00:24:40.000
Inderjit Dhillon: is equal to w transpose times X bar

260
00:24:40.160 --> 00:24:41.170
Inderjit Dhillon: one.

261
00:24:41.950 --> 00:24:43.010
Inderjit Dhillon: and that

262
00:24:43.320 --> 00:24:48.989
Inderjit Dhillon: should be close to W. One, because that's what the training data is

263
00:24:49.040 --> 00:24:51.440
Inderjit Dhillon: right. So that's how I should be choosing my

264
00:24:51.490 --> 00:24:52.790
Inderjit Dhillon: coefficients.

265
00:24:53.010 --> 00:24:55.450
Inderjit Dhillon: But of course I have end data points.

266
00:24:55.690 --> 00:25:01.289
Inderjit Dhillon: So I want y of x 2, which is W. Transpose x 2 bar

267
00:25:01.770 --> 00:25:04.250
Inderjit Dhillon: to be also close to y 2,

268
00:25:04.560 --> 00:25:05.910
Inderjit Dhillon: and similarly

269
00:25:06.240 --> 00:25:07.360
Inderjit Dhillon: why

270
00:25:07.460 --> 00:25:09.100
Inderjit Dhillon: of X, and

271
00:25:10.210 --> 00:25:12.990
Inderjit Dhillon: will be W. Transpose

272
00:25:13.190 --> 00:25:14.770
Inderjit Dhillon: X bar, and

273
00:25:15.200 --> 00:25:16.780
Inderjit Dhillon: should be close to

274
00:25:16.850 --> 00:25:18.040
Inderjit Dhillon: why.

275
00:25:19.010 --> 00:25:23.339
Inderjit Dhillon: But what do I mean by pros to right? So these are real numbers

276
00:25:23.810 --> 00:25:29.329
Inderjit Dhillon: right? So one natural way of doing it is to look at the difference

277
00:25:29.820 --> 00:25:31.110
Inderjit Dhillon: between

278
00:25:32.870 --> 00:25:34.800
Inderjit Dhillon: x transpose.

279
00:25:36.130 --> 00:25:38.930
Inderjit Dhillon: Let me not change the order.

280
00:25:43.670 --> 00:25:47.550
Inderjit Dhillon: W. Transpose x, one bar

281
00:25:48.300 --> 00:25:50.140
Inderjit Dhillon: minus y one.

282
00:25:52.260 --> 00:25:57.580
Inderjit Dhillon: I can look at this now. This could be, you know, if i'm over predicting this would be positive

283
00:25:57.670 --> 00:26:02.060
Inderjit Dhillon: if i'm under predicting and W. Transpose x, one is below y one.

284
00:26:02.150 --> 00:26:04.840
Inderjit Dhillon: This is negative.

285
00:26:05.050 --> 00:26:12.819
Inderjit Dhillon: right? So just this value and one doesn't make sense. So I either I can take absolute value of this which gives a magnitude of the error

286
00:26:13.410 --> 00:26:15.060
Inderjit Dhillon: on X One

287
00:26:15.110 --> 00:26:17.870
Inderjit Dhillon: or I can take the squared value.

288
00:26:18.360 --> 00:26:21.249
Inderjit Dhillon: and if we take the square value it is

289
00:26:21.690 --> 00:26:29.189
Inderjit Dhillon: much better. It it becomes much easier right? So we'll take the square value. So this is called least squares regression.

290
00:26:29.620 --> 00:26:33.270
Inderjit Dhillon: So similarly I have W. Transpose X 2 bar

291
00:26:34.140 --> 00:26:35.259
Inderjit Dhillon: my in this.

292
00:26:35.380 --> 00:26:37.030
Inderjit Dhillon: Why, to Square

293
00:26:38.010 --> 00:26:40.999
Inderjit Dhillon: and W. Transpose Xn. Bar.

294
00:26:41.110 --> 00:26:42.950
Inderjit Dhillon: Myness. Why, him?

295
00:26:46.460 --> 00:26:49.180
Inderjit Dhillon: Basically I want that this is small.

296
00:26:50.640 --> 00:26:53.070
Inderjit Dhillon: I want that this is small.

297
00:26:55.520 --> 00:26:58.169
Inderjit Dhillon: And then I want that this is small.

298
00:27:00.520 --> 00:27:05.200
Inderjit Dhillon: Okay? And then we can see what what we mean by small right.

299
00:27:05.700 --> 00:27:07.660
Inderjit Dhillon: Should it be exactly 0,

300
00:27:08.680 --> 00:27:13.700
Inderjit Dhillon: exactly as you know, would be mean that you actually fit the data. Exactly.

301
00:27:14.600 --> 00:27:19.579
Inderjit Dhillon: But what if there is a measurement error right? And

302
00:27:20.390 --> 00:27:25.569
Inderjit Dhillon: we, in that case we don't want to fit it. Exactly. So the problem

303
00:27:25.610 --> 00:27:27.619
Inderjit Dhillon: of trying to fit it more.

304
00:27:27.660 --> 00:27:28.290
Okay.

305
00:27:28.400 --> 00:27:29.340
Inderjit Dhillon: You know

306
00:27:29.440 --> 00:27:34.679
Inderjit Dhillon: you you want. The basic idea is that you want to be able to predict

307
00:27:34.930 --> 00:27:38.439
Inderjit Dhillon: on a new X. That's the purpose of regression.

308
00:27:38.890 --> 00:27:41.940
Inderjit Dhillon: But we already know the Y once to why ends.

309
00:27:42.420 --> 00:27:46.320
Inderjit Dhillon: We just want the prediction on those to be close.

310
00:27:46.660 --> 00:27:50.229
Inderjit Dhillon: Right? So we'll see what you know. There's something called

311
00:27:50.700 --> 00:28:00.059
Inderjit Dhillon: training error, and then there is test error. So what we want is that test error which is the error on new points

312
00:28:00.380 --> 00:28:06.930
Inderjit Dhillon: to be small. But of course we don't have any new points. The assumption implicitly that we are making.

313
00:28:07.070 --> 00:28:11.230
Inderjit Dhillon: Is there the current X's, or a represent or or

314
00:28:11.250 --> 00:28:15.020
Inderjit Dhillon: reflection of the distribution of the new access that we will get.

315
00:28:15.190 --> 00:28:15.920
Okay.

316
00:28:16.410 --> 00:28:19.620
Inderjit Dhillon: so we can try and make.

317
00:28:19.810 --> 00:28:27.129
Inderjit Dhillon: you know. If you want to make this small, we can just say, okay, I'm going to make the sum of all these small. So I can say that I have an objective function

318
00:28:27.390 --> 00:28:28.640
Inderjit Dhillon: F of W.

319
00:28:29.340 --> 00:28:30.730
Inderjit Dhillon: Which is

320
00:28:31.020 --> 00:28:36.040
Inderjit Dhillon: summation of W. Transpose x, I bar

321
00:28:37.340 --> 00:28:39.000
Inderjit Dhillon: minus y I square.

322
00:28:40.350 --> 00:28:44.169
Inderjit Dhillon: and I want to make this summation from I equal to one to N.

323
00:28:44.700 --> 00:28:46.030
Inderjit Dhillon: To be small.

324
00:28:46.490 --> 00:28:50.750
Inderjit Dhillon: and then, because there is a square, and you will see it and make things easier. I can just

325
00:28:51.040 --> 00:28:52.620
Inderjit Dhillon: put a half over here.

326
00:28:53.300 --> 00:28:55.489
Inderjit Dhillon: and so now go

327
00:28:57.360 --> 00:28:58.660
Inderjit Dhillon: is to

328
00:28:59.000 --> 00:28:59.690
Inderjit Dhillon: hmm.

329
00:28:59.840 --> 00:29:01.260
Inderjit Dhillon: Minimize.

330
00:29:02.800 --> 00:29:04.020
Inderjit Dhillon: Fw.

331
00:29:04.610 --> 00:29:06.190
Inderjit Dhillon: So find a W.

332
00:29:06.830 --> 00:29:11.880
Inderjit Dhillon: Find the coefficient. Vector find the parameters of the model, this very simple linear model.

333
00:29:12.240 --> 00:29:13.280
Inderjit Dhillon: so that

334
00:29:13.310 --> 00:29:15.590
Fw. Is

335
00:29:15.730 --> 00:29:18.079
Inderjit Dhillon: minimized. So this is

336
00:29:18.600 --> 00:29:20.209
Inderjit Dhillon: least squares a question.

337
00:29:28.580 --> 00:29:29.420
Inderjit Dhillon: Okay.

338
00:29:30.360 --> 00:29:31.540
Inderjit Dhillon: So now.

339
00:29:31.780 --> 00:29:36.329
Inderjit Dhillon: one of the I I discussed quite a bit about linear algebra

340
00:29:36.430 --> 00:29:42.880
Inderjit Dhillon: last in in the first lecture that you know that you should have a linear algebra course.

341
00:29:44.330 --> 00:29:51.640
Inderjit Dhillon: so we'll be able to write this much more compactly by using vectors and matrices. We've already used vectors

342
00:29:51.760 --> 00:30:00.659
Inderjit Dhillon: right. But let's make this more compact, and it'll enable us to find solutions kind of much more easily, and then also utilize

343
00:30:00.750 --> 00:30:02.930
facts from the

344
00:30:03.240 --> 00:30:06.019
Inderjit Dhillon: so. What we'll do is let's assemble

345
00:30:07.530 --> 00:30:09.259
Inderjit Dhillon: all the training data

346
00:30:09.720 --> 00:30:14.609
Inderjit Dhillon: augmented by the one. So x bar x, one bar x, 2 bar

347
00:30:14.690 --> 00:30:16.840
Inderjit Dhillon: i'm. Going to assemble them in a matrix.

348
00:30:17.390 --> 00:30:18.380
Inderjit Dhillon: So

349
00:30:18.990 --> 00:30:23.669
Inderjit Dhillon: this is Notation x one, so x is capital. X is a matrix.

350
00:30:24.020 --> 00:30:27.789
Inderjit Dhillon: The first column is x, one bar. The first training data

351
00:30:28.450 --> 00:30:29.330
Inderjit Dhillon: point.

352
00:30:29.590 --> 00:30:32.450
Inderjit Dhillon: The X 2 bar is the second training data

353
00:30:32.520 --> 00:30:33.900
Inderjit Dhillon: point or vector.

354
00:30:34.210 --> 00:30:36.159
Inderjit Dhillon: And then I have Xn. Bar.

355
00:30:36.870 --> 00:30:38.999
Inderjit Dhillon: So now x is a matrix.

356
00:30:40.530 --> 00:30:42.250
Inderjit Dhillon: which is

357
00:30:42.730 --> 00:30:45.490
Inderjit Dhillon: well, what is the dimension of each X I.

358
00:30:45.750 --> 00:30:47.479
Inderjit Dhillon: But it's d plus one.

359
00:30:48.870 --> 00:30:53.600
Inderjit Dhillon: And what is the total number of points that is? And

360
00:30:54.130 --> 00:30:57.069
Inderjit Dhillon: so X. The capital X

361
00:30:57.300 --> 00:31:00.219
Inderjit Dhillon: belongs to, or d plus one.

362
00:31:00.840 --> 00:31:02.310
Inderjit Dhillon: My.

363
00:31:04.610 --> 00:31:09.010
Inderjit Dhillon: and then let's make y to be a vector

364
00:31:09.890 --> 00:31:11.270
Inderjit Dhillon: which has

365
00:31:11.510 --> 00:31:12.910
Inderjit Dhillon: the labels

366
00:31:13.210 --> 00:31:17.030
Inderjit Dhillon: or the values target values associated with each. Xi.

367
00:31:17.360 --> 00:31:19.410
Inderjit Dhillon: So this is

368
00:31:19.660 --> 00:31:20.760
Inderjit Dhillon: why one

369
00:31:21.430 --> 00:31:23.979
Inderjit Dhillon: y 2 through y.

370
00:31:25.670 --> 00:31:26.890
Inderjit Dhillon: So, why

371
00:31:27.310 --> 00:31:28.340
Inderjit Dhillon: is

372
00:31:29.230 --> 00:31:30.280
Inderjit Dhillon: or am

373
00:31:30.610 --> 00:31:42.240
Inderjit Dhillon: Now, you know. Just remember that from context, that this why, that I've written over here is different than the why above the Why, above over here. Let me just circle it for for a minute.

374
00:31:42.350 --> 00:31:44.070
Inderjit Dhillon: like, for example, this one

375
00:31:45.150 --> 00:31:46.990
Inderjit Dhillon: Don't, you know it's a function.

376
00:31:47.100 --> 00:31:48.610
Inderjit Dhillon: Okay. But here.

377
00:31:48.960 --> 00:31:50.229
Inderjit Dhillon: over here.

378
00:31:50.410 --> 00:31:52.889
Inderjit Dhillon: I'm. Actually denoting a particular. Vector

379
00:31:53.620 --> 00:31:56.399
Inderjit Dhillon: So what I'm doing should be kind of peer from context.

380
00:31:56.980 --> 00:31:57.880
Yeah.

381
00:31:58.680 --> 00:32:01.370
Inderjit Dhillon: So now, what is X?

382
00:32:01.890 --> 00:32:06.449
Inderjit Dhillon: Okay. So if I look at that. Let me just expand it out.

383
00:32:07.290 --> 00:32:09.360
Inderjit Dhillon: I have one

384
00:32:09.440 --> 00:32:11.070
Inderjit Dhillon: x, 1, one.

385
00:32:11.850 --> 00:32:14.570
Inderjit Dhillon: x, 2, x, 1, 2,

386
00:32:23.850 --> 00:32:27.350
Inderjit Dhillon: and then x one, and

387
00:32:27.550 --> 00:32:33.179
Inderjit Dhillon: the second column is one, x, 2, one, x, 2, 2,

388
00:32:34.420 --> 00:32:36.100
Inderjit Dhillon: x, 2, and

389
00:32:37.490 --> 00:32:39.260
Inderjit Dhillon: and I have N. Columns

390
00:32:40.620 --> 00:32:43.639
Inderjit Dhillon: x, and well, no, that's just one

391
00:32:46.830 --> 00:32:49.630
Inderjit Dhillon: x and one

392
00:32:51.840 --> 00:32:54.430
Inderjit Dhillon: x and 2

393
00:32:55.720 --> 00:32:57.090
Inderjit Dhillon: X. And

394
00:32:58.150 --> 00:33:02.249
Inderjit Dhillon: so I see I've already made like a mistake. Hopefully, you guys got that.

395
00:33:03.490 --> 00:33:07.210
Inderjit Dhillon: and so I can correct my mistake.

396
00:33:08.790 --> 00:33:11.950
Inderjit Dhillon: And this is X one. D:

397
00:33:13.120 --> 00:33:15.090
Inderjit Dhillon: thanks to the

398
00:33:18.820 --> 00:33:19.520
Inderjit Dhillon: Okay.

399
00:33:20.740 --> 00:33:24.050
Inderjit Dhillon: So by constructing these matrices.

400
00:33:24.700 --> 00:33:29.020
Inderjit Dhillon: I can actually write my Fw. In a very compact

401
00:33:29.080 --> 00:33:30.010
Inderjit Dhillon: manner.

402
00:33:31.030 --> 00:33:32.919
Inderjit Dhillon: So, using linear algebra.

403
00:33:33.010 --> 00:33:35.789
Inderjit Dhillon: you know what matrix vector multiplication is.

404
00:33:35.960 --> 00:33:38.120
Inderjit Dhillon: I can write it as half

405
00:33:39.470 --> 00:33:40.420
Inderjit Dhillon: of

406
00:33:41.740 --> 00:33:44.150
Inderjit Dhillon: X. Transpose

407
00:33:44.640 --> 00:33:45.850
Inderjit Dhillon: w

408
00:33:46.320 --> 00:33:47.710
Inderjit Dhillon: minus y.

409
00:33:48.940 --> 00:33:51.289
Inderjit Dhillon: and then this is

410
00:33:51.330 --> 00:33:53.820
Inderjit Dhillon: the sum of squares.

411
00:33:54.670 --> 00:33:58.539
Inderjit Dhillon: and I can write it essentially as the squared

412
00:33:58.580 --> 00:33:59.450
Inderjit Dhillon: to norm.

413
00:34:02.500 --> 00:34:04.489
Inderjit Dhillon: So this is the least square subject.

414
00:34:15.100 --> 00:34:17.969
Inderjit Dhillon: So you should be able to see that these 2 are equivalent.

415
00:34:22.159 --> 00:34:26.489
Inderjit Dhillon: Okay, and that just you know, I've written the X out explicitly.

416
00:34:26.670 --> 00:34:30.039
Inderjit Dhillon: You can look at what X. Transpose Y is

417
00:34:31.230 --> 00:34:33.740
Inderjit Dhillon: x. Transpose Y.

418
00:34:33.770 --> 00:34:35.710
Inderjit Dhillon: We'll essentially leave these

419
00:34:35.889 --> 00:34:37.290
Inderjit Dhillon: inner products.

420
00:34:37.820 --> 00:34:41.170
Inderjit Dhillon: Right? X. Transpose Y is a vector

421
00:34:41.449 --> 00:34:46.949
Inderjit Dhillon: the I component of that will be W. Transpose X bar I,

422
00:34:47.219 --> 00:34:51.310
Inderjit Dhillon: and then you take the corresponding element in that vector Y.

423
00:34:51.880 --> 00:34:56.920
Inderjit Dhillon: You square it. And the 2 Norms square is just saying that i'm going to take the sum of squares.

424
00:34:57.300 --> 00:34:58.089
Inderjit Dhillon: Okay.

425
00:34:58.490 --> 00:34:59.939
Inderjit Dhillon: So you shouldn't

426
00:35:00.410 --> 00:35:01.799
Inderjit Dhillon: be able to go

427
00:35:01.850 --> 00:35:06.400
Inderjit Dhillon: see that this Fw. Is the same as the Fw. Above.

428
00:35:06.760 --> 00:35:08.919
Inderjit Dhillon: Okay, through properties of

429
00:35:09.210 --> 00:35:11.290
matrix vector multiplication.

430
00:35:12.140 --> 00:35:13.000
Inderjit Dhillon: Okay.

431
00:35:13.600 --> 00:35:14.620
Inderjit Dhillon: no.

432
00:35:14.640 --> 00:35:16.029
Inderjit Dhillon: The goal is

433
00:35:18.410 --> 00:35:19.390
Inderjit Dhillon: Find

434
00:35:19.680 --> 00:35:20.709
Inderjit Dhillon: W.

435
00:35:21.920 --> 00:35:23.200
Inderjit Dhillon: Such that

436
00:35:25.180 --> 00:35:26.450
Inderjit Dhillon: F. W.

437
00:35:26.780 --> 00:35:28.209
Inderjit Dhillon: Is minimized.

438
00:35:35.260 --> 00:35:37.079
Inderjit Dhillon: Okay? Well, how would I do that?

439
00:35:38.950 --> 00:35:40.919
Inderjit Dhillon: Well, this is

440
00:35:41.940 --> 00:35:48.930
Inderjit Dhillon: multivariate function. F. W. W. Is a. Vector so it has multiple variables, not just one variable

441
00:35:49.740 --> 00:35:50.419
Inderjit Dhillon: bye.

442
00:35:50.740 --> 00:35:54.690
Inderjit Dhillon: It's a function. I want to try and minimize it. So

443
00:35:54.870 --> 00:35:56.439
Inderjit Dhillon: from calculus

444
00:35:56.860 --> 00:35:58.640
Inderjit Dhillon: I can set the

445
00:35:58.670 --> 00:36:00.020
Inderjit Dhillon: derivative

446
00:36:00.350 --> 00:36:01.600
Inderjit Dhillon: to be 0,

447
00:36:02.150 --> 00:36:07.049
Inderjit Dhillon: and then try and find out the corresponding value of W.

448
00:36:08.050 --> 00:36:10.920
Inderjit Dhillon: But remember, it is a month. It's a

449
00:36:11.070 --> 00:36:12.580
Inderjit Dhillon: multivariate function.

450
00:36:12.850 --> 00:36:19.699
Inderjit Dhillon: Okay, so the derivative is going to be, you know, in the in the motivated case it's called the gradient

451
00:36:20.170 --> 00:36:24.850
Inderjit Dhillon: right. So the gradient of this function must be equal to 0,

452
00:36:25.170 --> 00:36:28.049
Inderjit Dhillon: So let's see what happens. But let's first

453
00:36:28.090 --> 00:36:29.720
Inderjit Dhillon: simplify this function.

454
00:36:30.640 --> 00:36:32.149
Inderjit Dhillon: So this

455
00:36:33.520 --> 00:36:37.470
Inderjit Dhillon: again from linear algebra. We know that

456
00:36:37.680 --> 00:36:39.399
Inderjit Dhillon: the 2 Norms square

457
00:36:39.430 --> 00:36:43.240
Inderjit Dhillon: is just the inner product of the vector. With itself.

458
00:36:43.870 --> 00:36:48.499
Inderjit Dhillon: So this is X. Transpose W minus y

459
00:36:48.820 --> 00:36:53.999
Inderjit Dhillon: transpose times X: transpose W minus Y.

460
00:36:55.230 --> 00:36:59.349
Inderjit Dhillon: Okay, so I can think of X. Transpose W minus y

461
00:36:59.410 --> 00:37:01.289
Inderjit Dhillon: as a residual vector

462
00:37:01.940 --> 00:37:06.809
Inderjit Dhillon: right. And now it is the inner product of the residual with itself.

463
00:37:08.150 --> 00:37:12.009
Inderjit Dhillon: Okay? Well, through properties of the matrix. Transpose

464
00:37:12.370 --> 00:37:15.369
Inderjit Dhillon: this backgrounds W. Transpose X

465
00:37:15.520 --> 00:37:17.569
Inderjit Dhillon: minus y transpose

466
00:37:18.450 --> 00:37:21.690
Inderjit Dhillon: X: transpose W minus Y.

467
00:37:23.170 --> 00:37:25.440
Inderjit Dhillon: Okay. So I multiply through

468
00:37:25.910 --> 00:37:27.290
Inderjit Dhillon: my good half

469
00:37:28.180 --> 00:37:30.930
Inderjit Dhillon: W. Transpose x

470
00:37:31.340 --> 00:37:33.839
Inderjit Dhillon: x transpose w

471
00:37:34.620 --> 00:37:39.989
Inderjit Dhillon: minus y transpose x transpose W

472
00:37:40.870 --> 00:37:43.160
Inderjit Dhillon: minus y

473
00:37:43.700 --> 00:37:44.560
Inderjit Dhillon: sorry

474
00:37:47.800 --> 00:37:49.169
Inderjit Dhillon: minus

475
00:37:50.130 --> 00:37:53.310
Inderjit Dhillon: w transpose x y

476
00:37:53.620 --> 00:37:56.179
Inderjit Dhillon: plus y transpose. Why.

477
00:37:57.510 --> 00:38:02.380
Inderjit Dhillon: okay, These terms this term and this term are the same.

478
00:38:02.800 --> 00:38:05.740
Inderjit Dhillon: because it's just

479
00:38:05.930 --> 00:38:09.950
Inderjit Dhillon: y transpose x transpose W. If you think about it.

480
00:38:10.470 --> 00:38:14.530
Inderjit Dhillon: is the inner product of a vector, with another vector it gives a number.

481
00:38:15.220 --> 00:38:20.479
Inderjit Dhillon: So these are just 2 different ways of writing a number where one is the transpose of the other

482
00:38:20.700 --> 00:38:22.810
Inderjit Dhillon: right. So this is

483
00:38:23.050 --> 00:38:24.020
Inderjit Dhillon: of

484
00:38:24.430 --> 00:38:27.779
Inderjit Dhillon: W. Transpose X. X. Transpose w

485
00:38:28.470 --> 00:38:29.950
Inderjit Dhillon: minus

486
00:38:30.120 --> 00:38:31.029
Inderjit Dhillon: 2,

487
00:38:31.750 --> 00:38:35.260
Inderjit Dhillon: y transpose x transpose w

488
00:38:35.630 --> 00:38:38.370
Inderjit Dhillon: plus what transpose y.

489
00:38:41.070 --> 00:38:44.129
Inderjit Dhillon: And now, like, I said, we'll take the gradient.

490
00:38:44.480 --> 00:38:46.669
Inderjit Dhillon: The function

491
00:38:46.940 --> 00:38:48.750
Inderjit Dhillon: I have a function of

492
00:38:48.900 --> 00:38:52.910
Inderjit Dhillon: W. So I want to take a gradient with respect to W,

493
00:38:53.490 --> 00:38:55.959
Inderjit Dhillon: and that can be shown to be.

494
00:38:56.120 --> 00:38:57.470
Inderjit Dhillon: It's a quadratic.

495
00:38:58.520 --> 00:39:05.300
Inderjit Dhillon: So just like you have in the single variable case, single, univariate case.

496
00:39:07.700 --> 00:39:08.950
Inderjit Dhillon: the

497
00:39:09.330 --> 00:39:14.489
Inderjit Dhillon: derivative becomes linear similar to over here, it becomes linear in

498
00:39:14.730 --> 00:39:16.550
Inderjit Dhillon: in W.

499
00:39:17.090 --> 00:39:18.149
Inderjit Dhillon: So

500
00:39:18.210 --> 00:39:21.129
Inderjit Dhillon: it becomes Xx transpose times W.

501
00:39:21.820 --> 00:39:25.100
Inderjit Dhillon: Minus X times. What?

502
00:39:29.310 --> 00:39:30.040
Inderjit Dhillon: Okay?

503
00:39:30.360 --> 00:39:31.479
Inderjit Dhillon: And then

504
00:39:31.600 --> 00:39:33.430
Inderjit Dhillon: W. Transpose.

505
00:39:33.750 --> 00:39:35.730
Inderjit Dhillon: Sorry the the

506
00:39:35.820 --> 00:39:40.150
Inderjit Dhillon: is the one that minimizes it. So up the W. Star

507
00:39:40.870 --> 00:39:43.310
Inderjit Dhillon: is

508
00:39:44.690 --> 00:39:45.930
Inderjit Dhillon: of Fw.

509
00:39:48.000 --> 00:39:52.159
Inderjit Dhillon: Augment just means that minimizing F. Of W.

510
00:39:52.200 --> 00:39:57.300
Inderjit Dhillon: And what is the W. That minimizes that? So this is just a notation. It's just saying that

511
00:39:57.380 --> 00:40:00.649
Inderjit Dhillon: W. Star minimizes F of

512
00:40:02.700 --> 00:40:04.529
Inderjit Dhillon: Okay. So that means that

513
00:40:04.950 --> 00:40:07.089
Inderjit Dhillon: gradient of F

514
00:40:07.530 --> 00:40:09.040
Inderjit Dhillon: at W. Star

515
00:40:09.470 --> 00:40:11.040
Inderjit Dhillon: must be equal to 0.

516
00:40:12.180 --> 00:40:13.769
Inderjit Dhillon: Which means that

517
00:40:14.060 --> 00:40:17.669
Inderjit Dhillon: my coefficients that I want from the training data.

518
00:40:17.890 --> 00:40:22.429
Inderjit Dhillon: or X X. Transpose W. Star

519
00:40:23.120 --> 00:40:25.450
Inderjit Dhillon: minus x, y

520
00:40:27.060 --> 00:40:28.430
Inderjit Dhillon: equals 0,

521
00:40:30.240 --> 00:40:34.479
Inderjit Dhillon: which means that X. X. Transpose W. Star

522
00:40:35.330 --> 00:40:38.040
Inderjit Dhillon: is equal to X times. Y.

523
00:40:39.810 --> 00:40:40.589
Inderjit Dhillon: Okay.

524
00:40:41.540 --> 00:40:45.140
Inderjit Dhillon: So remember, let's see what is Xx transpose?

525
00:40:46.990 --> 00:40:50.750
Inderjit Dhillon: Remember, we originally had this matrix. X:

526
00:40:51.930 --> 00:40:54.409
Inderjit Dhillon: right? This is the matrix X.

527
00:40:59.580 --> 00:41:02.549
Inderjit Dhillon: So X is

528
00:41:03.880 --> 00:41:05.560
Inderjit Dhillon: d plus one

529
00:41:05.770 --> 00:41:06.870
Inderjit Dhillon: by a

530
00:41:10.640 --> 00:41:11.509
Inderjit Dhillon: Okay.

531
00:41:12.130 --> 00:41:15.919
Inderjit Dhillon: So X was d plus one

532
00:41:17.230 --> 00:41:18.500
Inderjit Dhillon: by end.

533
00:41:18.920 --> 00:41:22.529
Inderjit Dhillon: So Xx transpose is a matrix which is

534
00:41:23.010 --> 00:41:24.790
Inderjit Dhillon: d plus one

535
00:41:24.880 --> 00:41:27.029
Inderjit Dhillon: by d plus one.

536
00:41:28.760 --> 00:41:32.539
Inderjit Dhillon: Okay, so it's a B, one by d plus one matrix.

537
00:41:33.180 --> 00:41:36.099
Inderjit Dhillon: And what is now the solution?

538
00:41:36.130 --> 00:41:37.339
Inderjit Dhillon: Do this?

539
00:41:38.110 --> 00:41:40.759
Inderjit Dhillon: This now is a linear system

540
00:41:40.870 --> 00:41:42.519
Inderjit Dhillon: of equations.

541
00:41:43.970 --> 00:41:46.830
Inderjit Dhillon: There are how many variables.

542
00:41:46.950 --> 00:41:50.649
Inderjit Dhillon: how many unknowns, 30 plus one right. We have w not

543
00:41:50.810 --> 00:41:53.400
Inderjit Dhillon: w one throughwd.

544
00:41:53.940 --> 00:41:59.490
Inderjit Dhillon: and it's reassuring that xx transpose is a d one by d plus one matrix.

545
00:41:59.900 --> 00:42:04.319
Inderjit Dhillon: so I have essentially d plus one linear equations.

546
00:42:04.810 --> 00:42:10.070
Inderjit Dhillon: If I look at each role of X. X. Transpose W. Star and Xy.

547
00:42:10.260 --> 00:42:12.080
Inderjit Dhillon: this is one linear question.

548
00:42:12.380 --> 00:42:14.660
Inderjit Dhillon: Each row corresponds to one equation.

549
00:42:14.960 --> 00:42:16.150
Inderjit Dhillon: So I have

550
00:42:16.820 --> 00:42:18.439
Inderjit Dhillon: d plus one.

551
00:42:19.580 --> 00:42:20.700
Inderjit Dhillon: Linear

552
00:42:20.950 --> 00:42:22.259
Inderjit Dhillon: any questions

553
00:42:24.080 --> 00:42:26.099
Inderjit Dhillon: I have d plus one

554
00:42:26.690 --> 00:42:27.859
Inderjit Dhillon: unknowns.

555
00:42:29.760 --> 00:42:33.910
Inderjit Dhillon: So in general I can actually solve for it uniquely.

556
00:42:36.060 --> 00:42:37.379
Inderjit Dhillon: What is the solution?

557
00:42:38.830 --> 00:42:41.400
Inderjit Dhillon: What is the solution to this linear system?

558
00:42:41.580 --> 00:42:45.509
Inderjit Dhillon: And it could be written but in terms of

559
00:42:45.770 --> 00:42:49.419
Inderjit Dhillon: the inverse of Xx. Transpose.

560
00:42:50.010 --> 00:42:52.529
Inderjit Dhillon: so it's exposed times

561
00:42:52.890 --> 00:42:55.589
Inderjit Dhillon: inverse times. X times Y.

562
00:42:56.120 --> 00:42:57.959
Inderjit Dhillon: But remember, this is okay.

563
00:42:58.300 --> 00:42:59.089
Inderjit Dhillon: If

564
00:42:59.810 --> 00:43:02.169
Inderjit Dhillon: X X transpose is

565
00:43:03.780 --> 00:43:05.520
Inderjit Dhillon: non-singular.

566
00:43:06.900 --> 00:43:10.910
Inderjit Dhillon: which means that this inverse excesses.

567
00:43:11.440 --> 00:43:14.760
Inderjit Dhillon: So this is the solution of the linear regression problem.

568
00:43:15.170 --> 00:43:16.339
Inderjit Dhillon: and

569
00:43:17.630 --> 00:43:19.500
Inderjit Dhillon: these are actually called

570
00:43:19.530 --> 00:43:20.419
Inderjit Dhillon: the

571
00:43:20.990 --> 00:43:22.689
Inderjit Dhillon: normal equations.

572
00:43:27.750 --> 00:43:29.629
Inderjit Dhillon: So this is okay.

573
00:43:32.910 --> 00:43:39.449
Inderjit Dhillon: And then, of course, you can ask me, okay, what happens if Xx transpose is non singular.

574
00:43:40.370 --> 00:43:48.769
Inderjit Dhillon: and then it becomes a little bit more interesting. So right now let's just take the case that X transpose is singular, but there are cases, For example.

575
00:43:49.160 --> 00:43:51.339
Inderjit Dhillon: suppose you have

576
00:43:51.480 --> 00:43:53.380
Inderjit Dhillon: the same measurements over here.

577
00:43:54.410 --> 00:43:57.919
Inderjit Dhillon: Right? So x 1 one is the first management.

578
00:43:58.760 --> 00:44:01.959
Inderjit Dhillon: you know some other values are another measurement.

579
00:44:02.080 --> 00:44:04.560
Inderjit Dhillon: What if you make the same measurement twice?

580
00:44:05.460 --> 00:44:07.189
Inderjit Dhillon: Where do you duplicate it?

581
00:44:07.440 --> 00:44:10.419
Inderjit Dhillon: You let's, say you duplicate a feature. Sometimes

582
00:44:10.460 --> 00:44:12.729
Inderjit Dhillon: each of these measurements is called a feature

583
00:44:13.540 --> 00:44:17.020
Inderjit Dhillon: Right? Well, in that case you will get singularity

584
00:44:18.070 --> 00:44:26.630
Inderjit Dhillon: right, because it has to do with this matrix, not having full rank, and we can come to that a little bit later. So there are

585
00:44:26.750 --> 00:44:28.430
Inderjit Dhillon: very important cases

586
00:44:28.790 --> 00:44:29.830
Inderjit Dhillon: where

587
00:44:29.950 --> 00:44:32.639
Inderjit Dhillon: this matrix Xx transpose might be.

588
00:44:34.540 --> 00:44:38.430
Inderjit Dhillon: So what I've shown you is that

589
00:44:38.580 --> 00:44:42.260
Inderjit Dhillon: by, you know, in some sense, going from first principles.

590
00:44:43.000 --> 00:44:46.440
Inderjit Dhillon: we started off with a linear model.

591
00:44:47.560 --> 00:44:51.280
Inderjit Dhillon: We had, like a reasonable

592
00:44:51.770 --> 00:44:53.359
Inderjit Dhillon: measure of error.

593
00:44:54.190 --> 00:44:56.799
Inderjit Dhillon: In this case we chose the square loss

594
00:44:57.050 --> 00:45:00.419
Inderjit Dhillon: that we were actually able to get the solution

595
00:45:00.510 --> 00:45:01.849
Inderjit Dhillon: in closed form.

596
00:45:02.100 --> 00:45:04.180
Inderjit Dhillon: Yes, it requires some competition.

597
00:45:04.730 --> 00:45:08.450
Inderjit Dhillon: right? Because now we need to figure out how to solve this linear system.

598
00:45:08.900 --> 00:45:12.519
Inderjit Dhillon: and we'll come to that. But I have the solution in closed form.

599
00:45:12.780 --> 00:45:15.559
Inderjit Dhillon: It doesn't always happen like this. But

600
00:45:15.900 --> 00:45:18.009
Inderjit Dhillon: this is the kind of

601
00:45:18.190 --> 00:45:20.810
Inderjit Dhillon: the advantage of using squared loss

602
00:45:21.730 --> 00:45:24.750
Inderjit Dhillon: you could have used absolute value.

603
00:45:25.350 --> 00:45:27.970
Inderjit Dhillon: But then things become a little bit more complicated.

604
00:45:28.570 --> 00:45:33.190
Inderjit Dhillon: But if I use the absolute value, then you know it's not a

605
00:45:33.710 --> 00:45:37.279
Inderjit Dhillon: The absolute value function is not differentiable.

606
00:45:38.290 --> 00:45:41.640
Inderjit Dhillon: Things get a little bit more complicated. The solution is not as simple.

607
00:45:42.780 --> 00:45:47.379
Inderjit Dhillon: Okay, it's quite lost. The best loss to use

608
00:45:47.450 --> 00:45:49.320
Inderjit Dhillon: in all your applications

609
00:45:50.330 --> 00:45:53.779
Inderjit Dhillon: might not be right. So what if, for example.

610
00:45:54.160 --> 00:45:57.449
Inderjit Dhillon: one of these. Why, i's an outlier.

611
00:46:00.100 --> 00:46:03.839
Inderjit Dhillon: But suppose you made a big error in measurement.

612
00:46:04.110 --> 00:46:07.200
Inderjit Dhillon: one of these values? Why, I is an outlier.

613
00:46:07.220 --> 00:46:11.719
Inderjit Dhillon: What could happen in a squared loss is when you square it it'll get amplified.

614
00:46:11.980 --> 00:46:13.780
Inderjit Dhillon: The it will get amplified.

615
00:46:13.920 --> 00:46:20.560
Inderjit Dhillon: And what this method will try and do is actually fit quite a lot to the outlier, and that might be bad.

616
00:46:22.370 --> 00:46:32.259
Inderjit Dhillon: So you might wonder you use a different loss function right, and we'll come to that. But right now, if you're using a squared loss, I've given you a derivation

617
00:46:32.970 --> 00:46:37.359
Inderjit Dhillon: which formulates it in linear algebraic terms

618
00:46:37.520 --> 00:46:43.010
Inderjit Dhillon: gets to a least squares problem. And then by doing calculus matrix calculus.

619
00:46:43.220 --> 00:46:45.799
Inderjit Dhillon: We've been able to find the solution.

620
00:46:46.720 --> 00:46:49.479
Inderjit Dhillon: Now, there are many. There are other ways to

621
00:46:49.700 --> 00:46:57.770
Inderjit Dhillon: kind of. Look at this problem. So let me give you a little bit of a a different flavor of trying to solve this by giving you a

622
00:46:58.250 --> 00:46:59.930
Inderjit Dhillon: more geometric view.

623
00:47:03.040 --> 00:47:05.349
Inderjit Dhillon: And so let's look at a geometric view.

624
00:47:06.630 --> 00:47:10.250
Inderjit Dhillon: So remember that X transpose

625
00:47:11.680 --> 00:47:14.859
Inderjit Dhillon: has this form. It's one

626
00:47:15.670 --> 00:47:19.410
Inderjit Dhillon: x, one transpose, remember x, one transpose is.

627
00:47:19.650 --> 00:47:22.609
or x, one is a d dimensional vector

628
00:47:23.360 --> 00:47:25.110
Inderjit Dhillon: So this is one.

629
00:47:25.150 --> 00:47:27.150
Inderjit Dhillon: x, 2 transpose

630
00:47:27.340 --> 00:47:28.390
Inderjit Dhillon: one

631
00:47:28.800 --> 00:47:30.450
Inderjit Dhillon: x, and transport

632
00:47:31.170 --> 00:47:33.129
Inderjit Dhillon: right. But remember that.

633
00:47:33.390 --> 00:47:35.749
Inderjit Dhillon: and this is the same as

634
00:47:36.270 --> 00:47:41.340
Inderjit Dhillon: x, one bar transpose x, 2 more transpose

635
00:47:42.540 --> 00:47:44.710
Inderjit Dhillon: X and or transpose.

636
00:47:46.760 --> 00:47:50.679
Inderjit Dhillon: And what I want is that X transpose W.

637
00:47:51.720 --> 00:47:53.779
Inderjit Dhillon: Is the

638
00:47:54.120 --> 00:47:55.429
Inderjit Dhillon: Y. Vector

639
00:47:56.910 --> 00:47:58.849
Inderjit Dhillon: What is x? Transpose what?

640
00:48:00.410 --> 00:48:02.240
Inderjit Dhillon: Sorry. X. Transpose? W:

641
00:48:05.600 --> 00:48:11.269
Inderjit Dhillon: Okay? Well, one way view is, hey? You're multiplying the matrix. X: Transpose with W:

642
00:48:11.980 --> 00:48:15.120
Inderjit Dhillon: Yeah. But there is a nice geometric way

643
00:48:15.270 --> 00:48:19.429
Inderjit Dhillon: of thinking about it, right? So that X transpose W

644
00:48:20.760 --> 00:48:24.280
Inderjit Dhillon: is. And this is where kind of you know, more

645
00:48:24.370 --> 00:48:26.560
Inderjit Dhillon: linear algebra concepts come in.

646
00:48:27.310 --> 00:48:29.619
Inderjit Dhillon: This is a linear combination

647
00:48:32.580 --> 00:48:33.520
Inderjit Dhillon: of

648
00:48:33.770 --> 00:48:35.339
Inderjit Dhillon: the columns

649
00:48:37.740 --> 00:48:38.629
Inderjit Dhillon: of

650
00:48:39.170 --> 00:48:40.569
Inderjit Dhillon: X. Transpose.

651
00:48:42.580 --> 00:48:49.310
Inderjit Dhillon: Okay, remember that X transpose is well, what what's the size of X? Transpose

652
00:48:49.800 --> 00:48:51.439
Inderjit Dhillon: it is, and

653
00:48:52.010 --> 00:48:52.990
Inderjit Dhillon: bye

654
00:48:53.100 --> 00:48:54.219
Inderjit Dhillon: d plus one.

655
00:48:56.470 --> 00:48:59.799
Inderjit Dhillon: So there are d plus one columns in X Transpose

656
00:49:00.330 --> 00:49:02.410
Inderjit Dhillon: and X transpose W.

657
00:49:02.480 --> 00:49:05.240
Inderjit Dhillon: Represents a linear combination

658
00:49:05.580 --> 00:49:10.270
Inderjit Dhillon: of the columns of X transpose right? And what is the linear combination? It is

659
00:49:10.430 --> 00:49:11.729
Inderjit Dhillon: W. Not

660
00:49:11.900 --> 00:49:14.699
Inderjit Dhillon: does so sorry. W. Not

661
00:49:18.450 --> 00:49:22.170
Inderjit Dhillon: W. Not Times column one

662
00:49:22.670 --> 00:49:26.760
Inderjit Dhillon: plus w one times column 2

663
00:49:27.600 --> 00:49:33.520
Inderjit Dhillon: all the way to Wt plus one times column D: one.

664
00:49:34.440 --> 00:49:40.119
Inderjit Dhillon: Okay, and that's of course, of the matrix. X Transpose extends both as D plus one calls.

665
00:49:40.900 --> 00:49:41.740
Inderjit Dhillon: Okay.

666
00:49:42.890 --> 00:49:44.069
Inderjit Dhillon: So

667
00:49:45.630 --> 00:49:53.209
Inderjit Dhillon: I can try and think about like visualizing this space of X Transpose W.

668
00:49:54.130 --> 00:49:55.700
Inderjit Dhillon: And that is called

669
00:49:55.790 --> 00:49:58.939
Inderjit Dhillon: the range space of X. Transpose.

670
00:49:59.310 --> 00:50:07.699
Inderjit Dhillon: Okay, the column space of X transpose or the green space of okay. So let me. You know it's a linear space.

671
00:50:08.210 --> 00:50:11.589
Inderjit Dhillon: So I'm: using this

672
00:50:12.050 --> 00:50:15.809
Inderjit Dhillon: visualization to say that this is the range space of

673
00:50:16.980 --> 00:50:18.410
Inderjit Dhillon: X transpose.

674
00:50:19.920 --> 00:50:22.350
Inderjit Dhillon: Okay. So any vector over here

675
00:50:23.000 --> 00:50:26.110
Inderjit Dhillon: is, what do I mean by the range space?

676
00:50:26.240 --> 00:50:32.889
Inderjit Dhillon: Any vector in this space is a linear combination of the columns of expansible. So

677
00:50:33.340 --> 00:50:37.660
Inderjit Dhillon: it basically, is represented by X. Transpose W.

678
00:50:37.760 --> 00:50:41.299
Inderjit Dhillon: And as W. Varies through all possible values.

679
00:50:41.420 --> 00:50:46.110
Inderjit Dhillon: that's what means that it is the range space of

680
00:50:46.690 --> 00:50:47.399
okay.

681
00:50:47.770 --> 00:50:48.720
Inderjit Dhillon: Now.

682
00:50:50.920 --> 00:50:52.719
Inderjit Dhillon: why is another? Vector

683
00:50:53.630 --> 00:50:57.049
Inderjit Dhillon: does? Why belong to the range space of X. Transpose?

684
00:50:58.650 --> 00:51:00.299
Inderjit Dhillon: But not necessarily.

685
00:51:00.890 --> 00:51:05.670
Inderjit Dhillon: What would it mean? For Why, to belong to the rain space of X transpose.

686
00:51:06.640 --> 00:51:09.860
Inderjit Dhillon: That would mean that there exists a linear combination

687
00:51:10.770 --> 00:51:15.110
Inderjit Dhillon: of the columns of X transpose so that it is exactly equal

688
00:51:15.280 --> 00:51:16.250
Inderjit Dhillon: to why.

689
00:51:16.780 --> 00:51:21.020
Inderjit Dhillon: which means that x transpose y would be equal to

690
00:51:21.270 --> 00:51:22.870
Inderjit Dhillon: X. Transpose W.

691
00:51:22.920 --> 00:51:25.549
Inderjit Dhillon: Would be equal to W for some

692
00:51:25.790 --> 00:51:26.929
W.

693
00:51:28.220 --> 00:51:30.749
Inderjit Dhillon: But that, of course, may not happen in general.

694
00:51:31.050 --> 00:51:34.700
Inderjit Dhillon: So this vector Y: sorry or

695
00:51:42.520 --> 00:51:47.270
Inderjit Dhillon: okay, this is the origin. So this vector Y is something for sorry it's. Again.

696
00:51:47.730 --> 00:51:51.090
Inderjit Dhillon: this vector. Y is some vector

697
00:51:53.090 --> 00:51:53.879
Inderjit Dhillon: Okay.

698
00:51:54.100 --> 00:51:54.959
Inderjit Dhillon: and

699
00:51:55.220 --> 00:51:58.070
Inderjit Dhillon: what minimizing the lead, the

700
00:51:58.210 --> 00:52:02.089
Inderjit Dhillon: least squares, the does, or the least squares objective

701
00:52:02.310 --> 00:52:04.749
Inderjit Dhillon: essentially corresponds to.

702
00:52:04.970 --> 00:52:13.430
Inderjit Dhillon: You know it's a square. The square is associated with the Pythagoras theorem. So what it is saying is that I want to find a projection

703
00:52:14.520 --> 00:52:17.300
Inderjit Dhillon: on to the rain space of

704
00:52:17.660 --> 00:52:19.149
Inderjit Dhillon: X transpose.

705
00:52:19.700 --> 00:52:21.979
Inderjit Dhillon: and that is

706
00:52:22.250 --> 00:52:23.899
Inderjit Dhillon: X. Transpose

707
00:52:24.450 --> 00:52:25.810
Inderjit Dhillon: W. Star.

708
00:52:26.440 --> 00:52:30.410
Inderjit Dhillon: and by the Pythagorean theorem this should be 90 degrees.

709
00:52:32.060 --> 00:52:37.199
Inderjit Dhillon: This is the residual. So this is Y. Minus x transpose

710
00:52:37.520 --> 00:52:38.680
Inderjit Dhillon: W. Still.

711
00:52:39.900 --> 00:52:41.770
Inderjit Dhillon: So what I want is

712
00:52:43.100 --> 00:52:44.080
Inderjit Dhillon: that

713
00:52:45.440 --> 00:52:47.309
Inderjit Dhillon: for all W.

714
00:52:48.280 --> 00:52:50.369
Inderjit Dhillon: So this is X. Transpose w.

715
00:52:50.950 --> 00:52:53.879
Inderjit Dhillon: and as W. Varies you get the space

716
00:52:54.480 --> 00:52:56.309
Inderjit Dhillon: that this should be

717
00:52:56.330 --> 00:52:58.639
Inderjit Dhillon: perpendicular to

718
00:52:58.660 --> 00:53:02.670
Inderjit Dhillon: y minus x. Transpose W. Star

719
00:53:03.490 --> 00:53:04.569
Inderjit Dhillon: for all.

720
00:53:04.740 --> 00:53:05.689
Inderjit Dhillon: No deal.

721
00:53:05.980 --> 00:53:08.070
Inderjit Dhillon: and there is to

722
00:53:08.480 --> 00:53:10.220
Inderjit Dhillon: minimize

723
00:53:13.700 --> 00:53:15.019
Inderjit Dhillon: of W.

724
00:53:17.840 --> 00:53:20.839
Inderjit Dhillon: Our is a

725
00:53:21.760 --> 00:53:23.109
Inderjit Dhillon: of

726
00:53:26.290 --> 00:53:28.619
Inderjit Dhillon: so geometrically. That's what it means

727
00:53:29.260 --> 00:53:31.510
Inderjit Dhillon: that this residual vector

728
00:53:31.930 --> 00:53:35.189
Inderjit Dhillon: is actually orthogonal or perpendicular.

729
00:53:35.350 --> 00:53:37.440
Inderjit Dhillon: 2. X. Transpose W. Star.

730
00:53:37.470 --> 00:53:39.939
Inderjit Dhillon: Well, what does it mean for one vector

731
00:53:40.050 --> 00:53:42.210
Inderjit Dhillon: to be orthogonal to another vector

732
00:53:43.180 --> 00:53:43.910
Inderjit Dhillon: right.

733
00:53:44.270 --> 00:53:49.679
Inderjit Dhillon: That means that the inner product between those 2 vectors must be equal to 0.

734
00:53:50.870 --> 00:53:52.490
Inderjit Dhillon: So that means that

735
00:53:52.720 --> 00:53:55.599
Inderjit Dhillon: I take X. Transpose W.

736
00:53:56.920 --> 00:54:00.319
Inderjit Dhillon: And the inner product of this. With

737
00:54:00.680 --> 00:54:03.780
Inderjit Dhillon: Y minus X. Transpose W. Star.

738
00:54:05.120 --> 00:54:06.520
Inderjit Dhillon: it should be equal to 0.

739
00:54:09.140 --> 00:54:14.230
Inderjit Dhillon: Okay, this is the inner product of X. Transpose W with the residual, vector

740
00:54:14.280 --> 00:54:16.969
Inderjit Dhillon: and this should be 0 for all.

741
00:54:18.990 --> 00:54:22.040
Inderjit Dhillon: So what that means again, we do.

742
00:54:22.480 --> 00:54:29.209
Inderjit Dhillon: You know the If I have extense was W. If I take the transpose of it. It is W. Transpose x

743
00:54:29.700 --> 00:54:33.579
Inderjit Dhillon: times y minus x transpose W. Star

744
00:54:33.640 --> 00:54:35.250
Inderjit Dhillon: must be equal to 0.

745
00:54:35.480 --> 00:54:39.870
Inderjit Dhillon: I can put this X inside, so I have x W. Transpose

746
00:54:40.220 --> 00:54:45.539
Inderjit Dhillon: x, y minus x x transpose W. Star equals 0.

747
00:54:46.140 --> 00:54:49.150
Inderjit Dhillon: This is for all W. For all W.

748
00:54:49.430 --> 00:54:52.789
Inderjit Dhillon: And since it is true for all W. This means that

749
00:54:52.920 --> 00:54:54.260
Inderjit Dhillon: this term

750
00:54:54.340 --> 00:54:55.650
Inderjit Dhillon: must be 0,

751
00:54:56.730 --> 00:55:00.870
Inderjit Dhillon: which means that X. X. Transpose of W. Star

752
00:55:01.120 --> 00:55:03.519
Inderjit Dhillon: must be equal to x one.

753
00:55:10.620 --> 00:55:15.290
Inderjit Dhillon: Now, this better be equal to the same thing that we got before. So just take a quick look at it.

754
00:55:15.580 --> 00:55:19.149
Inderjit Dhillon: It's x x. Transpose W. Star minus x y.

755
00:55:20.180 --> 00:55:22.860
Inderjit Dhillon: and when we are taking the derivative

756
00:55:23.100 --> 00:55:25.409
Inderjit Dhillon: the gradient approach and set it to 0,

757
00:55:25.640 --> 00:55:26.759
Inderjit Dhillon: it's the same

758
00:55:27.210 --> 00:55:28.910
Inderjit Dhillon: right I have.

759
00:55:29.710 --> 00:55:31.669
Inderjit Dhillon: Xx. Transpose

760
00:55:31.880 --> 00:55:34.439
Inderjit Dhillon: W. Star equal to X Y,

761
00:55:35.050 --> 00:55:38.400
Inderjit Dhillon: which is identical to what we got. That's good, right? So

762
00:55:39.140 --> 00:55:41.909
Inderjit Dhillon: But what I've shown you is basically 2

763
00:55:42.130 --> 00:55:46.020
Inderjit Dhillon: approaches different approaches. One is more calculus oriented.

764
00:55:46.250 --> 00:55:49.000
Inderjit Dhillon: The other is a geometric viewpoint.

765
00:55:49.460 --> 00:55:59.129
Inderjit Dhillon: right? Which basically gets you that the solution to the least squares problem or the linearly squares problem is given by the normal equations.

766
00:56:00.760 --> 00:56:03.180
Inderjit Dhillon: Okay? So we again

767
00:56:05.060 --> 00:56:07.290
Inderjit Dhillon: about the same thing, which is good.

768
00:56:10.700 --> 00:56:12.870
Inderjit Dhillon: But this was derived.

769
00:56:16.310 --> 00:56:17.229
Inderjit Dhillon: Oh.

770
00:56:18.260 --> 00:56:19.819
Inderjit Dhillon: geometrics.

771
00:56:28.960 --> 00:56:29.759
Inderjit Dhillon: Okay.

772
00:56:33.190 --> 00:56:34.450
Inderjit Dhillon: questions.

773
00:56:38.870 --> 00:56:42.680
Inderjit Dhillon: So what we did was we took the regression problem.

774
00:56:43.710 --> 00:56:46.249
Inderjit Dhillon: We looked at squared loss or squared error.

775
00:56:47.240 --> 00:56:50.960
Inderjit Dhillon: We formulated an objective

776
00:56:52.270 --> 00:56:59.350
Inderjit Dhillon: approach. One. We took the gradient and we were able to. That's the more general approach. That's the approach we will generally be using.

777
00:57:00.060 --> 00:57:04.760
Inderjit Dhillon: and we set it to 0, and we found that the normal equations gives us the coefficient. Vector

778
00:57:05.880 --> 00:57:09.829
Inderjit Dhillon: then I showed you a very kind of different kind of geometric viewpoint.

779
00:57:10.550 --> 00:57:11.870
Inderjit Dhillon: which also

780
00:57:12.500 --> 00:57:14.559
Inderjit Dhillon: gave the same conclusion.

781
00:57:16.530 --> 00:57:17.640
Inderjit Dhillon: So now

782
00:57:17.970 --> 00:57:19.870
Inderjit Dhillon: you know that

783
00:57:20.850 --> 00:57:23.160
Inderjit Dhillon: to solve the linearly squares problem.

784
00:57:23.420 --> 00:57:25.220
Inderjit Dhillon: All you need to do is

785
00:57:25.270 --> 00:57:27.299
Inderjit Dhillon: solve this equation.

786
00:57:27.820 --> 00:57:30.130
Inderjit Dhillon: And, like I said, this is

787
00:57:30.520 --> 00:57:31.890
Inderjit Dhillon: d one

788
00:57:31.930 --> 00:57:33.299
Inderjit Dhillon: equations.

789
00:57:35.420 --> 00:57:37.309
Inderjit Dhillon: and remember what our unknowns are.

790
00:57:40.430 --> 00:57:42.990
Inderjit Dhillon: There w not w one

791
00:57:43.530 --> 00:57:45.450
Inderjit Dhillon: twowd.

792
00:57:45.570 --> 00:57:47.360
Inderjit Dhillon: so there are d plus one.

793
00:57:47.910 --> 00:57:48.959
I don't

794
00:58:01.530 --> 00:58:03.849
Inderjit Dhillon: any questions so far.

795
00:58:10.190 --> 00:58:11.859
Inderjit Dhillon: You're free to ask questions.

796
00:58:15.500 --> 00:58:17.149
Inderjit Dhillon: Have people seen this before?

797
00:58:29.970 --> 00:58:30.709
Inderjit Dhillon: Well.

798
00:58:30.890 --> 00:58:33.489
Inderjit Dhillon: there's a reason why I'm. Kind of. You know.

799
00:58:33.880 --> 00:58:38.819
Inderjit Dhillon: I have taken these 2 different approaches and kind of

800
00:58:39.120 --> 00:58:40.759
Inderjit Dhillon: included a lot of

801
00:58:40.870 --> 00:58:42.529
Inderjit Dhillon: matrix algebra

802
00:58:42.580 --> 00:58:45.610
Inderjit Dhillon: in this first lecture. This is just to kind of

803
00:58:46.990 --> 00:58:48.529
Inderjit Dhillon: reinforced you.

804
00:58:48.930 --> 00:58:51.259
Inderjit Dhillon: But there's a lot of

805
00:58:51.420 --> 00:58:54.809
Inderjit Dhillon: linear algebra that comes up in

806
00:58:55.200 --> 00:58:56.300
Inderjit Dhillon: machine learning

807
00:58:57.280 --> 00:59:00.969
Inderjit Dhillon: right? And so there are these questions. For example.

808
00:59:01.490 --> 00:59:06.919
Inderjit Dhillon: when could there be a problem in solving these normal equations?

809
00:59:07.660 --> 00:59:17.459
Inderjit Dhillon: And then I could start asking these questions like, oh, actually, how do I solve these normal equations? How do I solve the system of deep plus one? Yeah, unknowns

810
00:59:17.600 --> 00:59:20.939
Inderjit Dhillon: d plus one equations and d one unknowns.

811
00:59:23.270 --> 00:59:27.209
Inderjit Dhillon: So anyone tell me how I would go about solving this.

812
00:59:28.350 --> 00:59:31.499
Inderjit Dhillon: So remember now we need to solve

813
00:59:34.130 --> 00:59:38.989
Inderjit Dhillon: X. X. Transpose W. Star is equal to X Times Y.

814
00:59:39.460 --> 00:59:40.909
Inderjit Dhillon: I need to solve

815
00:59:42.510 --> 00:59:43.509
Inderjit Dhillon: 4

816
00:59:43.570 --> 00:59:44.930
Inderjit Dhillon: W. Star

817
00:59:46.100 --> 00:59:48.990
Inderjit Dhillon: X. X. Transpose Y are given.

818
00:59:49.710 --> 00:59:51.060
Inderjit Dhillon: So I have

819
00:59:51.910 --> 00:59:53.870
Inderjit Dhillon: a linear system of equations.

820
01:00:04.750 --> 01:00:08.809
Inderjit Dhillon: Why do we want to try and make this a little bit more interactive because

821
01:00:09.010 --> 01:00:11.339
Inderjit Dhillon: i'm getting a little kind of

822
01:00:11.960 --> 01:00:15.560
Inderjit Dhillon: worried that i'm just speaking, and nobody is

823
01:00:17.490 --> 01:00:18.899
Inderjit Dhillon: able to follow.

824
01:00:19.380 --> 01:00:23.480
William Recktenwald: You could use a substitution of the equations.

825
01:00:25.030 --> 01:00:29.160
Inderjit Dhillon: Yes, so I have d plus one equations, and I have

826
01:00:29.920 --> 01:00:35.819
Inderjit Dhillon: d one unknown. So you're saying, I basically use substitution right?

827
01:00:36.040 --> 01:00:37.069
William Recktenwald: Yes.

828
01:00:37.120 --> 01:00:39.620
Inderjit Dhillon: can you give more details.

829
01:00:39.990 --> 01:00:43.469
Inderjit Dhillon: Suppose I actually gave this to you for homework? I I said, hey.

830
01:00:43.610 --> 01:00:45.890
Inderjit Dhillon: there are 10 training points.

831
01:00:46.560 --> 01:00:47.339
Inderjit Dhillon: right?

832
01:00:47.490 --> 01:00:50.520
Inderjit Dhillon: And I have

833
01:00:51.910 --> 01:00:53.859
Inderjit Dhillon: let's say d is 5.

834
01:00:55.500 --> 01:00:59.650
Inderjit Dhillon: So that means I would have a 6 by 6 matrix.

835
01:01:00.850 --> 01:01:01.729
William Recktenwald: Yes.

836
01:01:01.840 --> 01:01:20.090
William Recktenwald: I mean it. It gets messy. But if you just, you know, if you just have x and y right or 2 2 variables. Let's talk all the next, and why, let's call them x, one x, 2, you know we've all done that in a a middle school algebra for us right just, you know, substituted in. So I mean, I think you can do that for the same system. It just, you know.

837
01:01:20.100 --> 01:01:22.520
William Recktenwald: would be a very long process.

838
01:01:22.850 --> 01:01:26.630
Inderjit Dhillon: Right? Exactly. And remember now, we are talking about

839
01:01:27.450 --> 01:01:29.550
Inderjit Dhillon: modern day machine learning.

840
01:01:29.700 --> 01:01:38.169
Inderjit Dhillon: We have not d equal to 5, but realistically de in the thousands, if not 10,000. 100,000

841
01:01:38.600 --> 01:01:42.180
Inderjit Dhillon: And i'm looking at training data points which are.

842
01:01:42.410 --> 01:01:44.370
Inderjit Dhillon: you know, it could be in the millions

843
01:01:44.870 --> 01:01:50.369
Inderjit Dhillon: right? So we need like a systematic way to to do it.

844
01:01:50.530 --> 01:01:54.180
Inderjit Dhillon: So I see, like one suggestion from Chris.

845
01:01:54.440 --> 01:01:58.370
Inderjit Dhillon: Let me see to perform row operations to get in.

846
01:01:59.600 --> 01:02:02.310
Inderjit Dhillon: I do session on form

847
01:02:02.840 --> 01:02:05.519
Inderjit Dhillon: row, and reduce session on. Yes.

848
01:02:05.870 --> 01:02:12.660
Inderjit Dhillon: So now you see that linear algebra really kind of starts coming in right. So

849
01:02:14.040 --> 01:02:15.760
Inderjit Dhillon: we want to try and find out

850
01:02:16.070 --> 01:02:18.160
Inderjit Dhillon: what are various ways to solve it.

851
01:02:31.720 --> 01:02:36.530
Inderjit Dhillon: What you all are, both both sessions that we have so far

852
01:02:36.710 --> 01:02:38.830
Inderjit Dhillon: is essentially to do

853
01:02:39.100 --> 01:02:41.860
Inderjit Dhillon: what's called Gaussian elimination

854
01:02:56.390 --> 01:02:57.689
Inderjit Dhillon: to solve.

855
01:02:57.740 --> 01:03:00.790
Inderjit Dhillon: Xx. Transpose W. Star

856
01:03:01.140 --> 01:03:02.709
Inderjit Dhillon: is equal to X, one.

857
01:03:02.860 --> 01:03:04.799
Inderjit Dhillon: Okay, what that means is

858
01:03:05.030 --> 01:03:06.270
Inderjit Dhillon: you form

859
01:03:07.710 --> 01:03:10.109
Inderjit Dhillon: the matrix. Xx: transpose.

860
01:03:10.400 --> 01:03:12.439
Inderjit Dhillon: and we will just write it explicitly.

861
01:03:14.390 --> 01:03:15.479
Inderjit Dhillon: The

862
01:03:16.490 --> 01:03:19.959
Inderjit Dhillon: matrix. Xx: transpose.

863
01:03:20.510 --> 01:03:23.340
Inderjit Dhillon: Okay, then you form

864
01:03:24.570 --> 01:03:25.689
Inderjit Dhillon: the

865
01:03:25.770 --> 01:03:27.379
Inderjit Dhillon: right inside.

866
01:03:31.420 --> 01:03:33.469
Inderjit Dhillon: which is X. Y.

867
01:03:34.440 --> 01:03:38.869
Inderjit Dhillon: Suppose I say that this is equal to a, and this is equal to B,

868
01:03:39.420 --> 01:03:44.409
Inderjit Dhillon: and then what I need to do is solve a times W.

869
01:03:45.780 --> 01:03:46.649
Inderjit Dhillon: Star

870
01:03:48.040 --> 01:03:49.789
Inderjit Dhillon: equal to B.

871
01:03:51.750 --> 01:03:53.789
Inderjit Dhillon: And the way I would do it is

872
01:03:54.130 --> 01:03:55.570
Inderjit Dhillon: I would take a.

873
01:03:56.910 --> 01:04:01.540
Inderjit Dhillon: and I would do Boston. Elimination and gossip. Elimination can be thought of as

874
01:04:01.790 --> 01:04:03.419
Inderjit Dhillon: a is equal to

875
01:04:04.050 --> 01:04:06.029
Inderjit Dhillon: like an earlier decomposition.

876
01:04:15.440 --> 01:04:16.520
Inderjit Dhillon: But

877
01:04:17.500 --> 01:04:21.969
Inderjit Dhillon: it turns out that this particular a is equal to X. X Transpose

878
01:04:22.310 --> 01:04:24.300
Inderjit Dhillon: it's actually a special matrix.

879
01:04:25.270 --> 01:04:27.279
Inderjit Dhillon: Okay, it's what's called

880
01:04:28.640 --> 01:04:29.830
Inderjit Dhillon: positive

881
01:04:31.930 --> 01:04:33.779
Inderjit Dhillon: semi-definite matrix.

882
01:04:38.240 --> 01:04:39.660
Inderjit Dhillon: So it's symmetric

883
01:04:39.750 --> 01:04:41.530
Inderjit Dhillon: with the symmetric matrix

884
01:04:41.880 --> 01:04:44.459
Inderjit Dhillon: moreover it's what's called a positive

885
01:04:44.540 --> 01:04:48.349
Inderjit Dhillon: semi definite matrix we'll come to what that means a little bit later.

886
01:04:48.730 --> 01:04:51.569
Inderjit Dhillon: But what that essentially means is that it's

887
01:04:51.960 --> 01:04:56.729
Inderjit Dhillon: when you have done gossip and elimination, and so on, or L. You decomposition.

888
01:04:57.820 --> 01:05:01.229
Inderjit Dhillon: and your decomposition is also called elimination.

889
01:05:08.540 --> 01:05:17.319
Inderjit Dhillon: Then it turns out I don't know if many of you like. When you numerically solve this, you can get trouble like you can get like 0 pivots, and so on.

890
01:05:17.680 --> 01:05:23.029
Inderjit Dhillon: And positive, definite matrices are ones where you actually don't get this. And the

891
01:05:23.420 --> 01:05:26.609
Inderjit Dhillon: a new decomposition basically translates to

892
01:05:26.750 --> 01:05:29.549
Inderjit Dhillon: a is equal to L. L. Transpose.

893
01:05:31.760 --> 01:05:33.600
Inderjit Dhillon: and this, for

894
01:05:34.600 --> 01:05:39.049
Inderjit Dhillon: all the definite matrices, is also called, as Kolesky Decomposition.

895
01:05:40.230 --> 01:05:42.419
That's key decomposition.

896
01:05:44.520 --> 01:05:46.940
Inderjit Dhillon: Apparently Koleski was

897
01:05:47.630 --> 01:05:50.799
Inderjit Dhillon: a general in the Russian army.

898
01:05:51.240 --> 01:05:52.580
Inderjit Dhillon: I think, in the

899
01:05:53.360 --> 01:05:54.479
Inderjit Dhillon: nineteenth century

900
01:05:56.260 --> 01:05:57.449
Inderjit Dhillon: I'll have to check that.

901
01:05:59.170 --> 01:06:05.009
Inderjit Dhillon: Okay. So you form ll transpose. You get L. L. Transpose Times. W. Star

902
01:06:05.260 --> 01:06:06.750
Inderjit Dhillon: is equal to

903
01:06:06.920 --> 01:06:07.899
Inderjit Dhillon: B.

904
01:06:09.490 --> 01:06:14.649
Inderjit Dhillon: So this is the elimination. Sorry this is the Gaussian elimination phase.

905
01:06:14.960 --> 01:06:17.609
Inderjit Dhillon: Then you can basically

906
01:06:18.050 --> 01:06:21.080
Inderjit Dhillon: think of it as L of

907
01:06:21.750 --> 01:06:24.860
Inderjit Dhillon: I don't want to use y, so let me use Z

908
01:06:25.000 --> 01:06:26.379
Inderjit Dhillon: is equal to B

909
01:06:26.920 --> 01:06:30.930
Inderjit Dhillon: and L. Transpose W. Star.

910
01:06:32.650 --> 01:06:34.399
Inderjit Dhillon: So once you get this.

911
01:06:37.050 --> 01:06:38.069
Inderjit Dhillon: then

912
01:06:39.590 --> 01:06:41.209
Inderjit Dhillon: I know B.

913
01:06:41.330 --> 01:06:43.010
Inderjit Dhillon: So I can solve this.

914
01:06:45.210 --> 01:06:47.679
Inderjit Dhillon: and L is a lower triangular matrix.

915
01:06:48.360 --> 01:06:49.729
Inderjit Dhillon: It is

916
01:06:49.780 --> 01:06:51.229
Inderjit Dhillon: lower.

917
01:06:56.270 --> 01:06:58.350
Inderjit Dhillon: Okay? And you is

918
01:06:59.320 --> 01:07:00.410
Inderjit Dhillon: upper, right?

919
01:07:08.600 --> 01:07:16.510
Inderjit Dhillon: Okay? So when I have a lower triangular matrix. So I this system without this form, lower, triangular, vector

920
01:07:16.720 --> 01:07:20.559
Inderjit Dhillon: I can solve this by forward substitution. It's actually pretty easy.

921
01:07:26.160 --> 01:07:28.200
Inderjit Dhillon: I look at the first equation.

922
01:07:28.270 --> 01:07:30.020
Inderjit Dhillon: The solution is simple.

923
01:07:30.250 --> 01:07:36.419
Inderjit Dhillon: Then I substitute the value of the one into the second equation I can solve for Z 2, and so on.

924
01:07:36.850 --> 01:07:38.920
Inderjit Dhillon: So that way I can find out. See?

925
01:07:39.500 --> 01:07:43.460
Inderjit Dhillon: And now this L. Transpose is

926
01:07:45.480 --> 01:07:46.629
Inderjit Dhillon: up there trying to.

927
01:07:48.150 --> 01:07:49.599
Inderjit Dhillon: So once I know Z,

928
01:07:49.930 --> 01:07:51.769
Inderjit Dhillon: I can actually now solve

929
01:07:52.070 --> 01:07:53.600
Inderjit Dhillon: for W. Star

930
01:07:54.030 --> 01:07:56.600
Inderjit Dhillon: by backward.

931
01:08:07.280 --> 01:08:08.839
Inderjit Dhillon: so I can solve this.

932
01:08:09.840 --> 01:08:12.010
Inderjit Dhillon: What is the cost of solving this?

933
01:08:17.760 --> 01:08:22.119
Inderjit Dhillon: So remember I had n training points, each of them 3 dimensions.

934
01:08:27.010 --> 01:08:29.650
Inderjit Dhillon: How much does it cost me to do this?

935
01:08:30.229 --> 01:08:32.469
Inderjit Dhillon: How many operations does it take?

936
01:08:35.689 --> 01:08:37.210
Inderjit Dhillon: Remember that a.

937
01:08:38.410 --> 01:08:39.760
Inderjit Dhillon: Is.

938
01:08:40.220 --> 01:08:43.139
Inderjit Dhillon: remember a is equal to xx transpose.

939
01:08:43.520 --> 01:08:47.019
Inderjit Dhillon: and this is d plus one by d plus one.

940
01:08:52.920 --> 01:08:55.799
Inderjit Dhillon: Well forming a. So what are the different steps you need to do?

941
01:08:56.390 --> 01:08:58.260
Inderjit Dhillon: You need to form this?

942
01:09:02.529 --> 01:09:04.110
Inderjit Dhillon: How much does this cost.

943
01:09:04.229 --> 01:09:09.150
Inderjit Dhillon: How many operations, how many multiplications, how many additions do you need to do?

944
01:09:14.189 --> 01:09:18.220
Inderjit Dhillon: Remember that X. X. Transpose is d one by d plus one.

945
01:09:18.910 --> 01:09:20.580
Inderjit Dhillon: So this will cost

946
01:09:21.189 --> 01:09:23.670
Inderjit Dhillon: D square times that.

947
01:09:29.090 --> 01:09:31.340
Inderjit Dhillon: and then this Gaussian elimination

948
01:09:31.649 --> 01:09:34.600
Inderjit Dhillon: forming a is equal to L. Transpose.

949
01:09:35.240 --> 01:09:36.679
Inderjit Dhillon: This

950
01:09:36.970 --> 01:09:38.340
Inderjit Dhillon: will be

951
01:09:38.840 --> 01:09:39.889
Inderjit Dhillon: order

952
01:09:40.060 --> 01:09:40.929
Inderjit Dhillon: they killed.

953
01:09:45.720 --> 01:09:49.610
Inderjit Dhillon: and this will be order D square.

954
01:09:50.000 --> 01:09:51.170
Inderjit Dhillon: These are cheaper

955
01:09:51.430 --> 01:09:52.649
Inderjit Dhillon: order. D score.

956
01:09:55.270 --> 01:09:57.990
Inderjit Dhillon: Okay. So I can basically solve

957
01:09:59.700 --> 01:10:01.769
Inderjit Dhillon: in order

958
01:10:02.380 --> 01:10:05.050
Inderjit Dhillon: the Q plus d square n times.

959
01:10:10.870 --> 01:10:14.729
Inderjit Dhillon: So now you can start making, you know, like if the

960
01:10:15.150 --> 01:10:16.609
Inderjit Dhillon: it's very large.

961
01:10:17.700 --> 01:10:24.809
Inderjit Dhillon: Then for like, for example, if you have a problem, what does Dq. Mean? If you have a problem, and you increase

962
01:10:25.110 --> 01:10:32.049
Inderjit Dhillon: it by the problem increases by a factor of 2 to double the size of the problem. You double the dimensionality.

963
01:10:33.110 --> 01:10:37.460
Inderjit Dhillon: If you solve this, the time taken will increase by a factor of

964
01:10:37.740 --> 01:10:38.510
Inderjit Dhillon: 8.

965
01:10:39.360 --> 01:10:42.910
Inderjit Dhillon: That's what cubic means. If it was square complexity

966
01:10:42.990 --> 01:10:45.149
Inderjit Dhillon: it would have been by a factor of

967
01:10:45.260 --> 01:10:46.030
4,

968
01:10:48.430 --> 01:10:51.320
Inderjit Dhillon: but it is linear in the number of data points.

969
01:10:53.080 --> 01:10:57.440
Inderjit Dhillon: So if I increase the number of data points by a factor of 2,

970
01:10:58.490 --> 01:11:02.029
Inderjit Dhillon: the time to solve. It is going to increase by a factor of 2,

971
01:11:05.860 --> 01:11:07.470
Inderjit Dhillon: at least, the D square import.

972
01:11:08.870 --> 01:11:09.559
Okay.

973
01:11:09.840 --> 01:11:11.799
Inderjit Dhillon: So this can be quite expensive.

974
01:11:15.680 --> 01:11:18.620
Inderjit Dhillon: Well, what is the

975
01:11:19.110 --> 01:11:20.890
Inderjit Dhillon: quality of the solution.

976
01:11:22.360 --> 01:11:31.099
Inderjit Dhillon: And now the second question is, Well, what about the this is sorry One was the cost in terms of the number of operations. How much time will it take

977
01:11:31.660 --> 01:11:32.380
Inderjit Dhillon: right?

978
01:11:32.690 --> 01:11:35.599
Inderjit Dhillon: But what about the accuracy of the solution?

979
01:11:37.810 --> 01:11:40.889
Inderjit Dhillon: I mean, after all, we are doing this on the computer

980
01:11:42.300 --> 01:11:44.739
Inderjit Dhillon: with the solution we actually accurate

981
01:11:45.040 --> 01:11:46.590
Inderjit Dhillon: for my prediction problem.

982
01:11:48.700 --> 01:11:51.130
Inderjit Dhillon: That's actually a pretty complicated question.

983
01:11:52.060 --> 01:11:54.679
Inderjit Dhillon: right? But when people

984
01:11:55.080 --> 01:11:58.899
Inderjit Dhillon: let's forget about you know the quality of the prediction.

985
01:11:58.980 --> 01:12:00.880
Inderjit Dhillon: But this let's focus on.

986
01:12:01.180 --> 01:12:02.749
Inderjit Dhillon: You know what is the

987
01:12:03.420 --> 01:12:06.299
Inderjit Dhillon: W. Star that we get from this method?

988
01:12:10.370 --> 01:12:14.139
Inderjit Dhillon: What is the quality of W. Star to solve this linear system

989
01:12:14.710 --> 01:12:20.950
Inderjit Dhillon: A. W. Star equal to B or X. X. Transpose W. Star is equal to

990
01:12:23.520 --> 01:12:24.800
Inderjit Dhillon: X. Times. What

991
01:12:29.380 --> 01:12:31.030
Inderjit Dhillon: anybody have any

992
01:12:31.790 --> 01:12:33.009
Inderjit Dhillon: thoughts on it

993
01:12:37.840 --> 01:12:39.969
Inderjit Dhillon: like when I first kind of

994
01:12:40.900 --> 01:12:45.380
Inderjit Dhillon: started studying this in graduate school just like you.

995
01:12:45.410 --> 01:12:46.559
Inderjit Dhillon: most of you.

996
01:12:48.050 --> 01:12:54.700
Inderjit Dhillon: I would say. Hey, you know you provided me the software right. I'm not. I didn't write the software. You give me software. I used it.

997
01:12:55.460 --> 01:12:57.640
Inderjit Dhillon: It must be good. Must be correct, right?

998
01:12:59.710 --> 01:13:03.469
Inderjit Dhillon: But that's not really true. Right whenever you saw something on the computer.

999
01:13:03.910 --> 01:13:09.869
Inderjit Dhillon: There are multiple sources of er. One is just the fact that you are representing real numbers

1000
01:13:10.620 --> 01:13:12.180
Inderjit Dhillon: in floating point.

1001
01:13:12.980 --> 01:13:18.620
Inderjit Dhillon: All your operations are being conducted in floating point, so they are not really exact

1002
01:13:19.290 --> 01:13:24.359
Inderjit Dhillon: right? And then there is other issues that come in. For example, I talked a little bit about

1003
01:13:26.020 --> 01:13:27.649
Inderjit Dhillon: this matrix being

1004
01:13:28.350 --> 01:13:29.459
Inderjit Dhillon: non singular.

1005
01:13:32.370 --> 01:13:35.230
Inderjit Dhillon: Well, if it is non-singular, then what happens?

1006
01:13:37.080 --> 01:13:40.230
Inderjit Dhillon: That means that my matrix a is non singular.

1007
01:13:43.890 --> 01:13:45.670
Inderjit Dhillon: What happens in that case

1008
01:13:50.160 --> 01:13:51.800
Inderjit Dhillon: the solution exist.

1009
01:13:58.190 --> 01:14:01.099
Inderjit Dhillon: What if the matrix a is close to singularity?

1010
01:14:01.560 --> 01:14:05.069
Inderjit Dhillon: So it's not exactly non-singular but it is close to singularity

1011
01:14:06.130 --> 01:14:09.170
Inderjit Dhillon: in terms of a prediction problem. This might be that.

1012
01:14:10.100 --> 01:14:11.740
Inderjit Dhillon: you know one variable

1013
01:14:11.820 --> 01:14:13.519
Inderjit Dhillon: is

1014
01:14:14.380 --> 01:14:20.110
Inderjit Dhillon: very much correlated, or one feature is very much correlated to a combination of some other features.

1015
01:14:20.540 --> 01:14:25.560
Inderjit Dhillon: So, for example, if you're taking measurements that say you're taking flight and weight, and so on

1016
01:14:25.950 --> 01:14:29.330
Inderjit Dhillon: right, maybe there is a correlation between these variables.

1017
01:14:30.060 --> 01:14:36.030
Inderjit Dhillon: If they were exactly in your dependence. That means that the matrix would be exactly.

1018
01:14:37.260 --> 01:14:39.699
Inderjit Dhillon: for example, if height and weight

1019
01:14:40.040 --> 01:14:44.540
Inderjit Dhillon: for 2 variables, and they was a perfect linear relationship

1020
01:14:45.320 --> 01:14:47.430
Inderjit Dhillon: in those measurements.

1021
01:14:48.190 --> 01:14:50.249
Inderjit Dhillon: then the matrix will be exactly single.

1022
01:14:52.360 --> 01:14:55.749
Inderjit Dhillon: and then you need to be very careful when you solve this in your system.

1023
01:14:58.380 --> 01:14:59.000
Okay.

1024
01:14:59.340 --> 01:15:02.539
Inderjit Dhillon: So there is something called the condition number

1025
01:15:10.020 --> 01:15:11.519
Inderjit Dhillon: of the matrix a.

1026
01:15:13.520 --> 01:15:16.779
Inderjit Dhillon: Okay. And if the condition number is large.

1027
01:15:18.470 --> 01:15:21.800
Inderjit Dhillon: that's essentially saying that the matrix a.

1028
01:15:23.420 --> 01:15:24.990
Inderjit Dhillon: He is

1029
01:15:25.640 --> 01:15:26.960
Inderjit Dhillon: close to

1030
01:15:27.060 --> 01:15:28.210
Inderjit Dhillon: singularity.

1031
01:15:34.130 --> 01:15:34.889
Inderjit Dhillon: Okay.

1032
01:15:35.600 --> 01:15:37.189
Inderjit Dhillon: And in that case

1033
01:15:37.270 --> 01:15:40.259
Inderjit Dhillon: it turns out that this method is actually not that good.

1034
01:15:41.070 --> 01:15:48.250
Inderjit Dhillon: This method of forming the normal equation normal, just equations and then solving them

1035
01:15:48.270 --> 01:15:50.719
Inderjit Dhillon: can actually give a larger error.

1036
01:15:50.770 --> 01:15:53.420
Inderjit Dhillon: Then you can get by some other means.

1037
01:15:56.110 --> 01:15:57.559
Inderjit Dhillon: Okay, so

1038
01:15:58.290 --> 01:16:00.130
Inderjit Dhillon: normal equations

1039
01:16:05.450 --> 01:16:07.890
Inderjit Dhillon: or solving normal equations

1040
01:16:09.660 --> 01:16:11.699
Inderjit Dhillon: and yield

1041
01:16:13.010 --> 01:16:14.750
Inderjit Dhillon: large error

1042
01:16:16.050 --> 01:16:17.110
Inderjit Dhillon: when

1043
01:16:17.320 --> 01:16:18.250
Inderjit Dhillon: hey

1044
01:16:18.830 --> 01:16:20.990
Inderjit Dhillon: is poorly

1045
01:16:22.160 --> 01:16:23.059
condition.

1046
01:16:27.230 --> 01:16:29.119
Inderjit Dhillon: what is the condition? Number of a?

1047
01:16:33.530 --> 01:16:35.680
Inderjit Dhillon: But it turns out it is the

1048
01:16:35.990 --> 01:16:39.130
Inderjit Dhillon: norm of a times the norm of a and

1049
01:16:40.030 --> 01:16:42.299
Inderjit Dhillon: well, what is the norm of a matrix?

1050
01:16:42.660 --> 01:16:43.380
Okay.

1051
01:16:43.420 --> 01:16:47.330
Inderjit Dhillon: So these are kind of concepts that we

1052
01:16:47.410 --> 01:16:48.530
Inderjit Dhillon: will review

1053
01:16:49.050 --> 01:16:51.570
Inderjit Dhillon: in the next lecture. So

1054
01:16:51.870 --> 01:16:57.130
Inderjit Dhillon: I kind of have asked told you that you need a linear algebra course as a prerequisite.

1055
01:16:57.470 --> 01:17:04.090
Inderjit Dhillon: But at the same time I said that I will refresh your prerequisites to the extent I can.

1056
01:17:04.290 --> 01:17:11.250
Inderjit Dhillon: So on the next lecture, which I believe, is next Wednesday, because

1057
01:17:11.540 --> 01:17:14.350
Inderjit Dhillon: Monday is a holiday. It's modern looking day

1058
01:17:15.680 --> 01:17:23.019
Inderjit Dhillon: we will actually do a linear algebra background where I really do a very quick tool of

1059
01:17:23.250 --> 01:17:24.610
linear algebra.

1060
01:17:24.690 --> 01:17:27.459
Inderjit Dhillon: Okay, it will partly serve to kind of

1061
01:17:27.660 --> 01:17:34.359
Inderjit Dhillon: refresh your memory. And then throughout the course there might be some

1062
01:17:34.550 --> 01:17:43.680
Inderjit Dhillon: discussions that we might have, or some material we teach that will end up being using linear algebra, especially in these days, when

1063
01:17:43.800 --> 01:17:51.410
Inderjit Dhillon: you know deep learning is becoming more and more popular. It's essentially all vectors and matrices.

1064
01:17:52.860 --> 01:17:54.500
Inderjit Dhillon: Alright, Well, but

1065
01:17:54.570 --> 01:17:59.239
Inderjit Dhillon: let me just take the next 2min to kind of say that.

1066
01:17:59.600 --> 01:18:01.730
Inderjit Dhillon: Well, this can this method

1067
01:18:01.810 --> 01:18:12.619
Inderjit Dhillon: of solving the linear of solving regression problems by solving the linear solving the normal equations by Gaussian and domination can use large error.

1068
01:18:12.720 --> 01:18:13.630
Inderjit Dhillon: But

1069
01:18:13.980 --> 01:18:15.370
Inderjit Dhillon: better method

1070
01:18:18.900 --> 01:18:19.990
Inderjit Dhillon: is

1071
01:18:20.010 --> 01:18:23.230
Inderjit Dhillon: to use Svd.

1072
01:18:24.210 --> 01:18:25.050
We'll see.

1073
01:18:25.860 --> 01:18:28.789
Inderjit Dhillon: Okay, and Svd. Is the

1074
01:18:30.900 --> 01:18:32.110
Inderjit Dhillon: singular.

1075
01:18:32.640 --> 01:18:33.840
Inderjit Dhillon: Well, do you

1076
01:18:34.890 --> 01:18:36.150
Inderjit Dhillon: decomposition?

1077
01:18:38.290 --> 01:18:39.829
Inderjit Dhillon: When I say better.

1078
01:18:40.040 --> 01:18:41.160
Inderjit Dhillon: I mean

1079
01:18:42.030 --> 01:18:43.750
Inderjit Dhillon: more accurate.

1080
01:18:45.690 --> 01:18:48.349
Inderjit Dhillon: So it's not a curl.

1081
01:18:48.750 --> 01:18:53.300
Inderjit Dhillon: When you have something which is close to singular, it's just harder intrinsically

1082
01:18:53.780 --> 01:18:55.119
Inderjit Dhillon: to get the solution.

1083
01:18:55.650 --> 01:18:57.979
Inderjit Dhillon: but the Svd. Will do a better job.

1084
01:18:58.030 --> 01:18:59.810
Inderjit Dhillon: Then go some information

1085
01:18:59.830 --> 01:19:01.489
Inderjit Dhillon: on the normal equations.

1086
01:19:02.060 --> 01:19:03.669
Inderjit Dhillon: What the drawback is.

1087
01:19:04.050 --> 01:19:07.900
Inderjit Dhillon: I mean, if it's a little more accurate. Why doesn't everybody use it all the time?

1088
01:19:08.750 --> 01:19:12.230
Inderjit Dhillon: But people won't. Use it all the time, because it is

1089
01:19:12.920 --> 01:19:14.380
Inderjit Dhillon: more expensive

1090
01:19:17.670 --> 01:19:18.639
to compute.

1091
01:19:22.730 --> 01:19:25.379
Inderjit Dhillon: so it'll end up taking more time. It's still

1092
01:19:25.690 --> 01:19:29.490
Inderjit Dhillon: going to be order. Dq. But the constant is going to be

1093
01:19:29.550 --> 01:19:30.830
Inderjit Dhillon: much higher.

1094
01:19:32.600 --> 01:19:35.729
Inderjit Dhillon: Okay. So let me

1095
01:19:37.660 --> 01:19:39.080
Inderjit Dhillon: stop sharing

1096
01:19:41.640 --> 01:19:42.800
this.

1097
01:19:44.670 --> 01:19:45.480
Inderjit Dhillon: Okay.

1098
01:19:46.500 --> 01:19:51.929
Inderjit Dhillon: So that concludes the first lecture.

1099
01:19:52.200 --> 01:19:59.300
Inderjit Dhillon: Ourta is right now on a flight which I think is coming from Munich to San Francisco.

1100
01:19:59.470 --> 01:20:04.319
Inderjit Dhillon: Oh, not San Francisco to Austin, I think, or Dallas, so I don't know where he is going through.

1101
01:20:04.440 --> 01:20:06.719
but once he returns

1102
01:20:06.740 --> 01:20:18.070
Inderjit Dhillon: i'll ask him to put the video recordings, so that they can be accessed, and also kind of the lecture notes, so that you can actually take a look at them.

1103
01:20:18.240 --> 01:20:21.289
Inderjit Dhillon: But today's lecture was about new squares regression.

1104
01:20:21.400 --> 01:20:26.539
Inderjit Dhillon: and just gives you an idea about when you start looking at this particular prediction problem.

1105
01:20:26.600 --> 01:20:32.759
Inderjit Dhillon: Then various aspects of linear algebra and kind of, you know mathematical computing come into the picture.

1106
01:20:33.130 --> 01:20:39.840
Inderjit Dhillon: So on Wednesday we will review. Do a review of linear algebra.

1107
01:20:39.990 --> 01:20:43.270
Inderjit Dhillon: and i'll see you all next Wednesday.

1108
01:20:43.910 --> 01:20:48.069
William Recktenwald: Good question for you on the the the geometric motivation. Yeah.

1109
01:20:48.110 --> 01:20:51.739
William Recktenwald: When we had that explained, compared to that.

1110
01:20:51.990 --> 01:20:53.559
William Recktenwald: A. Y vector

1111
01:20:53.630 --> 01:21:01.109
William Recktenwald: with the the W. 0 that would essentially move the plane vertically. Is that how that would work?

1112
01:21:02.020 --> 01:21:06.339
Inderjit Dhillon: Not really the I actually just made it one linear system.

1113
01:21:06.530 --> 01:21:10.140
Inderjit Dhillon: So the motivation that I gave included

1114
01:21:10.650 --> 01:21:12.240
Inderjit Dhillon: included

1115
01:21:14.240 --> 01:21:15.539
W. 0.

1116
01:21:16.140 --> 01:21:26.220
Inderjit Dhillon: So when I talked about, for example, it's the linear combination of the columns of I think it was X. Transpose one of the columns of X transpose is the all one. Vector

1117
01:21:26.970 --> 01:21:27.769
William Recktenwald: Okay.

1118
01:21:28.030 --> 01:21:31.080
Inderjit Dhillon: right? That that's what kind of captures W: Not

1119
01:21:31.590 --> 01:21:39.539
Inderjit Dhillon: so. So whenever i'm talking, I'm: basically kind of including that W: Not in that one's vector index.

1120
01:21:41.060 --> 01:21:43.460
William Recktenwald: I see. Okay. So

1121
01:21:44.400 --> 01:21:45.230
William Recktenwald: okay.

1122
01:21:46.250 --> 01:21:47.000
Inderjit Dhillon: okay.

1123
01:21:47.900 --> 01:21:57.650
Inderjit Dhillon: okay. So have a great rest of the week and a great 3 day weekend, and i'll see you guys next see you all next Wednesday.

1124
01:21:58.380 --> 01:21:59.179
Inderjit Dhillon: Good bye.

WEBVTT

1
00:01:22.950 --> 00:01:24.090
Nilesh Gupta: Hello, everyone.

2
00:01:25.580 --> 00:01:29.620
Nilesh Gupta: So today i'll pick up from

3
00:01:31.190 --> 00:01:33.240
Nilesh Gupta: where we left in the last class.

4
00:01:33.500 --> 00:01:42.570
Nilesh Gupta: and in the last class we were looking at the overview of deep learning. And then we looked at neural network in detail. But

5
00:01:42.690 --> 00:01:52.400
Nilesh Gupta: we only looked at the forward propagation of neural networks that is given the inputs. How do we generate output from the neural network? And

6
00:01:53.960 --> 00:02:04.890
Nilesh Gupta: what are the building blocks for that neural network is so in this class the plan is to cover the back propagation part. basically given

7
00:02:04.910 --> 00:02:10.370
Nilesh Gupta: a call loss function. How do we adjust our weights inside our neural networks

8
00:02:10.449 --> 00:02:17.560
Nilesh Gupta: so that we are moving in the direction we are minimizing the loss.

9
00:02:17.850 --> 00:02:21.950
and that will do through back propagation. I'll

10
00:02:22.000 --> 00:02:25.560
Nilesh Gupta: first cover that. and in the Hmm.

11
00:02:25.720 --> 00:02:34.240
Nilesh Gupta: Later part of this class. I'll go into details of how to put everything that we have seen so far into practice.

12
00:02:34.270 --> 00:02:37.940
Nilesh Gupta: and more specifically how to do that in 5 torch.

13
00:02:38.950 --> 00:02:45.600
Nilesh Gupta: So some of you might be already familiar with Python, so like

14
00:02:45.660 --> 00:02:51.380
Nilesh Gupta: it might be a bit redundant for you. But still, I I think, like there are a lot of people like who

15
00:02:51.430 --> 00:02:57.880
Nilesh Gupta: don't have background on pi touch, and how in general deep learning it's written.

16
00:02:58.070 --> 00:03:06.840
Nilesh Gupta: So I I think, for them it will be helpful, and i'm hoping for others like you. There might be some bits that might be useful to you.

17
00:03:07.480 --> 00:03:13.140
Nilesh Gupta: So all right, let's get started. So last time.

18
00:03:13.400 --> 00:03:18.080
Maybe when we looked at the forward propagation.

19
00:03:18.750 --> 00:03:21.230
Nilesh Gupta: we are looking at this particular example.

20
00:03:25.600 --> 00:03:28.270
Nilesh Gupta: where, like we had an image.

21
00:03:28.670 --> 00:03:33.270
Nilesh Gupta: and this image can potentially have a cat or not.

22
00:03:33.470 --> 00:03:38.500
Nilesh Gupta: and we were converting this image into a one dimensional web to.

23
00:03:41.060 --> 00:03:42.890
Nilesh Gupta: and then we had

24
00:03:45.110 --> 00:03:47.220
Nilesh Gupta: 3 layer neural network here.

25
00:03:49.950 --> 00:03:58.190
Nilesh Gupta: Yeah. So the first layer which is the input layer. It's connected to all of the Inputs. Thank you.

26
00:04:00.560 --> 00:04:03.490
Nilesh Gupta: And there this is the hidden layer.

27
00:04:05.150 --> 00:04:07.150
Nilesh Gupta: This is connected to all of the

28
00:04:08.320 --> 00:04:10.220
Nilesh Gupta: output of the input layer.

29
00:04:11.360 --> 00:04:14.820
Nilesh Gupta: And then this is the output link

30
00:04:16.360 --> 00:04:20.860
Nilesh Gupta: which is connected to the output of the hidden layers. And

31
00:04:21.300 --> 00:04:24.500
Nilesh Gupta: they they are producing this? Why, Hat

32
00:04:26.700 --> 00:04:40.390
Nilesh Gupta: and i'll just briefly like right of the notation that we were using for the forward propagation. So oh. the we were representing the linear combination part

33
00:04:40.400 --> 00:04:43.740
Nilesh Gupta: of a particular layer by Z.

34
00:04:44.060 --> 00:04:46.880
Nilesh Gupta: So Z. One calls

35
00:04:48.290 --> 00:04:50.930
Nilesh Gupta: W. One transport

36
00:04:53.920 --> 00:04:56.500
Nilesh Gupta: x. the

37
00:04:57.760 --> 00:04:58.480
Nilesh Gupta: one.

38
00:04:59.900 --> 00:05:06.690
Nilesh Gupta: and we were representing the output after the activation function by a. So

39
00:05:09.170 --> 00:05:10.610
Nilesh Gupta: take my at all.

40
00:05:16.490 --> 00:05:17.080
Nilesh Gupta: It's like.

41
00:05:31.160 --> 00:05:33.800
Nilesh Gupta: So similarly, the other equations are

42
00:05:34.070 --> 00:05:36.130
Nilesh Gupta: 22

43
00:05:36.590 --> 00:05:37.890
Nilesh Gupta: equals.

44
00:05:39.530 --> 00:05:41.260
Nilesh Gupta: Okay to

45
00:05:57.410 --> 00:05:58.520
Nilesh Gupta: me, too.

46
00:06:25.500 --> 00:06:30.180
Nilesh Gupta: And this a 3 is essentially the output of the network which we are calling by hat.

47
00:06:31.730 --> 00:06:32.750
Nilesh Gupta: So

48
00:06:33.920 --> 00:06:40.300
Nilesh Gupta: these were the forward propagation equations like how we are coming up with our predicted probability. Why have?

49
00:06:40.580 --> 00:06:49.330
Nilesh Gupta: And now let's write the loss. So this is a binary classification problem. We were using the Binary Cross entropy loss, which is

50
00:06:50.870 --> 00:06:54.410
Nilesh Gupta: losses. I'm: I'm. Just starting this loss for one data point.

51
00:06:54.740 --> 00:06:57.410
So this is going to be

52
00:06:58.590 --> 00:06:59.610
Nilesh Gupta: right.

53
00:07:00.010 --> 00:07:00.670
I

54
00:07:15.950 --> 00:07:17.400
Nilesh Gupta: It's

55
00:07:23.030 --> 00:07:25.770
Nilesh Gupta: so. This why I here is the

56
00:07:29.100 --> 00:07:34.980
Nilesh Gupta: true label indicative, which was one if the cat was present, and it was. is the

57
00:07:36.910 --> 00:07:38.710
if the cat was not present.

58
00:07:38.970 --> 00:07:46.730
Nilesh Gupta: So this is a binding percent trophy loss. And now let's. What we want to do is we want to find

59
00:07:46.790 --> 00:07:52.790
Nilesh Gupta: the gradient of of L. With respect to each of the weight

60
00:07:52.950 --> 00:08:00.220
Nilesh Gupta: each of the parameters that we have, and by parameters like this is one parameter

61
00:08:00.420 --> 00:08:10.980
Nilesh Gupta: like these are our parameters. The weights and biases like these addition terms we call biases, and the weights that we multiply to the input like, These are the weights.

62
00:08:10.990 --> 00:08:21.600
Nilesh Gupta: So we have W. One B, one. then W, 2, B, 2, then W, 3, V. 3. These are our parameters, and for each parameter we want to tweak them.

63
00:08:21.650 --> 00:08:25.950
Nilesh Gupta: so that, like we are matching our losses many miles.

64
00:08:26.070 --> 00:08:38.890
Nilesh Gupta: So as we discussed earlier that one way of doing this is through gradient descent and for gradient descent. We need these partial derivative vector

65
00:08:38.940 --> 00:08:44.820
Nilesh Gupta: basically then and by okay. W. Hi.

66
00:08:45.440 --> 00:08:49.040
Nilesh Gupta: where I here is 1 2 4 3.

67
00:08:49.860 --> 00:08:53.940
Nilesh Gupta: So this will we want this derivative.

68
00:08:54.870 --> 00:08:56.030
Nilesh Gupta: So

69
00:08:58.830 --> 00:09:01.760
Nilesh Gupta: let's look at this network like

70
00:09:04.200 --> 00:09:17.400
Nilesh Gupta: since why hat is the final output that is directly interacting with the ground truth in the loss function. So that's it's intuitively like it's it's easier to see that like

71
00:09:17.470 --> 00:09:25.610
Nilesh Gupta: it's easier to measure like If I tweak my hat. how much will the loss will change? So?

72
00:09:25.850 --> 00:09:35.330
Nilesh Gupta: So what what I want to say here is like when we want to calculate gradients, I intuitively it will. It is easier to

73
00:09:35.520 --> 00:09:37.490
Nilesh Gupta: kind of see that.

74
00:09:38.230 --> 00:09:50.980
Nilesh Gupta: since, like the gradient, is calculated on the final most output, the loss is computed on the final most output. So we will try to go backward. We'll first compute our gradients

75
00:09:51.030 --> 00:09:55.730
for the outermost layers, and then we'll try to move backwards.

76
00:09:55.950 --> 00:10:02.710
Nilesh Gupta: So here, like in back propagation, what we said, what we'll essentially use is the

77
00:10:04.220 --> 00:10:08.400
the chain rule of a derivative calculation.

78
00:10:08.610 --> 00:10:09.780
Nilesh Gupta: and

79
00:10:10.130 --> 00:10:15.030
Nilesh Gupta: and like we'll, let's just first try to write

80
00:10:16.350 --> 00:10:28.050
Nilesh Gupta: what the gradient is going to be for the out of most parameters. So the outermost parameters are W. 3 and B 3. So let's just try to write

81
00:10:28.110 --> 00:10:29.970
Nilesh Gupta: what a den, and by

82
00:10:29.990 --> 00:10:31.530
Nilesh Gupta: Del W.

83
00:10:32.120 --> 00:10:33.510
Nilesh Gupta: She is going to be.

84
00:10:34.550 --> 00:10:35.530
Nilesh Gupta: So.

85
00:10:36.610 --> 00:10:40.810
Nilesh Gupta: So if you want to calculate this gradient, let's just write our

86
00:10:41.080 --> 00:10:43.410
loss function

87
00:10:44.360 --> 00:10:45.770
Nilesh Gupta: in terms of

88
00:10:51.820 --> 00:10:59.530
Nilesh Gupta: loss function in terms of W. 3 directly. So if you want to right our loss function, it's going to be

89
00:11:02.020 --> 00:11:06.920
Nilesh Gupta: Well, I I no both sigmoid, or

90
00:11:08.490 --> 00:11:11.190
Nilesh Gupta: that you she

91
00:11:13.160 --> 00:11:14.000
Nilesh Gupta: cool.

92
00:11:15.520 --> 00:11:17.560
Nilesh Gupta: Yeah, this was a 2,

93
00:11:17.870 --> 00:11:18.440
yes.

94
00:11:19.280 --> 00:11:20.220
Nilesh Gupta: V. 3,

95
00:11:22.580 --> 00:11:24.200
Nilesh Gupta: and then there's

96
00:11:24.440 --> 00:11:27.560
Nilesh Gupta: another term, which is one minus y I

97
00:11:29.440 --> 00:11:31.780
Nilesh Gupta: no 6 point

98
00:11:33.350 --> 00:11:35.260
Nilesh Gupta: one minus the still.

99
00:11:36.390 --> 00:11:36.960
Nilesh Gupta: But

100
00:11:51.230 --> 00:11:53.300
Nilesh Gupta: i'm running out of space here.

101
00:11:57.140 --> 00:11:57.810
Yeah.

102
00:11:59.790 --> 00:12:04.960
Nilesh Gupta: Yeah. So i'm I've just written my loss function directly in terms of W. 3,

103
00:12:05.150 --> 00:12:07.780
Nilesh Gupta: and what i'll do here is

104
00:12:09.100 --> 00:12:13.600
Nilesh Gupta: oh. i'll try to take this derivative so

105
00:12:13.620 --> 00:12:20.890
Nilesh Gupta: like Why, I doesn't depend on Wc. So we can just take it out. I'm trying to write this first first term.

106
00:12:21.250 --> 00:12:32.150
Nilesh Gupta: so this becomes why I then we have a log. So this log of anything is when we take the derivative it's one by

107
00:12:32.300 --> 00:12:35.530
Nilesh Gupta: that quantity. So well

108
00:12:35.700 --> 00:12:38.800
Nilesh Gupta: just that's second there, some minus here.

109
00:12:41.140 --> 00:12:41.730
Nilesh Gupta: Yeah.

110
00:12:42.420 --> 00:12:52.080
Nilesh Gupta: So we'll just like this will become one by log of this quantity, and this quantity sigmoid of W, 3 2 plus b 3. This is

111
00:12:52.310 --> 00:13:05.240
Nilesh Gupta: exactly a 3, as we. a 3, a sigma of Z. 3, which is particular quantity. So i'm just writing it as one by 8 3, then inside, if we go like, we have the sigmoid function.

112
00:13:05.300 --> 00:13:07.130
Nilesh Gupta: S0 0ne

113
00:13:07.780 --> 00:13:09.110
like. If you

114
00:13:09.170 --> 00:13:14.460
Nilesh Gupta: try to take the derivative of sigmoid, you will find out that Sigmoid derivative is.

115
00:13:15.000 --> 00:13:18.870
Nilesh Gupta: take my X times one minus sigmoid X.

116
00:13:19.960 --> 00:13:30.260
Nilesh Gupta: So this is a result that i'll use directly. And here we can write. So this is going to be sigmoid of.

117
00:13:32.760 --> 00:13:34.930
Nilesh Gupta: You need 3.

118
00:13:41.020 --> 00:13:41.720
Nilesh Gupta: Okay.

119
00:13:45.010 --> 00:13:54.420
Nilesh Gupta: And then there's this linear combination, and there like. If I take the derivative, this will give me just 8, a, 2,

120
00:13:56.170 --> 00:14:02.960
Nilesh Gupta: and d, 3 doesn't depend on W. 3 here, so like that, like, they will get 0 great into here.

121
00:14:03.380 --> 00:14:06.830
Nilesh Gupta: So this is the great in terms that we will get

122
00:14:12.740 --> 00:14:14.380
Nilesh Gupta: when we are at a

123
00:14:15.170 --> 00:14:18.530
taking the derivative with respect to the first.

124
00:14:18.920 --> 00:14:30.380
Nilesh Gupta: and let's see like what the second term is. Second term is the addition to this. so similarly like one minus y. I is one minus y i.

125
00:14:30.760 --> 00:14:33.440
Nilesh Gupta: and then we have one. By

126
00:14:34.000 --> 00:14:38.010
taking the derivative. Of the log will get one minus Sigma

127
00:14:38.370 --> 00:14:39.570
Nilesh Gupta: a 3.

128
00:14:41.110 --> 00:14:44.920
Nilesh Gupta: Then we, if we take the derivative of the Sigma, we will get

129
00:14:46.320 --> 00:14:48.350
Nilesh Gupta: 6 0ne, c. 3

130
00:14:49.220 --> 00:14:51.890
Nilesh Gupta: times, one minus 6, point 3,

131
00:14:58.620 --> 00:15:03.080
Nilesh Gupta: and then that, taking the derivative. With respect to the linear term, we will get

132
00:15:03.320 --> 00:15:04.800
Nilesh Gupta: well again, Get a to.

133
00:15:15.020 --> 00:15:16.050
Nilesh Gupta: So

134
00:15:18.310 --> 00:15:27.440
Nilesh Gupta: this is, and like, we can simplify some things here, basically this a 3, and Sigma Z. 3 are the same thing like a 3 is equal to Sigma Z. 3.

135
00:15:27.650 --> 00:15:29.440
Nilesh Gupta: So this can.

136
00:15:37.190 --> 00:15:37.780
Okay.

137
00:15:37.990 --> 00:15:48.370
Nilesh Gupta: So this cancels out. And here like this one minus Sigma, 3 and one minus Sigma Z. 3 cancels out, so we are left with oh.

138
00:15:48.390 --> 00:15:50.200
Nilesh Gupta: 10, and I

139
00:15:58.380 --> 00:16:00.550
Nilesh Gupta: one minus 6 1

140
00:16:00.650 --> 00:16:01.300
Nilesh Gupta: she.

141
00:16:04.260 --> 00:16:05.800
Nilesh Gupta: Next. I'll just call it

142
00:16:13.530 --> 00:16:14.590
Nilesh Gupta: time seat

143
00:16:21.950 --> 00:16:25.980
Nilesh Gupta: from the second term. What we get is one minus y I

144
00:16:35.890 --> 00:16:37.280
Nilesh Gupta: and 3.

145
00:16:38.560 --> 00:16:39.360
Let's see

146
00:16:42.180 --> 00:16:43.020
Nilesh Gupta: this, too.

147
00:16:45.930 --> 00:16:49.140
Nilesh Gupta: and we can probably further simplify this.

148
00:16:50.530 --> 00:16:53.310
Nilesh Gupta: Oh, this to me.

149
00:16:55.260 --> 00:16:58.260
Nilesh Gupta: My, I thank you.

150
00:17:12.970 --> 00:17:14.800
Nilesh Gupta: Yes.

151
00:17:17.920 --> 00:17:18.589
and

152
00:17:19.430 --> 00:17:20.140
Nilesh Gupta: morning.

153
00:17:36.410 --> 00:17:42.480
Nilesh Gupta: Sorry I made a mistake here like there should be like one negative sign as well, which will come out.

154
00:17:42.710 --> 00:17:46.680
The negative sign will come out, because, like there is log of one minus Sigma.

155
00:17:48.110 --> 00:17:50.070
Nilesh Gupta: So then, we are

156
00:17:50.280 --> 00:17:57.540
Nilesh Gupta: taking the derivative of one minus Sigma part like that negative sign will come on

157
00:17:57.940 --> 00:18:00.020
Nilesh Gupta: so like this this

158
00:18:03.230 --> 00:18:04.450
Nilesh Gupta: minus.

159
00:18:05.800 --> 00:18:10.260
Nilesh Gupta: Oh, this whole term. So this will become yeah, minus a 3 8 2,

160
00:18:10.630 --> 00:18:15.860
Nilesh Gupta: That's why I 3, 82,

161
00:18:19.400 --> 00:18:22.330
Nilesh Gupta: And so these 2 terms cancel

162
00:18:22.510 --> 00:18:23.210
Nilesh Gupta: cool.

163
00:18:23.870 --> 00:18:27.050
Nilesh Gupta: Then we are left with one minus Yi.

164
00:18:27.610 --> 00:18:28.620
Nilesh Gupta: You, too.

165
00:18:36.130 --> 00:18:42.080
Nilesh Gupta: which essentially is so taking the negative sign insight it's.

166
00:19:00.330 --> 00:19:04.650
Nilesh Gupta: So this a 3 is our final prediction, which is why hat?

167
00:19:04.870 --> 00:19:07.860
Nilesh Gupta: So it it just turns out to be this.

168
00:19:08.140 --> 00:19:10.540
Nilesh Gupta: So I've just written a 3 as by head.

169
00:19:10.680 --> 00:19:12.280
Nilesh Gupta: so like

170
00:19:12.440 --> 00:19:16.240
Nilesh Gupta: the gradient with respect to W. 3 is got, it

171
00:19:16.280 --> 00:19:23.540
Nilesh Gupta: turned out to be pretty simple, and like it's. It's also intuitive as well that it's it's only saying that

172
00:19:27.190 --> 00:19:33.060
Nilesh Gupta: if if the why had this, this, this Y hat is close t0 0ne.

173
00:19:37.330 --> 00:19:38.890
Nilesh Gupta: So this is my

174
00:19:39.950 --> 00:19:50.320
Nilesh Gupta: yeah. This is only saying that if Yi and y had match. then, like Don't make any updates which makes sense.

175
00:19:50.480 --> 00:19:58.540
Nilesh Gupta: and If the Y. E and Y hat are going to be different like, then you will be making adjustments in the direction of

176
00:19:58.580 --> 00:20:02.030
the input that W. 3 got, which was a 2.

177
00:20:02.640 --> 00:20:05.800
Nilesh Gupta: So I hope this intuition

178
00:20:05.850 --> 00:20:12.870
Nilesh Gupta: make sense, and, like I just worked out for W. 3. What kind of greetings that we get?

179
00:20:14.410 --> 00:20:19.120
Nilesh Gupta: Let me know if you guys have any doubts so far, i'll

180
00:20:19.290 --> 00:20:20.690
Nilesh Gupta: talk for a moment.

181
00:20:36.510 --> 00:20:38.500
So so far like I've only

182
00:20:38.550 --> 00:20:44.680
Nilesh Gupta: just done this math of basically like. If you had this binary cross Entropy laws.

183
00:20:44.800 --> 00:20:52.680
Nilesh Gupta: then, if you are taking derivative, with respect t0 0ur W. 3, which is the parameters weight parameter for the outermost layer.

184
00:20:52.830 --> 00:20:55.710
Nilesh Gupta: Then this is kind of the derivative that we'll get.

185
00:21:00.110 --> 00:21:09.010
Nilesh Gupta: and which somehow turned out to be pretty neat, and it has some intuitive meaning as well. It's always nice to see.

186
00:21:11.390 --> 00:21:13.620
Nilesh Gupta: So okay.

187
00:21:14.350 --> 00:21:16.620
Nilesh Gupta: let me go ahead, then. Oh.

188
00:21:17.050 --> 00:21:23.960
Nilesh Gupta: now, let's say like, if you want to find out our gradients with respect t0 0ur headed way, which is the second layer.

189
00:21:24.340 --> 00:21:30.370
Nilesh Gupta: and then, like what we want to find out is Del L. By

190
00:21:30.710 --> 00:21:32.790
Nilesh Gupta: Del W. 2.

191
00:21:34.590 --> 00:21:35.700
Nilesh Gupta: So

192
00:21:35.720 --> 00:21:44.460
Nilesh Gupta: will you. So the core idea and back propagation is like you. Will. New will use the chain rule to solve that.

193
00:21:44.700 --> 00:21:51.770
Nilesh Gupta: So but so by chain rule. I mean that you will try this as well

194
00:21:51.920 --> 00:21:53.740
Nilesh Gupta: by then

195
00:21:57.170 --> 00:22:02.390
Nilesh Gupta: 8 3 dot. then 3.

196
00:22:34.000 --> 00:22:39.960
Nilesh Gupta: So well, we'll break down this derivative into

197
00:22:40.020 --> 00:22:41.750
Nilesh Gupta: like product of

198
00:22:42.150 --> 00:22:49.620
Nilesh Gupta: 5, derivative and like, Why, specifically like I wrote like this is because, like, if you look at your forward equations.

199
00:22:49.690 --> 00:22:51.600
Nilesh Gupta: so this is

200
00:22:51.710 --> 00:22:54.450
Nilesh Gupta: how your forward equation is like

201
00:23:00.740 --> 00:23:12.130
Nilesh Gupta: your forward equation. Loss, function is some function, let's say so. What I've written down is, and

202
00:23:12.370 --> 00:23:15.450
Nilesh Gupta: this is also equal to say of a 3.

203
00:23:17.420 --> 00:23:29.420
Nilesh Gupta: So like we are first taking derivative with respect to a 3. Now a 3 is the like dependent on Z. 3, s0 0ne. Now we are taking Del a 3 by Del Z. 3,

204
00:23:29.650 --> 00:23:30.570
Nilesh Gupta: then

205
00:23:30.670 --> 00:23:37.390
Nilesh Gupta: Z 3 is dependent. Like it depends. It is connected to W, 2 through a 2,

206
00:23:37.470 --> 00:23:44.800
Nilesh Gupta: So that's why we have taken del Z, 3 by 10, a 2, and a 2 is connected to W. 2 by

207
00:23:44.850 --> 00:23:54.920
Nilesh Gupta: this lead to so like we have taken Del a 2 by V. 2, and finally, the Z 2 is connected to W. 2 by this equation.

208
00:23:54.980 --> 00:23:58.360
Nilesh Gupta: So we can like compute del Z. 2 by delta W. 2,

209
00:23:59.660 --> 00:24:06.050
Nilesh Gupta: So that's why, like, since our forward propagation equations are like that. So I just like

210
00:24:06.070 --> 00:24:11.360
Nilesh Gupta: kind of traced it backwards, and, like I first gone from like a

211
00:24:11.470 --> 00:24:12.950
Nilesh Gupta: i'm calculating that

212
00:24:13.050 --> 00:24:17.160
Nilesh Gupta: by changing my a 3 like how much my a. L. Is changing.

213
00:24:17.190 --> 00:24:25.370
Nilesh Gupta: So this quantity is calculating that. and this quantity is calculating by changing Z 3 by some amount. How much? My! A 3 is changing?

214
00:24:25.590 --> 00:24:35.260
Nilesh Gupta: So it it is quantifying that. And then, like by the chain rule of differential calculus we can. We know that, like

215
00:24:35.500 --> 00:24:42.940
Nilesh Gupta: we can write a Del L by Del W. 2. That is, by changing my W. 2. How much my end is changing by

216
00:24:42.950 --> 00:24:47.960
dividing that into multiple components of of the equation.

217
00:24:49.310 --> 00:24:53.720
Nilesh Gupta: So this is how i'm writing this. And

218
00:24:57.140 --> 00:25:00.880
Nilesh Gupta: so if if you look at this

219
00:25:01.340 --> 00:25:05.780
Nilesh Gupta: this quantity. We have already calculated somewhere here.

220
00:25:06.570 --> 00:25:09.780
Nilesh Gupta: So because when we are calculating. oh.

221
00:25:13.540 --> 00:25:23.940
Nilesh Gupta: when we are calculating Delta by 10 W. 3, the essentially what we did was then l by

222
00:25:27.440 --> 00:25:28.570
Nilesh Gupta: 3

223
00:25:29.230 --> 00:25:31.080
Nilesh Gupta: by 13 3

224
00:25:33.610 --> 00:25:34.920
Nilesh Gupta: times 10,

225
00:25:35.310 --> 00:25:36.530
3

226
00:25:37.340 --> 00:25:38.980
Nilesh Gupta: point l. 33.

227
00:25:42.070 --> 00:25:54.290
Nilesh Gupta: So when we were doing this, like we have already calculated this part. and this Del Z 3 and Del W 3, if you look since

228
00:25:55.010 --> 00:25:57.530
Nilesh Gupta: V. 3 equal to

229
00:25:59.600 --> 00:26:01.600
Nilesh Gupta: W. 3 transpose

230
00:26:02.260 --> 00:26:03.630
Nilesh Gupta: we 2 things

231
00:26:05.650 --> 00:26:07.170
Nilesh Gupta: they simply tell you.

232
00:26:16.040 --> 00:26:22.250
Nilesh Gupta: So this a 2 was coming from this component and the rest of the But this

233
00:26:22.540 --> 00:26:30.340
Nilesh Gupta: is this: specifically this dot del a 3 dot del a 3, delta, a 3 by 10, c. Through.

234
00:26:30.780 --> 00:26:31.620
Nilesh Gupta: So

235
00:26:32.300 --> 00:26:40.900
Nilesh Gupta: this component, if you try to trace it backward, and this competition, you will realize that this is nothing but this particular

236
00:26:41.220 --> 00:26:42.810
Nilesh Gupta: minus y minus.

237
00:26:44.900 --> 00:26:46.460
Nilesh Gupta: Why, I

238
00:26:48.640 --> 00:26:53.870
Nilesh Gupta: so now, like we don't need to compute this, and we only need to compute this part

239
00:26:54.640 --> 00:26:57.430
Nilesh Gupta: and this

240
00:26:57.820 --> 00:26:58.450
Oh.

241
00:26:58.760 --> 00:27:04.720
Nilesh Gupta: we we know how to do, because, like we have all the these equations.

242
00:27:05.320 --> 00:27:11.340
Nilesh Gupta: so we can just write Del Z 3 by delay to as

243
00:27:14.930 --> 00:27:16.590
Nilesh Gupta: this is going to be

244
00:27:19.720 --> 00:27:25.540
Nilesh Gupta: so. This is the equation. So like if we take Del Z 3 by delay, 2 like it will give us 33,

245
00:27:31.220 --> 00:27:35.090
Nilesh Gupta: and this thing del a 2 by Del z 2.

246
00:27:37.180 --> 00:27:43.500
Nilesh Gupta: This, if you take the derivative like, we can like it's just like more of z 2, so we can get

247
00:27:45.960 --> 00:27:46.730
Nilesh Gupta: Well.

248
00:27:47.890 --> 00:27:49.680
Nilesh Gupta: it will just be

249
00:27:56.100 --> 00:27:57.190
Nilesh Gupta: a 2

250
00:27:58.750 --> 00:28:00.920
Nilesh Gupta: times one minus C 2,

251
00:28:04.710 --> 00:28:08.000
Nilesh Gupta: and then this Del C, 2 by Del W. 2.

252
00:28:08.090 --> 00:28:16.590
Nilesh Gupta: We can get from this equation Z. 2 equal to W. 2 transfers, even as B 2. So like this will give us a one

253
00:28:21.810 --> 00:28:22.600
Nilesh Gupta: so

254
00:28:24.220 --> 00:28:27.450
Nilesh Gupta: so like. This is how we can compute

255
00:28:27.740 --> 00:28:29.650
the derivative of

256
00:28:29.710 --> 00:28:34.830
Nilesh Gupta: loss with respect t0 0ur hidden layer. Oh, but i'm a to

257
00:28:34.910 --> 00:28:39.700
Nilesh Gupta: and like I've worked out for like, for in this and simplistic case.

258
00:28:40.030 --> 00:28:49.590
Nilesh Gupta: and so so far like it, it does look complicated, but like you, you can see that there is a method like method to be like getting to these

259
00:28:49.690 --> 00:28:51.370
particular

260
00:28:51.380 --> 00:28:52.460
Nilesh Gupta: gradients

261
00:28:52.870 --> 00:28:57.630
Nilesh Gupta: like it's not like you don't have to do everything from scratch like when you are

262
00:28:57.640 --> 00:29:09.030
Nilesh Gupta: computing the gradient. So so like. This seems to be like kind of a algorithm or method that you can write which can do this automatically for you

263
00:29:09.340 --> 00:29:15.880
Nilesh Gupta: and and like that, turns out to be true and like in general, like when you are.

264
00:29:15.960 --> 00:29:22.120
Nilesh Gupta: you're writing any machine D planning code. You are not computing these Greek ingredients by hand.

265
00:29:22.300 --> 00:29:27.690
Nilesh Gupta: You are only defining your loss functions, and how you are like doing the forward propagation.

266
00:29:27.900 --> 00:29:30.140
Nilesh Gupta: and you use a

267
00:29:30.180 --> 00:29:33.650
a package called auto differentiation.

268
00:29:33.920 --> 00:29:36.470
Nilesh Gupta: which, in short, is referred to as auto deaf.

269
00:29:36.760 --> 00:29:42.530
Nilesh Gupta: and that package can automatically do this gradient calculation for you.

270
00:29:44.540 --> 00:29:45.530
Nilesh Gupta: So.

271
00:29:45.880 --> 00:29:52.720
Nilesh Gupta: But it's still good to know like and like you like, how exactly it's computing, those

272
00:29:52.870 --> 00:29:55.870
doing, the performing back propagation.

273
00:29:55.990 --> 00:29:59.550
Nilesh Gupta: And how is it calculating the gradients? So

274
00:30:00.470 --> 00:30:10.950
Nilesh Gupta: most likely you will, unless it's a exercise you will never write. Try to compute these gradients by hand and try to write it. But yeah.

275
00:30:10.970 --> 00:30:15.060
Nilesh Gupta: it's still better to like. Have a good understanding of them

276
00:30:15.480 --> 00:30:23.240
Nilesh Gupta: of how it's happening. So yeah, let me stop for a moment. And I think, like

277
00:30:23.280 --> 00:30:28.000
Nilesh Gupta: I had just on this back back propagation a bit.

278
00:30:28.020 --> 00:30:30.540
Nilesh Gupta: because I wanted to cover some other materials, too.

279
00:30:30.630 --> 00:30:36.570
Nilesh Gupta: But yeah, please let me know if you have any questions so far, and if anything was unclear.

280
00:30:36.770 --> 00:30:38.450
Nilesh Gupta: otherwise I will

281
00:30:38.660 --> 00:30:40.430
try to

282
00:30:40.540 --> 00:30:44.980
Nilesh Gupta: go towards this, and s0 0n practical side of deep learning.

283
00:30:46.470 --> 00:30:47.830
Nilesh Gupta: I'll get by touch.

284
00:30:56.840 --> 00:30:57.460
Nilesh Gupta: Okay.

285
00:31:10.400 --> 00:31:22.390
Nilesh Gupta: So either is, it is too clear, or either that is too confusing. But hope so like your homework, too, will have some questions which will ask you to write

286
00:31:23.020 --> 00:31:25.290
Nilesh Gupta: down this back propagation steps

287
00:31:26.450 --> 00:31:40.420
Nilesh Gupta: in code and like. In simple term, you'll have to do it explicitly. So I think, like I, if you have not understand this like it might make things a bit difficult. But, like definitely.

288
00:31:40.460 --> 00:31:53.950
Nilesh Gupta: I would say like even when I was learning this, just looking at the the question, didn't make sense. So it's. I. I hope that, like when you are writing that down in code things will become much clearer.

289
00:31:54.080 --> 00:32:02.560
Nilesh Gupta: and that's what like for me. How I learned essentially like it's it's not a bit too complicated

290
00:32:02.840 --> 00:32:09.460
Nilesh Gupta: concept, but it's just a bit hard to wrap your head around it. So unless you have

291
00:32:09.620 --> 00:32:11.680
Nilesh Gupta: done that yourself.

292
00:32:12.060 --> 00:32:13.200
Nilesh Gupta: that's it. So

293
00:32:13.780 --> 00:32:16.830
Nilesh Gupta: it might be a bit Hi to. Okay.

294
00:32:18.650 --> 00:32:20.410
Nilesh Gupta: So all right.

295
00:32:20.600 --> 00:32:22.700
Nilesh Gupta: i'll move on to the next part.

296
00:32:23.220 --> 00:32:25.900
Nilesh Gupta: which is the by torch.

297
00:32:26.250 --> 00:32:27.190
Nilesh Gupta: So

298
00:32:27.730 --> 00:32:29.990
Nilesh Gupta: let me stop sharing this.

299
00:32:54.850 --> 00:32:56.640
Nilesh Gupta: Yeah. So

300
00:32:58.300 --> 00:33:03.250
Nilesh Gupta: So let me just give a brief introduction of what by torches.

301
00:33:03.320 --> 00:33:04.880
Nilesh Gupta: My torch is a

302
00:33:04.920 --> 00:33:09.780
Nilesh Gupta: deep learning framework. It's a library like number, but for deep learning.

303
00:33:09.950 --> 00:33:11.270
Nilesh Gupta: so it has.

304
00:33:11.540 --> 00:33:19.520
Nilesh Gupta: It is extremely well written and very stable, and, I would say, like the most popular deep learning framework out there.

305
00:33:19.750 --> 00:33:21.320
Nilesh Gupta: and

306
00:33:21.390 --> 00:33:32.920
Nilesh Gupta: if it was not for

307
00:33:33.150 --> 00:33:40.230
Nilesh Gupta: so it it does play a very crucial role, and it kind of provides you abstract

308
00:33:40.710 --> 00:33:51.150
Nilesh Gupta: it. It kind of provides the abstraction over the fundamental deep learning concepts and like makes it easier to play around. So

309
00:33:52.170 --> 00:33:53.910
Nilesh Gupta: that that's the

310
00:33:54.240 --> 00:33:58.060
Nilesh Gupta: brief kind of view Sentence. Introduction for python.

311
00:33:58.300 --> 00:34:02.660
Nilesh Gupta: One of the most fundamental things inside by tor is the tensors.

312
00:34:02.800 --> 00:34:05.650
Nilesh Gupta: S0 10 sets are nothing but

313
00:34:06.570 --> 00:34:14.520
Nilesh Gupta: num like. If you are familiar with numb by ndras, like, they are kind of similar to ndras, and they

314
00:34:14.570 --> 00:34:15.510
Nilesh Gupta: yes.

315
00:34:15.690 --> 00:34:20.159
Nilesh Gupta: just supposed to store any n dimensional array data.

316
00:34:20.260 --> 00:34:32.120
Nilesh Gupta: and everything inside Python is going to be. or 10 set, like most of the thing like any data, any a network like everything is going to be a collection of tenses.

317
00:34:32.350 --> 00:34:37.469
Nilesh Gupta: So it's it's a nice starting point to start talking about.

318
00:34:41.150 --> 00:34:53.540
Nilesh Gupta: So, as I said, like, they are the central data abstraction. and they are very similar to a number nd. But they have, like some additional functionalities, as well, such as the

319
00:34:54.040 --> 00:35:03.290
Nilesh Gupta: they, they can be transferred easily on to Gpus and like. Can you can perform computations on them on Gpus

320
00:35:03.360 --> 00:35:07.480
Nilesh Gupta: and one of the other things that you will see is like

321
00:35:07.530 --> 00:35:13.820
Nilesh Gupta: they. They can also, instead of just holding the data. They can also have a special

322
00:35:14.330 --> 00:35:23.640
Nilesh Gupta: element which which can hold on to gradients of for that data as well. So we'll take a look at that later. So

323
00:35:24.190 --> 00:35:36.830
Nilesh Gupta: But first let's just start with like, how can we create a tensor? So the simplest way? And, by the way, like these 2 tools that i'm using the official 5 torch tutorials, I think they are the best out there.

324
00:35:36.940 --> 00:35:43.800
Nilesh Gupta: so like if it if it's something like you feel like you are not getting, or like

325
00:35:43.810 --> 00:35:53.990
Nilesh Gupta: you didn't get first time. You can just take a look. The official tutorials and there are videos included with each tutorial as well.

326
00:35:54.060 --> 00:36:06.150
Nilesh Gupta: which is from the official Py Dodge Maintainer. So I think, yeah, you guys can take a look like if something I don't cover or like what I covered my is confusing.

327
00:36:06.860 --> 00:36:07.740
Nilesh Gupta: So

328
00:36:08.910 --> 00:36:12.360
Nilesh Gupta: yeah, the simplest way to create a tensor is like you can just

329
00:36:12.380 --> 00:36:17.840
Nilesh Gupta: do a tors dot empty, and you can provide it as shape. So like, if you

330
00:36:18.500 --> 00:36:19.340
Nilesh Gupta: okay.

331
00:36:20.590 --> 00:36:23.850
Nilesh Gupta: so import your torch first.

332
00:36:25.540 --> 00:36:29.610
Nilesh Gupta: and then you can like create this so

333
00:36:29.970 --> 00:36:37.430
Nilesh Gupta: you can call tall. Start empty, and you can say 3 comma 4, and it will create a tensor of dimension, 3 cross 4.

334
00:36:37.560 --> 00:36:49.400
Nilesh Gupta: And if, since you are initializing with empty like, it will just fill it with in garbage like it won't initialize anything like, so it will just pick out any garbage value that is already being present at that memory. Allocation

335
00:36:49.710 --> 00:36:55.000
Nilesh Gupta: so so like. This is kind of the simplest way

336
00:36:56.630 --> 00:37:13.300
Nilesh Gupta: to create tensors, and then, like, if you want to create some specific tensors like Dodge dot zeros, which is very similar to numpy dot zeros, it will create a tensor of all zeros You can similarly do torst at once.

337
00:37:13.380 --> 00:37:18.840
Nilesh Gupta: and then there is stores to try, and which will fill it with random values between 0 and one.

338
00:37:19.040 --> 00:37:22.350
Nilesh Gupta: So this is what is shown in this block.

339
00:37:23.490 --> 00:37:34.310
Nilesh Gupta: Hmm. Going next. as you notice like, since we are generating the stores, Dot ran it. Will it

340
00:37:34.900 --> 00:37:41.410
Nilesh Gupta: it like there is almost nothing. So as randomness and

341
00:37:41.460 --> 00:37:51.610
Nilesh Gupta: computer program, there is like nothing completely random like these. Random numbers are picked out from a pseudo random number generator. So

342
00:37:51.770 --> 00:37:57.480
sometimes what you want is you want, like your code, to be reproducible. So you want, like

343
00:37:57.560 --> 00:38:15.950
Nilesh Gupta: the random numbers to be same as well. So that like Tosh provides you that ability to do by setting this manual seed. So if you set a particular seat, and then you generate your random numbers like they will always all come out to be the same random number. So, for example, like, if you.

344
00:38:16.230 --> 00:38:32.020
Nilesh Gupta: if I set my manual, c. T0 1 7 2 9 here, and then generate this random numbers like this: first tensor and this third tensor are going to be the same, because, like I did, I set explicitly the manual seed after

345
00:38:33.530 --> 00:38:38.770
Nilesh Gupta: I before running, creating these random numbers

346
00:38:38.990 --> 00:38:39.960
Nilesh Gupta: so

347
00:38:40.410 --> 00:38:50.620
Nilesh Gupta: like when you want reproducibility, and you want your r of numbers to be exactly same. You can do that by setting up the you know these manual seeds.

348
00:38:50.840 --> 00:38:55.460
Nilesh Gupta: and then, just like you can

349
00:38:55.570 --> 00:39:00.550
Nilesh Gupta: like, get the shape of your tensors by examining extort shape.

350
00:39:01.010 --> 00:39:13.850
Nilesh Gupta: and you can create tensors like or different tensors. So like this storage or zeros like, will create a tensor, which is, which has all the properties of X. But it it's filled with zeros.

351
00:39:14.360 --> 00:39:22.540
Nilesh Gupta: so that's what no it's going on here. I won't go into, like all of the details.

352
00:39:25.180 --> 00:39:25.970
Nilesh Gupta: Let's see.

353
00:39:26.200 --> 00:39:30.450
Nilesh Gupta: So you can also build tensor from a list

354
00:39:30.500 --> 00:39:32.150
Nilesh Gupta: or a list of lists

355
00:39:32.530 --> 00:39:37.950
so like this is what what is shown here like you are just passing a list of lists.

356
00:39:38.140 --> 00:39:46.750
Nilesh Gupta: and it's creating a 2D tensor out of it, which is the shape of this thing. So if you give a regular shape like Point

357
00:39:47.410 --> 00:39:54.710
Nilesh Gupta: 9.2, this will complain that, like this is not a regular shape, and I can't create a tensor out of it.

358
00:39:54.750 --> 00:40:05.980
Nilesh Gupta: So yeah, when you are initializing like this, like you have to give regular shapes here which can be converted into

359
00:40:06.130 --> 00:40:09.380
Nilesh Gupta: So this is some.

360
00:40:10.260 --> 00:40:19.530
These are the data types, standard data types you have in 16 fl0 64, the in 32 like these are going to be yours

361
00:40:19.690 --> 00:40:33.090
Nilesh Gupta: standard data types you can specify by passing this D type argument. and to go it so so far like as as you have seen, like, it behaves very similar to the number ndras.

362
00:40:33.220 --> 00:40:35.760
Nilesh Gupta: and you can do all the operations with them.

363
00:40:35.830 --> 00:40:39.650
These are some math operation which is shown which I will not go into.

364
00:40:39.660 --> 00:40:43.160
Nilesh Gupta: They. You also have the power of broadcasting

365
00:40:43.290 --> 00:40:48.590
Nilesh Gupta: so broadcasting. Just a we is a way to perform an operation between tensor

366
00:40:48.610 --> 00:40:58.480
Nilesh Gupta: that have similarities in their shape. So, for example, like this, ran: the tensor like this has the shape to comma 4.

367
00:40:58.730 --> 00:41:08.760
Nilesh Gupta: But if you try to multiply with a tensor, which is of shape. One comma 4 like torch won't. Complain. What it will do is, it will replicate

368
00:41:08.950 --> 00:41:15.260
Nilesh Gupta: that one come of 4 4, tensor, 2 times, and it will make a 2 comma for tensor.

369
00:41:15.540 --> 00:41:18.230
Nilesh Gupta: and then, like it, will do the multiplication.

370
00:41:18.400 --> 00:41:25.610
Nilesh Gupta: So this is like commonly called broadcasting, and it's it's very helpful in writing, our code.

371
00:41:26.950 --> 00:41:32.380
Nilesh Gupta: and it's just a ease like you don't have to like manually replicate it every time.

372
00:41:32.490 --> 00:41:44.320
Nilesh Gupta: and this you will see many times when you are writing neural networks that you you want to perform an operation like W. Matrix multiplication, Wx plus b.

373
00:41:44.510 --> 00:41:49.570
Nilesh Gupta: and this will give you the the of

374
00:41:49.970 --> 00:42:05.580
Nilesh Gupta: N comma L. Let's say, and this will give you a ray of one kil. So like, if you want to, just like matrix multiplication in very strict matrix addition, in very strict sense. This doesn't allow you to add these kind of

375
00:42:07.670 --> 00:42:20.770
Nilesh Gupta: matrices, which is n komile and one hom, but like in torch or in numpy like this works completely fine, because, like it will just broadcast this one Kamil n Times, and it will make a and come out

376
00:42:20.780 --> 00:42:33.450
Nilesh Gupta: where each of the row is exact copy of the one Kamile B that you had. So this is what broadcasting is. and it's so.

377
00:42:36.440 --> 00:42:38.690
Nilesh Gupta: This is not with tensors.

378
00:42:39.740 --> 00:42:41.170
Nilesh Gupta: And

379
00:42:47.030 --> 00:42:47.770
Nilesh Gupta: yeah.

380
00:42:49.710 --> 00:42:50.740
Nilesh Gupta: so.

381
00:42:50.960 --> 00:42:59.860
Nilesh Gupta: 0 0ne more comment is there that, like if you do, assignments with tenses usually like the assignments, are

382
00:43:00.080 --> 00:43:00.900
Nilesh Gupta: oh.

383
00:43:01.170 --> 00:43:14.570
Nilesh Gupta: assigned they are assigned by reference, so like they they, if you feel like. If you do a equal to toss dot once and be equal to a, and then you change a, then B will also get altered like we will also have the starving.

384
00:43:14.840 --> 00:43:22.440
Nilesh Gupta: So if you really want to like, create a duplicate of a particular tensor, then you will need to use a dot clone

385
00:43:24.700 --> 00:43:27.340
Nilesh Gupta: now like it, will it?

386
00:43:27.370 --> 00:43:35.720
Nilesh Gupta: It will. This will create a different tensor, which is just it. A clone of the first tensor, which was a

387
00:43:36.310 --> 00:43:37.630
Nilesh Gupta: So

388
00:43:38.780 --> 00:43:53.480
Nilesh Gupta: these are some standard operations that that you can do, and, like the other part that I mentioned is that why tensors are more powerful than just the regular number.

389
00:43:53.680 --> 00:44:06.010
Nilesh Gupta: and right now so like on my laptop, I don't have any gpu so like, if you like, compute this tors.co.is available, it will say false.

390
00:44:06.070 --> 00:44:12.190
Nilesh Gupta: so like it's only Cp. Only, but like the syntax for like transferring any

391
00:44:12.310 --> 00:44:20.680
Nilesh Gupta: cancer to CPU is very simple, simple like you like. Let's say you have this a

392
00:44:20.840 --> 00:44:26.320
Nilesh Gupta: tensor, then, like typically just you do either a or 2 coda.

393
00:44:26.530 --> 00:44:31.490
Nilesh Gupta: and it will transfer that 10 set to. Oh.

394
00:44:31.730 --> 00:44:37.020
Gpu! Or if you have multiple gpus you can assign Ids here

395
00:44:37.390 --> 00:44:42.520
Nilesh Gupta: so like this will move to the Gpu 0, and this will move to gpu one.

396
00:44:42.740 --> 00:44:55.280
Nilesh Gupta: So so it's. It's a pretty simple like. How you how you transfer these things, for in this particular demonstration, since I don't have coda so like it. It it's complaining that there

397
00:44:55.320 --> 00:44:57.470
toss not compiled with.

398
00:44:57.800 --> 00:44:58.860
Nilesh Gupta: So

399
00:45:01.210 --> 00:45:05.840
Nilesh Gupta: So that's all like I what? Hmm. I wanted to say on tensors.

400
00:45:05.880 --> 00:45:08.180
Nilesh Gupta: So the key

401
00:45:08.530 --> 00:45:19.790
Nilesh Gupta: highlight. Is that like they can be moved easily to different devices and by devices here I mean Cpus and Gpus, and maybe different gpus that you if you have on your machine.

402
00:45:20.150 --> 00:45:21.040
Nilesh Gupta: so

403
00:45:22.300 --> 00:45:28.150
we'll take a look at the like the gradient part of these tensors

404
00:45:28.180 --> 00:45:29.080
Nilesh Gupta: and a bit.

405
00:45:29.170 --> 00:45:32.040
Nilesh Gupta: so let me know if there is any questions

406
00:45:32.190 --> 00:45:33.010
Nilesh Gupta: so far.

407
00:45:42.910 --> 00:45:43.640
Nilesh Gupta: Hello!

408
00:45:45.270 --> 00:45:56.550
Nilesh Gupta: Let's look at this autograph. So. as I said this, like the back propagation that we were doing, we were doing everything by hand so.

409
00:45:56.780 --> 00:46:02.200
and all the deep learning frameworks you don't have to do and like they, you can just simply

410
00:46:02.670 --> 00:46:16.310
Nilesh Gupta: rely on their autograph package, which is the way their way of doing automatic differentiation. And let's just look at the examples like how this is done.

411
00:46:16.680 --> 00:46:17.740
Nilesh Gupta: So

412
00:46:18.870 --> 00:46:23.040
Nilesh Gupta: one thing to mention here is, as I said, like. If you

413
00:46:23.210 --> 00:46:24.880
Nilesh Gupta: create a tensor.

414
00:46:27.420 --> 00:46:33.650
Nilesh Gupta: they are like this. There is this argument which is called requires Grab

415
00:46:34.080 --> 00:46:41.810
Nilesh Gupta: that you can pass, and by default, it is set to false, so like this will create a tensor

416
00:46:41.970 --> 00:46:46.790
Nilesh Gupta: which has required grad as false. But if you provide

417
00:46:47.140 --> 00:46:49.280
Nilesh Gupta: requires grad as true

418
00:46:49.580 --> 00:46:59.050
Nilesh Gupta: like this will start storing another variable called a dot, a dot grad. which

419
00:46:59.590 --> 00:47:06.790
Nilesh Gupta: where, like, if you compute ingredients like, do this loss, this backward call, which will just see in a bit.

420
00:47:06.900 --> 00:47:22.560
Nilesh Gupta: and so like it will start storing the gradient inside, and your auto grad package will also start tracking this variable to compute gradients. If the requires grad is false, the auto grad will just treat it as like. I don't want like

421
00:47:22.600 --> 00:47:35.880
Nilesh Gupta: there's no gate and computations required on this, so it will. It will not compute gradients for this particular variable. But if you so like for any trainable parameters, you want the requires grad to be true.

422
00:47:36.720 --> 00:47:37.760
Nilesh Gupta: So

423
00:47:38.500 --> 00:47:47.560
Nilesh Gupta: here, like it's a we are just creating a tensor, which which is like equally spaced between 0 t0 2 pi.

424
00:47:47.810 --> 00:47:57.980
Nilesh Gupta: and this is what the 10 set looks like, and, as you can see, like the requires grad option, is set to true. so it has a tensor of size 25,

425
00:47:58.330 --> 00:48:01.320
Nilesh Gupta: and, like it's equally space between 0 t0 2 pi.

426
00:48:01.540 --> 00:48:04.490
If you plot it, it will look something like this.

427
00:48:07.070 --> 00:48:11.280
Nilesh Gupta: It is it to just assign function. and

428
00:48:12.350 --> 00:48:24.600
Nilesh Gupta: oh. sorry. B. We are doing this. We are creating another tensor, which is b equal to tors dot sine a. and we are plotting a V against a so like. We are just getting the signed function.

429
00:48:25.120 --> 00:48:28.850
Nilesh Gupta: If you, we can print the function B,

430
00:48:28.900 --> 00:48:39.240
Nilesh Gupta: and then like for this particular demonstration, we are creating another 2 variables, which are C and T. C is 2 into B and D is C plus one.

431
00:48:39.560 --> 00:48:42.540
Nilesh Gupta: so you can run, and these are the

432
00:48:42.900 --> 00:48:44.920
variables that we get

433
00:48:46.370 --> 00:48:51.050
Nilesh Gupta: so. And finally, like we are creating this out equal to d dot sum.

434
00:48:51.380 --> 00:49:03.230
Nilesh Gupta: and we are printing out, and it's 25. You guys can check it. That's correct. But the nice part is now we we can look at like. since, like

435
00:49:04.010 --> 00:49:12.270
Nilesh Gupta: since a had this requires grad option. True, all of these variables, which are computer, which have been computed like.

436
00:49:12.640 --> 00:49:22.900
Hmm. Python. The the autograph package of by torch is keeping track of like what function led to this particular variable.

437
00:49:22.990 --> 00:49:26.260
Nilesh Gupta: So if you look@thisd.grad function.

438
00:49:27.380 --> 00:49:40.750
Nilesh Gupta: so d dot grad function is, add backward by this, add backward, just tells you that it d got created by a additional function, and it is also tracking, like what were the 2 arguments of that additional function

439
00:49:40.770 --> 00:49:43.910
Nilesh Gupta: so like there it will track that there is C and one.

440
00:49:44.050 --> 00:49:49.000
Nilesh Gupta: and when back propagating it will use that function to compute gradients.

441
00:49:49.750 --> 00:50:00.340
Nilesh Gupta: So similarly, if you look the d dot grad function dot next functions. then it will give you a multiply. So this multiply is because.

442
00:50:00.500 --> 00:50:04.280
Nilesh Gupta: if you like, g0 0ne step backward to de.

443
00:50:04.720 --> 00:50:09.530
You will get like that, c. Was created by this multiplication.

444
00:50:09.730 --> 00:50:12.030
Nilesh Gupta: But that's why you have this multiplication.

445
00:50:12.650 --> 00:50:23.420
Nilesh Gupta: and similarly like, if you like, look at the grad functions of C. It was multiplication. and the grad function of B. Remember that be equal to B is sign of a.

446
00:50:23.600 --> 00:50:27.320
Nilesh Gupta: so like it was signed backward. So.

447
00:50:28.320 --> 00:50:32.760
Nilesh Gupta: and a dot grad function is none, because, like a is a leaf variable.

448
00:50:32.800 --> 00:50:37.450
Nilesh Gupta: So the parameters like, which was not function of anything

449
00:50:37.540 --> 00:50:39.650
it's like

450
00:50:39.700 --> 00:50:43.310
Nilesh Gupta: has, like it's called the leaf variable.

451
00:50:43.510 --> 00:50:53.880
Nilesh Gupta: and so like in this case a is the leaf variable, and since, like it was not a function of anything like that, for, like you have this a dot grad function as none.

452
00:50:54.380 --> 00:50:55.250
Nilesh Gupta: So

453
00:50:55.330 --> 00:51:04.450
Nilesh Gupta: this is like, you can see that, like the auto grad, is keeping track of of every like how every variable, evolved, and like

454
00:51:05.960 --> 00:51:10.880
Nilesh Gupta: how every variable is connected to the leaf variables. So

455
00:51:11.340 --> 00:51:18.520
Nilesh Gupta: it is the most powerful part of the autographed like. If you called or out dot backward.

456
00:51:20.000 --> 00:51:21.220
Nilesh Gupta: it will

457
00:51:21.370 --> 00:51:26.800
calculate all the gradients for you and like. And now, if you check the a dot.

458
00:51:27.020 --> 00:51:28.230
Nilesh Gupta: that part

459
00:51:28.380 --> 00:51:32.280
Nilesh Gupta: like now this is populated with gradient values.

460
00:51:33.290 --> 00:51:34.200
Nilesh Gupta: So

461
00:51:35.690 --> 00:51:54.320
Nilesh Gupta: so this is the most powerful like you kind of did, and like here like it's very simplistic that you are doing only a multiplication in addition that may be assigned operation. But you can do anything like any kind of mathematical operations on it, and when you call dot backward.

462
00:51:54.320 --> 00:51:59.300
Nilesh Gupta: It will compute the right gradient for you. and you can print it.

463
00:51:59.340 --> 00:52:02.240
Nilesh Gupta: And if you like, plot right.

464
00:52:02.560 --> 00:52:03.510
Nilesh Gupta: so

465
00:52:05.310 --> 00:52:09.960
Nilesh Gupta: so yeah, you can't d0 0ut dot backward twice.

466
00:52:10.790 --> 00:52:12.330
Nilesh Gupta: Yeah. So

467
00:52:12.630 --> 00:52:24.680
Nilesh Gupta: if you plot it like this will turn out to be the cost function, because the derivative of the signed function is the cost function. So it is giving you the cost function multiplied by 2, because there was this.

468
00:52:24.730 --> 00:52:28.330
Nilesh Gupta: Oh. C was 2 times B.

469
00:52:29.010 --> 00:52:33.220
Nilesh Gupta: So that's exactly what you are getting like between 2 and minus 2 like

470
00:52:33.350 --> 00:52:41.020
Nilesh Gupta: It's just giving you a a multiplied cost function. which is the gradient of a So

471
00:52:43.390 --> 00:52:54.430
Nilesh Gupta: that's kind of what I wanted to show you that, like all the back propagation that we were doing, we can just do here by one simple call, which is

472
00:52:54.470 --> 00:53:04.480
Nilesh Gupta: out dot backward, and it will do all that propagation for you. And, like the leaf variables, the the gradient of the leaf variables will be populated.

473
00:53:04.600 --> 00:53:08.860
Nilesh Gupta: and you can access them by a dot. Grad. So

474
00:53:11.450 --> 00:53:23.020
Nilesh Gupta: there are like other things to talk about, autograph as well, but like for this particular demonstration, like I won't. Go into there. That so yeah, let me know if there are any

475
00:53:23.330 --> 00:53:34.150
Nilesh Gupta: things you want to discuss, or maybe elaborate for people like Webinars who are seeing this for the first time. It might be Oh. okay.

476
00:53:34.860 --> 00:53:41.370
Nilesh Gupta: not clear, like what's exactly happening. So yeah. please feel free to ask questions.

477
00:54:04.810 --> 00:54:05.560
Nilesh Gupta: Alright.

478
00:54:07.510 --> 00:54:13.240
Nilesh Gupta: So, as I said, like these tutorials are in available like on the by touch. Official

479
00:54:13.340 --> 00:54:18.000
rep also. Like, if anything is not clear, you can refer to that as well, or

480
00:54:18.300 --> 00:54:20.150
Nilesh Gupta: can just ask us later.

481
00:54:20.240 --> 00:54:37.750
Nilesh Gupta: So now, like these were the 2 basic building blocks, for by torch one was tensors, the concept of, like the data abstraction that they have, which they call tensor, and the other was autocrat, which is their mechanism of doing back propagation.

482
00:54:38.590 --> 00:54:48.120
Nilesh Gupta: So those were the 2 kind of basic concepts. Now let's just try to put everything together, and maybe, like i'll introduce some of the other

483
00:54:48.150 --> 00:54:56.460
Nilesh Gupta: things is specific to Python here and try to do some classification problem and bytage. So

484
00:55:04.730 --> 00:55:07.260
Nilesh Gupta: yeah, so here, like

485
00:55:08.300 --> 00:55:09.490
Nilesh Gupta: I'm.

486
00:55:11.310 --> 00:55:16.320
Nilesh Gupta: i'm like running this demonstration on the C for 10 data set.

487
00:55:16.830 --> 00:55:18.490
Nilesh Gupta: which is so

488
00:55:18.670 --> 00:55:33.340
Nilesh Gupta: image classification data set like you have images, and you want to classify them int0 10 classes, and the 10 classes are displaying car per cat, dear dog, frog or ship and truck.

489
00:55:33.700 --> 00:55:37.750
Nilesh Gupta: So that's a standard classification problem.

490
00:55:37.760 --> 00:55:53.100
Nilesh Gupta: And here, like I'm just importing torch and torch vision Torch vision is kind of a sister package of torch which is specifically designed for images and video. And so

491
00:55:53.690 --> 00:55:59.790
Nilesh Gupta: i'm importing that as well, because like for this particular example, i'm choosing image data set.

492
00:56:00.020 --> 00:56:06.150
Nilesh Gupta: And then there's like a a very nice utility called Tensor Board.

493
00:56:06.420 --> 00:56:09.590
This kind of lets you

494
00:56:09.830 --> 00:56:21.880
Nilesh Gupta: keep track of your experiments and anything that you're doing and visualize things better. So we'll take a look while we we are going through this demonstration.

495
00:56:22.050 --> 00:56:28.220
Nilesh Gupta: But yeah, so this is so. We are importing summary writer from Tensor board, and

496
00:56:28.670 --> 00:56:34.630
Nilesh Gupta: and we are creating an object called of a somebody writer, and we are providing this so

497
00:56:36.200 --> 00:56:43.810
Nilesh Gupta: particular location that, like this, is specifying, that put the results here, and runs slack, c. For 10.

498
00:56:44.230 --> 00:56:45.230
Nilesh Gupta: So

499
00:56:46.000 --> 00:56:48.500
in this now I like I'm.

500
00:56:48.540 --> 00:56:52.720
Nilesh Gupta: I might not explain everything here, but I just want to give you a

501
00:56:52.780 --> 00:57:02.690
Nilesh Gupta: brief overview of what things are possible and like what are the standard practices of when you are writing code and by touch. So

502
00:57:05.100 --> 00:57:12.480
Nilesh Gupta: 1 0ne more thing like to visualize these results intensible like you need to. Oh.

503
00:57:12.820 --> 00:57:17.650
Nilesh Gupta: 3 8 attends about server and to do that like, oh.

504
00:57:44.170 --> 00:57:47.290
Nilesh Gupta: okay. So to do that like

505
00:57:47.420 --> 00:57:55.610
Nilesh Gupta: once you have installed tensor board like you. You just have to run this tensor board minus minus log D. I are equal to runs.

506
00:57:55.620 --> 00:58:03.780
Nilesh Gupta: and you can provide a port as well, so like here I provided the portal 8 8 9 2, and hit, enter, and then it will

507
00:58:04.320 --> 00:58:10.470
Nilesh Gupta: just open this tensor board server, and it will give you this a particular

508
00:58:11.780 --> 00:58:15.260
Nilesh Gupta: URL that you can open in your browser.

509
00:58:16.500 --> 00:58:17.680
Nilesh Gupta: And

510
00:58:19.400 --> 00:58:32.960
Nilesh Gupta: for now it will just say that no dashboards are active for the current data set. So like, since we have not like written anything, we have not run our experiment yet, so like it's just saying that there is nothing to show.

511
00:58:33.010 --> 00:58:42.260
Nilesh Gupta: But like as we'll see that we say as we start writing stuff here, like in populating stuff in tens of voting that things will appear on this page.

512
00:58:42.400 --> 00:58:49.800
Nilesh Gupta: So let's just go ahead. So here.

513
00:58:50.600 --> 00:58:57.890
Nilesh Gupta: loading like loading the data and creating them, creating a data set out of it.

514
00:58:57.970 --> 00:59:01.540
Nilesh Gupta: So usually in Pythos

515
00:59:01.600 --> 00:59:09.740
Nilesh Gupta: a any data that you have like which you will use for training or evaluation or validation you are creating. You are storing them inside

516
00:59:09.780 --> 00:59:20.190
Nilesh Gupta: class called or Torch dot 8 0f it. and it's the main Oh. kind of

517
00:59:20.840 --> 00:59:33.120
Nilesh Gupta: the function that that the data set expects is like that. It should be indexable like if you, if I pass train set and 0 train data set in 0 like it should give you the data point

518
00:59:33.170 --> 00:59:38.760
Nilesh Gupta: and like it, should also like, give you the length of the data set. If you called

519
00:59:39.730 --> 00:59:49.380
Nilesh Gupta: so, you can create your own custom datasets as well here like, since this is a very popular image, classification data set so like it's already presented.

520
00:59:49.420 --> 00:59:52.750
Nilesh Gupta: Touch vision, and and i'm directly going to use that.

521
00:59:52.870 --> 01:00:01.040
Nilesh Gupta: But if you take a look like there are more like you can also create your own data sets and load them.

522
01:00:01.280 --> 01:00:10.570
Nilesh Gupta: So here i'm creating the trade data set, and the other thing is like you want to create a iterator over that data set as well.

523
01:00:10.660 --> 01:00:15.600
Nilesh Gupta: So by iterated, I mean, like, when you are doing your training loop

524
01:00:15.680 --> 01:00:20.400
Nilesh Gupta: usually like you are like taking random mini batches of your data

525
01:00:20.430 --> 01:00:24.150
Nilesh Gupta: and then using that in each iteration. So

526
01:00:24.320 --> 01:00:34.650
Nilesh Gupta: this data loader is a nice utility to package that to like to do that iteration for you like. Where like this, will this can.

527
01:00:35.210 --> 01:00:37.540
This can

528
01:00:37.560 --> 01:00:42.760
Nilesh Gupta: create many batches of data points from the data set.

529
01:00:43.250 --> 01:00:53.720
Nilesh Gupta: So, like as you can see, like it, the the that you give is going to be the data set and the bad size, so by bad size like it will every time it will give you

530
01:00:53.860 --> 01:00:59.970
Nilesh Gupta: Oh. data points so like that size number of data points.

531
01:01:00.260 --> 01:01:13.130
Nilesh Gupta: So and then, like you can provide this argument, shuffle so Shuffle will like like randomly mix and match the data points when you are creating the Mini patch.

532
01:01:13.140 --> 01:01:22.100
Nilesh Gupta: And if you like. If Shuffle was set to false, then it will give you many batches in the order that they are present in the data set.

533
01:01:22.990 --> 01:01:27.760
Nilesh Gupta: So the during training, since you want like random, many batches.

534
01:01:27.880 --> 01:01:39.260
Nilesh Gupta: So that's why we have shuffle equal to true. But if you see I can test loader. we have shuffle equal to falls. So because, like, since we want to evaluate like linearly

535
01:01:39.570 --> 01:01:40.590
Nilesh Gupta: So

536
01:01:43.150 --> 01:01:45.080
Nilesh Gupta: that's the the main part.

537
01:01:45.390 --> 01:01:56.390
Nilesh Gupta: so like data is handled, but like in kind of 2 main classes. The data set and the data loaded data set is what stores the button, all the data and

538
01:01:56.410 --> 01:02:07.300
Nilesh Gupta: data loaded as the iterator over it, which is a utility to give you a batch to version of the data. A Mini dashed version of the data. So

539
01:02:09.120 --> 01:02:12.190
Nilesh Gupta: so here, like I already have this downloaded

540
01:02:12.450 --> 01:02:16.240
Nilesh Gupta: so like it's just saying downloaded and verified.

541
01:02:16.500 --> 01:02:23.200
Nilesh Gupta: And here i'm just kind of showing some sample. They

542
01:02:23.630 --> 01:02:30.700
data points that exist like here. I am showing 4 sample data parts. which is the these photos.

543
01:02:30.970 --> 01:02:33.920
Nilesh Gupta: And this is the phot of a bird. This is cat.

544
01:02:34.260 --> 01:02:36.780
Nilesh Gupta: Hard to see. It's a that it is.

545
01:02:37.090 --> 01:02:44.010
Nilesh Gupta: It's a phot0 0f a frog and a bird. And here, if you see, like, I have written these this particular grid

546
01:02:44.050 --> 01:02:52.110
Nilesh Gupta: into my tensor board as well. So by doing the writer dot add image. See for 10 sample images.

547
01:02:52.320 --> 01:02:55.720
Nilesh Gupta: I've transferred it here by calling this.

548
01:02:55.760 --> 01:03:02.220
Nilesh Gupta: and if I rewrote my page. Here you can see those data sets also like are shown here.

549
01:03:02.610 --> 01:03:10.460
Nilesh Gupta: So you can like document and understand about like by call like images by calling this add image function.

550
01:03:10.830 --> 01:03:25.730
Nilesh Gupta: So these are just some sample images. And here I have defined this, my network class. So my network. any network that you define you define it as a class.

551
01:03:25.800 --> 01:03:35.950
Nilesh Gupta: and the class is inherited like from the N. And dot module class. This n is part of torch.

552
01:03:36.060 --> 01:03:42.680
Nilesh Gupta: and any network that you define is typically going to be inherited from this N dot module like it.

553
01:03:42.690 --> 01:03:46.900
Nilesh Gupta: provide some simple functionalities that you are

554
01:03:46.980 --> 01:03:48.950
Nilesh Gupta: going to use.

555
01:03:49.250 --> 01:03:54.800
Nilesh Gupta: So this is how you do it. And here, like as you can see, like I have created

556
01:03:54.960 --> 01:03:59.220
Nilesh Gupta: a neural network of 2 layers, the first layer right?

557
01:03:59.410 --> 01:04:03.450
Nilesh Gupta: It is just like it's the input size comma 120.

558
01:04:03.540 --> 01:04:07.770
Nilesh Gupta: So, like there are 1 20 neurons. and the input size

559
01:04:07.840 --> 01:04:10.500
is the input of my

560
01:04:10.580 --> 01:04:24.700
Nilesh Gupta: image. So, since my image is 32, plus 32, and there are like 3 channels for our gp that's why, it's 3 goes 32 across 32, and then like the outs of the number of neurons that i'm having in this particular

561
01:04:24.710 --> 01:04:26.020
layer.

562
01:04:26.070 --> 01:04:31.260
Nilesh Gupta: and then, like I have my final output layer. which is like

563
01:04:31.330 --> 01:04:40.870
Nilesh Gupta: taking like all the 120 neurons from the first layer, as input and it is like giving like 10 0utputs. So it has 10 neurons.

564
01:04:41.100 --> 01:04:44.350
Nilesh Gupta: Well. so it's 1 20 comma 10,

565
01:04:44.530 --> 01:04:52.690
Nilesh Gupta: and this nn dot Leonard, is just a package like a way to package your baits and biases

566
01:04:52.720 --> 01:04:57.370
Nilesh Gupta: that we were like looking earlier int0 0ne thing

567
01:04:57.450 --> 01:05:09.470
Nilesh Gupta: so like this linear is like it will take. It will apply like W. Transpose x plus p. Where x is your input so W and B are like part of this linear

568
01:05:09.620 --> 01:05:12.120
an end dot linear module.

569
01:05:12.630 --> 01:05:15.520
Nilesh Gupta: S0 0ne thing that you need to define

570
01:05:15.580 --> 01:05:26.360
Nilesh Gupta: in a torch network is like this forward function. This forward function tells you how to do the forward propagation that given our input how to produce the output.

571
01:05:26.620 --> 01:05:37.860
Nilesh Gupta: And you Don't, need to define your backward, because, as I said. like backward is done automatically through calling Dot backward and auto Grad will take care of it.

572
01:05:38.060 --> 01:05:47.850
Nilesh Gupta: So here, like my forward is pretty simple. I'm just flattening out like the input I will get is going to be 3 cross 32, plus 32.

573
01:05:47.880 --> 01:06:00.920
Nilesh Gupta: So i'm just flattening out to be like a one dimensional, vector and then i'm applying my first fully connected network, and then, like like my nonlinearity, is relu.

574
01:06:01.200 --> 01:06:03.500
Nilesh Gupta: and then.

575
01:06:04.350 --> 01:06:10.570
like I'm applying the final output layer on top of it and returning the output.

576
01:06:10.780 --> 01:06:13.520
Nilesh Gupta: So this is my network

577
01:06:13.790 --> 01:06:15.140
Nilesh Gupta: and

578
01:06:16.120 --> 01:06:19.660
Nilesh Gupta: let's look at these later.

579
01:06:20.040 --> 01:06:23.160
I'm just like creating my network.

580
01:06:23.750 --> 01:06:26.940
Nilesh Gupta: And one thing I wanted to show was like, I can

581
01:06:27.740 --> 01:06:35.030
Nilesh Gupta: add, like visualize my network by calling this writer dot, add graph net and some sample input.

582
01:06:35.090 --> 01:06:46.110
Nilesh Gupta: So this images is the sample input that I generated here. So so like on either, like. If you add craft takes 2 arguments. First is the network.

583
01:06:46.220 --> 01:06:48.830
Nilesh Gupta: and the second is the sample.

584
01:06:48.840 --> 01:06:50.120
Input

585
01:06:50.300 --> 01:06:56.920
Nilesh Gupta: So if you call this, this will write to Tensor both. And here you can visualize the network

586
01:06:57.080 --> 01:07:04.800
Nilesh Gupta: pretty neatly like it will come into this graph subsection. So if you take a look like this is the input

587
01:07:05.800 --> 01:07:15.370
Nilesh Gupta: and. as you can see, like I was visualizing 4 data points. So, like the input shape is 4. Cost 3 goes 32, plus 32.

588
01:07:15.740 --> 01:07:20.270
Nilesh Gupta: Which makes sense. This is the input. And you can also visualize your network.

589
01:07:20.320 --> 01:07:23.360
Nilesh Gupta: And well. yeah.

590
01:07:23.930 --> 01:07:40.010
Nilesh Gupta: So this is your network like your this is the flattening operation you are doing here. Then your fully connected layer to fully go the F. C. One and F. C. 2, and in between. This is the like. If you click on it, it will say, operation value.

591
01:07:40.240 --> 01:07:43.610
Nilesh Gupta: So, like all operations appear, and oval shapes.

592
01:07:43.670 --> 01:08:02.680
Nilesh Gupta: and the modules that you have which have parameters appear in this square things, and you can like click on it again to like, expand it. And this will show you that it is taking this as input and using the weight and biases as parameters to compute the output.

593
01:08:02.790 --> 01:08:03.890
Nilesh Gupta: Well, from this.

594
01:08:04.220 --> 01:08:17.149
Nilesh Gupta: So this is a pretty simple network, so like it's a kind of even without the graph it is pretty easy to know, like what is going on. But if you have like more and more complicated things like

595
01:08:17.170 --> 01:08:33.330
Nilesh Gupta: this, visualization can be very helpful in figuring out what exactly is going on in your forward path, or in general, like if you are looking at someone else's network, and you want to figure out how to go. What exactly is happening. You can use these visualizations to do that.

596
01:08:34.470 --> 01:08:37.010
Nilesh Gupta: So yeah.

597
01:08:37.180 --> 01:08:47.850
Nilesh Gupta: hmm. So this I I was able to do this by calling this writer.com. and the passing net and sample images

598
01:08:47.910 --> 01:08:49.410
Nilesh Gupta: as in. Input

599
01:08:50.779 --> 01:08:54.210
Nilesh Gupta: So this is about visualizing that

600
01:08:54.380 --> 01:09:06.660
Nilesh Gupta: the next part is like, i'm creating like typically like you call a loss as a criterion where your loss function is criter. And so i'm creating this loss function as N. Dot cross entropy loss.

601
01:09:06.920 --> 01:09:18.700
Nilesh Gupta: and the optimizer I'm using is as Gd. You can call it, like this, OP: them dot Sgd. And you have to pass the parameters of your network as input and the learning rate.

602
01:09:18.800 --> 01:09:32.210
Nilesh Gupta: So there are other parameters possible as well. But like since we have only looked at the vanilla version of Sd. So like, you will only take a look at the learning rate for down the basic argument which is the network parameters.

603
01:09:32.430 --> 01:09:36.800
Nilesh Gupta: So this will create the optimizer and the loss function for you.

604
01:09:37.080 --> 01:09:45.840
Nilesh Gupta: And this is the training loop I have like I am doing for you talking range, too, so i'm only like I like doing 2 epoch training.

605
01:09:46.029 --> 01:09:59.560
Nilesh Gupta: and then, like for each, she pop like I'm. I trading over all of my training data like and my, as, as as I said earlier, like the iterator that i'll use for the training data. Is there going to be train loaders?

606
01:10:00.450 --> 01:10:01.590
Nilesh Gupta: So

607
01:10:01.930 --> 01:10:05.290
Nilesh Gupta: i'm integrating over all my training data.

608
01:10:05.420 --> 01:10:08.090
getting the input and the labels.

609
01:10:08.830 --> 01:10:17.730
Nilesh Gupta: This is also like you want to d0 0r you want to d0 0ptimizer dot 0 that. So why do you need to do this, because

610
01:10:17.830 --> 01:10:24.540
Nilesh Gupta: once you have calculated the backward gradient. and let's say, like you have not zeroed them out.

611
01:10:24.580 --> 01:10:38.960
Nilesh Gupta: and in the next iteration you do another back propagation, the great. So the default functionality of Pi Torches that it will sum the gradients up. You will still, like your previous gradient, and your new gradients will be summed up.

612
01:10:39.070 --> 01:10:55.140
Nilesh Gupta: So this is not the behavior that you want you want like in every it. You want to calculate fresh gradients, and you want to do updates based on that. So that's why you need to call this optimizer dot 0 grad, which will 0 0ut all the gradients that this optimizer is tracking.

613
01:10:56.370 --> 01:10:57.440
Nilesh Gupta: So

614
01:10:57.750 --> 01:11:04.960
Nilesh Gupta: this is one non-trivial bit that you need to be careful about that. Yeah, You want to call this your grad before.

615
01:11:05.170 --> 01:11:14.090
Nilesh Gupta: and then you are doing net dot forward. You are calling the forward propagation on the input and it is giving you the output

616
01:11:14.230 --> 01:11:21.050
Nilesh Gupta: You' like you are computing your loss function on the outputs and the labels that you have.

617
01:11:21.360 --> 01:11:25.300
Nilesh Gupta: So this will give you loss. Then you are calling the lost out backward.

618
01:11:25.420 --> 01:11:32.110
Nilesh Gupta: which will do the back propagation for you, and compute all the gradients for all the parameters in the network that you had.

619
01:11:32.330 --> 01:11:41.340
Nilesh Gupta: And then you are doing this optimizer dot step. So this is actually performing this. The wait updates. So this only computed the gradients.

620
01:11:41.400 --> 01:11:58.330
Nilesh Gupta: Now, like we' in in gradient descent like you want to update your weights by W. Equal to W. Minus learning rate times the gradients. So this will do that for you like optimizer dot step. And here i'm just kind of

621
01:11:58.630 --> 01:12:10.960
Nilesh Gupta: keeping track of some statistics. One thing that you will realize is that i'm keeping track of this training loss, and i'm writing it to my tensor board writer. So this this will like. Let you

622
01:12:10.990 --> 01:12:16.380
Nilesh Gupta: keep track of for the training loss, and it will plot a nice curve for you

623
01:12:16.410 --> 01:12:25.940
Nilesh Gupta: of learning of your training loss against the number of as as you keep on progressing, so i'll just run this.

624
01:12:26.140 --> 01:12:33.440
Nilesh Gupta: and so like i'm doing this for every 2,000 iteration the also.

625
01:12:33.510 --> 01:12:38.850
Nilesh Gupta: It's usually a good idea to like, not not at every step you print that. But

626
01:12:39.080 --> 01:12:40.090
Nilesh Gupta: so.

627
01:12:41.260 --> 01:12:48.270
Nilesh Gupta: yeah, this I started, and if I just do a rewrote here. There will be a tab called scalars.

628
01:12:48.620 --> 01:12:53.190
Nilesh Gupta: and this will plot the training loss so like it is plotting that loss for me.

629
01:12:53.470 --> 01:12:56.320
Nilesh Gupta: And so this like I can

630
01:12:57.550 --> 01:13:02.450
Nilesh Gupta: keep track of how my training losses, the

631
01:13:02.550 --> 01:13:13.170
Nilesh Gupta: you know. evolving over time so so like. One thing to note here is like this is not active page. You have to do a reload

632
01:13:13.290 --> 01:13:17.210
Nilesh Gupta: to like, see the new content. So yeah.

633
01:13:17.930 --> 01:13:24.030
Nilesh Gupta: So like, if I just do a reload like this will keep on getting updated to the new state that I have.

634
01:13:27.240 --> 01:13:29.190
Nilesh Gupta: Yeah. So

635
01:13:29.450 --> 01:13:35.470
Nilesh Gupta: this was kind of the I think. Yeah, I'm: almost at time I'll just so

636
01:13:35.630 --> 01:13:51.770
Nilesh Gupta: say few things which is like you can see after every training you want to save your network, so you can do by calling this tors dot save of net dot state D, and give up. Give it a path. It will store your network weights here.

637
01:13:51.860 --> 01:13:56.570
Nilesh Gupta: and you can load your network by calling this net dot load state debt.

638
01:13:56.620 --> 01:13:59.540
Nilesh Gupta: and torched out load of path, so it will.

639
01:14:01.030 --> 01:14:12.450
Nilesh Gupta: It can retrieve those saved weights from that path. And here like if we can like, evaluate like how on on the not test data like.

640
01:14:12.500 --> 01:14:20.420
Nilesh Gupta: How like what's the accuracy? So for in this case, since we were using a very simple network like it's only 45%,

641
01:14:20.540 --> 01:14:30.690
Nilesh Gupta: but it's still better than random guessing, because, like if you were randomly guessing, it would have been 10%. So, although, like the the C 5 10 is kind of almost solved

642
01:14:30.820 --> 01:14:32.100
Nilesh Gupta: like that, I

643
01:14:32.450 --> 01:14:36.280
networks can easily achieve 99% accuracy on this. But

644
01:14:36.480 --> 01:14:39.340
Nilesh Gupta: yeah.

645
01:14:39.350 --> 01:14:43.730
like since we are using the simple, simple network, like we are only getting 45%.

646
01:14:44.020 --> 01:14:50.190
Nilesh Gupta: So that said that's the usual flow like the

647
01:14:50.580 --> 01:14:55.960
Nilesh Gupta: the standard way of writing a deep learning solution in python.

648
01:14:57.770 --> 01:15:02.530
I think, like, yeah, we are almost at time. So

649
01:15:02.620 --> 01:15:06.460
Nilesh Gupta: i'll stop, and i'll stay here. If you guys have any questions.

650
01:15:09.550 --> 01:15:10.140
Nilesh Gupta: Yeah.

651
01:15:14.890 --> 01:15:16.450
Yuhan Gao: Hello, Nilish.

652
01:15:16.600 --> 01:15:17.260
Nilesh Gupta: Yeah.

653
01:15:17.580 --> 01:15:21.260
I have a question. So last last lecture

654
01:15:21.350 --> 01:15:23.080
Yuhan Gao: we talk about 2

655
01:15:23.110 --> 01:15:36.140
Yuhan Gao: to train a deep learning with a specific image. So, for example, you give the the networks of a lot of a lot of cats image, and then

656
01:15:36.810 --> 01:15:38.570
Yuhan Gao: you give

657
01:15:38.940 --> 01:15:40.770
Yuhan Gao: the the network

658
01:15:41.230 --> 01:15:43.070
Another test

659
01:15:43.390 --> 01:15:45.450
Yuhan Gao: like sets

660
01:15:45.510 --> 01:15:50.350
Yuhan Gao: to to to to identify if it's a a cat or not.

661
01:15:57.140 --> 01:16:00.740
Nilesh Gupta: Yeah, that's a supervised learning frame, like.

662
01:16:00.760 --> 01:16:10.050
Nilesh Gupta: So, by supervised learning, like, we only mean that, like the in your training data, you have a X and a Y like, there is an input and there is a target.

663
01:16:10.440 --> 01:16:17.010
Nilesh Gupta: So like, if there is no target. Then we like, say, it's not supervised. If there is a target like we say, it is a super wide.

664
01:16:17.570 --> 01:16:25.680
Yuhan Gao: Okay, so can the deep learning model that we learned last lecture. Do the I supervise learning.

665
01:16:26.700 --> 01:16:32.680
Nilesh Gupta: So what do you mean by unsupervised like? What kind of a specific problem that you're

666
01:16:32.740 --> 01:16:34.970
Nilesh Gupta: talking about that

667
01:16:35.230 --> 01:16:49.690
Yuhan Gao: does the same, does it? The same question like your you? If the now works a bunch of images and then light it to under the 5 which is got cats, which one is a lion.

668
01:16:49.700 --> 01:16:51.420
Yuhan Gao: or which one is a dog.

669
01:16:51.460 --> 01:16:52.970
But when you train

670
01:16:53.070 --> 01:16:58.490
Yuhan Gao: the data set you are not giving any labels, any target. I see.

671
01:16:58.740 --> 01:17:12.570
Nilesh Gupta: So the the network that we that I was discussing in this class and the last class that network can do it right, because the loss function that we are using

672
01:17:12.820 --> 01:17:15.320
Nilesh Gupta: it does need a label or a

673
01:17:15.440 --> 01:17:17.940
Nilesh Gupta: There are ways of doing that.

674
01:17:18.170 --> 01:17:28.530
Yuhan Gao: like without knowing what are my labels like.

675
01:17:28.720 --> 01:17:29.910
Yuhan Gao: And

676
01:17:30.000 --> 01:17:39.350
Nilesh Gupta: yeah, there are definitely ways and like deep learning methods can do that. And most likely, like the state of the art, there is going to be some deep learning method.

677
01:17:46.130 --> 01:17:49.840
Nilesh Gupta: so will will come to those things later in the course.

678
01:17:50.130 --> 01:17:52.980
Nilesh Gupta: but not the specific

679
01:17:53.290 --> 01:18:03.850
Nilesh Gupta: network that we talked about. Can't do that. But like, there are ways of training and like formulating the problem in which a deep learning method can

680
01:18:04.120 --> 01:18:06.990
solve that unsupervised program as well.

681
01:18:07.110 --> 01:18:08.370
Nilesh Gupta: But try to solve them.

682
01:18:08.860 --> 01:18:09.550
Yuhan Gao: Okay.

683
01:18:10.380 --> 01:18:10.980
Nilesh Gupta: Thank you.

684
01:18:11.310 --> 01:18:12.760
Yuhan Gao: Alright, Thank you.

685
01:18:32.820 --> 01:18:45.300
Nilesh Gupta: All right. If there are no questions I will drop Thanks. Everyone by sending the class and by next lecture or in the

686
01:18:46.670 --> 01:18:48.160
Nilesh Gupta: okay. Bye.

WEBVTT

1
00:02:02.430 --> 00:02:04.240
Inderjit Dhillon: Hey? Good afternoon, everybody.

2
00:02:05.580 --> 00:02:07.140
Inderjit Dhillon: Hopefully. You can hear me

3
00:02:12.580 --> 00:02:29.440
Inderjit Dhillon: so before I start any questions, you know the last 3 lectures have been taught by Niles. And you know, if I recall what I was doing before we talked about logistic regression.

4
00:02:29.760 --> 00:02:35.410
Inderjit Dhillon: then what was taught 3 weeks ago was that, hey? You know, these are

5
00:02:37.080 --> 00:02:42.590
Inderjit Dhillon: both regression and classification can be thought of in a similar

6
00:02:42.870 --> 00:02:53.940
Inderjit Dhillon: erm or empirical, just minimization framework. where you can have different loss functions that capture, whether it is a regression problem or a classification problem.

7
00:02:54.050 --> 00:03:00.710
Inderjit Dhillon: Then the leash went into how you know you use the deep learning framework to solve

8
00:03:00.730 --> 00:03:11.740
Inderjit Dhillon: regression as well as classification problems, and then talked a little bit about forward propagation, backward propagation, which is how

9
00:03:11.760 --> 00:03:22.010
Inderjit Dhillon: neural networks are implemented. The optimization, the training is done using optimization techniques such as stochastic, gradient descent.

10
00:03:22.260 --> 00:03:28.780
Inderjit Dhillon: And then finally he talked about how to get everything together in a.

11
00:03:28.960 --> 00:03:37.140
Inderjit Dhillon: And just you know, we've already put out homework. which is going to be due next Friday.

12
00:03:37.190 --> 00:03:52.610
Inderjit Dhillon: and that has a a question about, you know, implementing in by torch, and I believe Malaysia already kind of went through that kind of problem or kind of approach in

13
00:03:53.100 --> 00:03:55.520
Inderjit Dhillon: the class last Wednesday.

14
00:03:56.820 --> 00:04:13.220
Inderjit Dhillon: Okay, so that's kind of a recap of what happened the last 3 lectures. So any questions on any of that. So remember the homework 2 is out and homework 2 homework. 2 is due on Friday of next week.

15
00:04:17.620 --> 00:04:21.260
Inderjit Dhillon: Okay, if there are no questions, and I'm going to start today's lecture.

16
00:04:23.010 --> 00:04:26.810
Inderjit Dhillon: Let me see. I have to share screen.

17
00:04:50.950 --> 00:04:55.270
Inderjit Dhillon: Okay. So today we will

18
00:04:58.360 --> 00:05:01.570
Inderjit Dhillon: let me just get this one.

19
00:05:16.660 --> 00:05:17.420
Inderjit Dhillon: Okay.

20
00:05:17.460 --> 00:05:30.010
Inderjit Dhillon: So today we'll talk. We'll talk a little bit more about classification and talk actually about some classical techniques for classification. Okay. So the first technique. So this is

21
00:05:30.740 --> 00:05:32.580
Inderjit Dhillon: notification.

22
00:05:36.280 --> 00:05:40.740
Inderjit Dhillon: And then one of the take the first technique I will talk about today

23
00:05:41.280 --> 00:05:51.250
Inderjit Dhillon: is something that you might have. What about very important quantity or very important concept

24
00:05:51.450 --> 00:05:56.950
Inderjit Dhillon: in machine learning? And AI actually was done a long time ago. Right? It was first

25
00:05:58.150 --> 00:05:59.420
Inderjit Dhillon: proposed

26
00:06:00.050 --> 00:06:03.110
Inderjit Dhillon: more than 60 years ago. Okay.

27
00:06:03.470 --> 00:06:07.510
Inderjit Dhillon: First proposed in 1962.

28
00:06:07.950 --> 00:06:11.880
Inderjit Dhillon: It has a lot of kind of rich historical context. Also

29
00:06:12.010 --> 00:06:23.000
Inderjit Dhillon: So let's see what a perceptron does. Okay. So remember. we are in this classification problem setting where I have.

30
00:06:23.620 --> 00:06:24.480
Inderjit Dhillon: you know

31
00:06:24.580 --> 00:06:26.430
Inderjit Dhillon: one.

32
00:06:27.460 --> 00:06:29.920
Inderjit Dhillon: I have those 2,

33
00:06:31.390 --> 00:06:37.280
Inderjit Dhillon: one represented by crosses, the other represented by little circles.

34
00:06:37.510 --> 00:06:49.420
Inderjit Dhillon: and we talked about, you know, trying to have like a separating hyperplane between these classes, as one way to decide whether a new prime that comes along

35
00:06:49.910 --> 00:06:54.470
Inderjit Dhillon: is in class, one or class 2. Okay. So suppose

36
00:06:54.580 --> 00:06:59.080
Inderjit Dhillon: this is class 2, and this is class one.

37
00:06:59.310 --> 00:07:07.210
Inderjit Dhillon: Okay. And the way i'm going to import it is that the Y for these points will be plus one.

38
00:07:07.340 --> 00:07:13.790
Inderjit Dhillon: And the why i' for this points will be minus one so plus one for class, one

39
00:07:13.960 --> 00:07:25.760
Inderjit Dhillon: minus one for class 2. So again a binary classification problem. Okay. And we talked about, you know, getting these separating hyperplanes

40
00:07:27.220 --> 00:07:37.550
Inderjit Dhillon: initially when we started talking about it. We talked about the separating hyperplane that you can get when you kind of make a modeling assumption that the classes

41
00:07:37.590 --> 00:07:43.960
Inderjit Dhillon: Gaussian, each class is Gaussian. but with the

42
00:07:44.140 --> 00:07:54.860
Inderjit Dhillon: you know, uniform covariance, matrix or an identity covariance matrix. Then we saw that even if you make the Covariance matrices

43
00:07:55.130 --> 00:08:10.370
Inderjit Dhillon: equal for the 2 classes some convenience matrix, Sigma, but the same. Then you still get a linear separating surface. And of course, we also then saw logistic regression, which also gets you a linear separating surface.

44
00:08:11.220 --> 00:08:18.490
Inderjit Dhillon: So what does Perceptron do? Well, it actually does things a bit differently.

45
00:08:18.870 --> 00:08:20.400
Inderjit Dhillon: And

46
00:08:21.830 --> 00:08:23.870
Inderjit Dhillon: we

47
00:08:24.090 --> 00:08:30.070
Inderjit Dhillon: say that you know, for example, let this be the hyperplane. Let me say that the normal to the hyperplane is

48
00:08:30.160 --> 00:08:48.000
Inderjit Dhillon: W. And of course, if I look at the normalized vector in the direction of W. It is W. Divided by Norm of W. So that's the unit vector which is normal to this hyperplane. Now suppose I take any point

49
00:08:48.150 --> 00:08:52.680
Inderjit Dhillon: any point over here, and let me say that this point is

50
00:08:52.800 --> 00:09:02.480
Inderjit Dhillon: X. Okay? And if I look at I can ask the question, what is the distance of this point

51
00:09:02.690 --> 00:09:10.260
Inderjit Dhillon: to the hyperplane. Okay. So if you remember, if I take any point on the hyperplane, which is x not.

52
00:09:10.750 --> 00:09:12.630
Inderjit Dhillon: and if this is X not.

53
00:09:13.240 --> 00:09:16.650
Inderjit Dhillon: And then you can think of this as

54
00:09:19.220 --> 00:09:22.260
Inderjit Dhillon: X minus X. Naught is this. Vector

55
00:09:22.740 --> 00:09:27.990
Inderjit Dhillon: and then i'm projecting it on to W, divided by norm of W.

56
00:09:28.140 --> 00:09:32.700
Inderjit Dhillon: So kind of if I change. let me just colors.

57
00:09:32.900 --> 00:09:38.260
Inderjit Dhillon: So this distance, My, the question is, what is this distance?

58
00:09:38.870 --> 00:09:44.920
Inderjit Dhillon: Okay. and this distance? Let me write it out again right. So remember

59
00:09:45.030 --> 00:09:53.860
Inderjit Dhillon: what I just said, I said, this is the projection of X minus X naught on to the hyperplane

60
00:09:53.950 --> 00:09:57.510
Inderjit Dhillon: where the normal is W. So

61
00:09:57.660 --> 00:10:02.840
Inderjit Dhillon: it will be the unit vector in this direction. So that's W. Trans.

62
00:10:03.270 --> 00:10:07.340
Inderjit Dhillon: In a product with X minus X. Now.

63
00:10:09.500 --> 00:10:17.260
Inderjit Dhillon: okay, so that's the signed distance. So remember we had covered this before. So this is the signed distance

64
00:10:19.190 --> 00:10:20.990
Inderjit Dhillon: of X

65
00:10:22.020 --> 00:10:24.100
Inderjit Dhillon: to the hyperplane.

66
00:10:27.600 --> 00:10:32.340
Inderjit Dhillon: Okay, and that's given by this quantity over here.

67
00:10:32.770 --> 00:10:37.460
Inderjit Dhillon: Okay, remember the signed means. It could be positive or negative.

68
00:10:37.590 --> 00:10:42.740
Inderjit Dhillon: so it'll be positive in the direction of when X is in the direction of W.

69
00:10:43.010 --> 00:10:54.000
Inderjit Dhillon: And so for Class C one it'll be positive. But for Class C 2, the sign distance will actually be negative. Okay. So remember that you know this

70
00:10:54.240 --> 00:10:58.610
Inderjit Dhillon: the hyperplane is given by W. Transpose x

71
00:10:58.840 --> 00:11:02.880
Inderjit Dhillon: plus w naught equal to 0.

72
00:11:03.180 --> 00:11:04.680
Inderjit Dhillon: Maybe

73
00:11:05.230 --> 00:11:06.930
Inderjit Dhillon: let me make it clear

74
00:11:10.860 --> 00:11:17.850
Inderjit Dhillon: W. Transpose x plus w naught equals 0. That is this hyperfine.

75
00:11:19.930 --> 00:11:27.780
Inderjit Dhillon: Okay. So this is equal to W. Transpose X minus W. Transpose X: not

76
00:11:27.870 --> 00:11:31.410
Inderjit Dhillon: new item by norm of W.

77
00:11:32.460 --> 00:11:41.380
Inderjit Dhillon: Okay. And remember that W. Transpose x not lies on this hyperplane right? So in particular

78
00:11:41.700 --> 00:11:50.850
Inderjit Dhillon: of the fact that x not lies in the cyber on the hyperplane means that W. Transpose X: not plus w not equal to 0.

79
00:11:51.580 --> 00:11:57.340
Inderjit Dhillon: Yeah, because that is the equation of all the points that lie on this particular hyperplane.

80
00:11:57.480 --> 00:12:03.480
Inderjit Dhillon: So that means that W. Transpose X not is equal to minus top.

81
00:12:04.490 --> 00:12:12.450
Inderjit Dhillon: which means that this is equal to W. Transpose X plus w, naught divided by

82
00:12:12.930 --> 00:12:15.500
Inderjit Dhillon: okay. And remember this is the 2 norm of

83
00:12:18.010 --> 00:12:26.460
Inderjit Dhillon: so what I can then say is that for remember what I said about the signed distance.

84
00:12:26.560 --> 00:12:30.510
Inderjit Dhillon: So let me repeat that over here for Class C, one

85
00:12:31.100 --> 00:12:35.000
Inderjit Dhillon: W transpose or class, you one remember, is this class.

86
00:12:36.060 --> 00:12:36.790
Inderjit Dhillon: Okay.

87
00:12:36.900 --> 00:12:39.450
Inderjit Dhillon: W. Transpose X: I

88
00:12:39.760 --> 00:12:50.430
Inderjit Dhillon: plus W. Naught is greater than 0. Okay. So what this means is gonna be for X. I belong to plus C one. This is true.

89
00:12:50.690 --> 00:12:54.850
Inderjit Dhillon: Okay. So this is for let me just repeat that for all

90
00:12:55.370 --> 00:13:00.110
Inderjit Dhillon: points. So let me actually raise this part.

91
00:13:01.600 --> 00:13:04.420
Inderjit Dhillon: Okay, for all points. X. I.

92
00:13:04.860 --> 00:13:06.830
Inderjit Dhillon: In C, one

93
00:13:07.970 --> 00:13:11.310
Inderjit Dhillon: that are correctly classified.

94
00:13:17.520 --> 00:13:31.790
Inderjit Dhillon: So remember, right now this is picture that i'm drawing of the hyperplane which is separating the 2 surfaces. But initially we won't be given such a hyperplane. We may start off with some random w and some random w, not

95
00:13:31.960 --> 00:13:36.420
Inderjit Dhillon: right, but for the points that are correctly classified.

96
00:13:36.660 --> 00:13:44.220
Inderjit Dhillon: Then you can see that W. Transpose xi plus W. Naught, which is this quantity over here the numerator

97
00:13:44.540 --> 00:13:46.350
Inderjit Dhillon: right for

98
00:13:46.360 --> 00:14:01.480
Inderjit Dhillon: all points in the C one. This quality will be greater than, or equal to 0, greater than 0. And then for Class C 2, I have the W. Transpose X: I plus w, not

99
00:14:01.680 --> 00:14:06.750
Inderjit Dhillon: It's actually less than 0. Okay, this is for all points

100
00:14:07.900 --> 00:14:12.270
Inderjit Dhillon: X, I and C 2 that

101
00:14:14.690 --> 00:14:17.140
Inderjit Dhillon: all directly.

102
00:14:24.740 --> 00:14:32.920
Inderjit Dhillon: Okay. So remember the encoding that I have. I have. Yi is plus one for C one.

103
00:14:34.190 --> 00:14:39.240
Inderjit Dhillon: and Y. I is minus 1, 4, plus c, 2.

104
00:14:40.490 --> 00:14:44.450
Inderjit Dhillon: Okay. So if I think about Class C one, just look at it.

105
00:14:44.980 --> 00:14:52.030
Inderjit Dhillon: What is y I? What is the sign of Y. I. Times W. Transpose X: I:

106
00:14:54.530 --> 00:14:58.480
Inderjit Dhillon: Okay. So for the points that are correctly classified.

107
00:14:59.370 --> 00:15:04.510
Inderjit Dhillon: Okay, again, let me do you some more. Give myself a little bit more space.

108
00:15:04.700 --> 00:15:08.160
Inderjit Dhillon: All the points that are correctly classified.

109
00:15:10.240 --> 00:15:15.340
Inderjit Dhillon: W. Transpose Xi. Plus w naught when I multiply it by.

110
00:15:15.910 --> 00:15:17.340
Inderjit Dhillon: Why, I

111
00:15:19.930 --> 00:15:24.830
Inderjit Dhillon: then this is greater than 0. Let me again write this for

112
00:15:26.070 --> 00:15:27.450
Inderjit Dhillon: directly

113
00:15:30.480 --> 00:15:31.900
Inderjit Dhillon: classified

114
00:15:34.410 --> 00:15:35.650
Inderjit Dhillon: points

115
00:15:37.580 --> 00:15:38.830
Inderjit Dhillon: excellent.

116
00:15:39.560 --> 00:15:42.680
Inderjit Dhillon: and C one or C 2,

117
00:15:43.310 --> 00:15:57.760
Inderjit Dhillon: and this holds so. Just make sure that you know. So remember, for Class C one y I is one w transpose xi plus w naught is greater than 0. So y I

118
00:15:57.900 --> 00:16:10.000
Inderjit Dhillon: time w transpose xi plus, why not? Is greater than 0, whereas over here or Class C 2, I have that Y. I is negative one.

119
00:16:10.610 --> 00:16:13.600
Inderjit Dhillon: right? So I have negative Y. I

120
00:16:13.850 --> 00:16:17.750
Inderjit Dhillon: and I also have negative W. Transpose Xi plus w not?

121
00:16:18.060 --> 00:16:24.630
Inderjit Dhillon: Which means that this statement is true. Okay, for directly classified points.

122
00:16:24.750 --> 00:16:31.160
Inderjit Dhillon: Y: I. Times W. Transpose Xi. Plus W. Naught is greater than 0.

123
00:16:31.920 --> 00:16:36.600
Inderjit Dhillon: And what does that mean? Well. for

124
00:16:38.340 --> 00:16:40.660
Inderjit Dhillon: misclassified points.

125
00:16:42.670 --> 00:16:43.600
Inderjit Dhillon: Okay.

126
00:16:46.810 --> 00:16:51.900
Inderjit Dhillon: Now for misclassified points, what will happen is that this is no longer going to be true.

127
00:16:52.240 --> 00:16:55.950
Inderjit Dhillon: This will not be true. Right? So

128
00:16:56.330 --> 00:17:00.850
Inderjit Dhillon: W. Transpose xi plus w, naught will actually be less than 0,

129
00:17:06.880 --> 00:17:09.339
Inderjit Dhillon: and that is for plus c one.

130
00:17:09.760 --> 00:17:15.119
Inderjit Dhillon: And so what happens is that if I look at again the quantity above

131
00:17:15.819 --> 00:17:21.000
Inderjit Dhillon: this is what happens. which is that for mis classified points.

132
00:17:23.170 --> 00:17:31.610
Inderjit Dhillon: Your prediction basically does not match. So the prediction with the current W. Which is W. Transpose Xi plus W. Not

133
00:17:32.440 --> 00:17:36.590
Inderjit Dhillon: okay, does not match the correct. Why, I

134
00:17:37.530 --> 00:17:41.140
Inderjit Dhillon: So this is a measure of

135
00:17:41.800 --> 00:17:46.390
Inderjit Dhillon: You know the improper job

136
00:17:46.530 --> 00:17:50.530
Inderjit Dhillon: that the hyperplane is currently doing

137
00:17:50.690 --> 00:17:55.510
Inderjit Dhillon: so. What you want to do is, you want to try and reduce the number of misclassified points.

138
00:17:55.690 --> 00:18:04.740
Inderjit Dhillon: So you want to basically change the classification you want to modify the hyperplane, so that this classification is changed.

139
00:18:05.090 --> 00:18:18.380
Inderjit Dhillon: So, as a result, the perception, perceptron, criterion becomes the negative of this quantity. Okay, so let me write that down. So the perceptron is going to be an algorithm

140
00:18:18.460 --> 00:18:22.270
Inderjit Dhillon: that'll keep on modifying W. And W. Not

141
00:18:22.470 --> 00:18:32.850
Inderjit Dhillon: until it starts classifying points correctly. Right. Now, just assume that all the points are, or the 2 classes are linearly separable.

142
00:18:32.960 --> 00:18:37.440
Inderjit Dhillon: which means that direct just a hyperplane that can separate class, c. One

143
00:18:37.550 --> 00:18:39.150
Inderjit Dhillon: from class, c. 2.

144
00:18:39.740 --> 00:18:50.140
Inderjit Dhillon: So the perceptron criterion. Okay, it's going to be so. It's a function of it's, you know some

145
00:18:50.220 --> 00:18:55.350
Inderjit Dhillon: function of W. And W. Not. Those are the parameters

146
00:18:55.580 --> 00:19:00.990
Inderjit Dhillon: and is going to be. I'm gonna keep it as a positive quantity.

147
00:19:01.290 --> 00:19:05.230
Inderjit Dhillon: So i'm going to make it as minus of this.

148
00:19:05.510 --> 00:19:19.530
Inderjit Dhillon: Thank you. Okay. So minus of why I W. Transpose Xi plus. But this summation is only over.

149
00:19:19.990 --> 00:19:23.100
Inderjit Dhillon: That's called the set M and

150
00:19:23.200 --> 00:19:26.120
Inderjit Dhillon: the set M indexes

151
00:19:26.450 --> 00:19:29.160
Inderjit Dhillon: the misclassified.

152
00:19:36.990 --> 00:19:39.490
Inderjit Dhillon: Okay. And what we want to do is

153
00:19:39.760 --> 00:19:43.500
Inderjit Dhillon: this quantity? November is not positive. So this quantity

154
00:19:44.650 --> 00:19:45.580
Inderjit Dhillon: okay

155
00:19:45.720 --> 00:19:56.960
Inderjit Dhillon: for miss classified points we studied. We looked at it, and Y. I times W. Transpose Xi. Plus W. Naught is less than 0. I have a negative sign outside it. So this quantity is

156
00:19:57.020 --> 00:20:00.980
Inderjit Dhillon: positive. So what I want to do is, I want to find

157
00:20:01.390 --> 00:20:02.690
Inderjit Dhillon: W.

158
00:20:03.830 --> 00:20:08.350
Inderjit Dhillon: And W. Not okay. That kind of minimizes this

159
00:20:09.250 --> 00:20:10.140
Inderjit Dhillon: quantity.

160
00:20:12.450 --> 00:20:16.570
Inderjit Dhillon: Any questions so far in the formulation of the perceptron criterion.

161
00:20:26.380 --> 00:20:27.190
Inderjit Dhillon: Okay.

162
00:20:27.420 --> 00:20:35.250
Inderjit Dhillon: So remember this the larger this quantity is, that means the worse the perceptron is doing on your current training. Set

163
00:20:35.450 --> 00:20:38.240
Inderjit Dhillon: the smaller of this quantity. D is

164
00:20:38.290 --> 00:20:45.340
Inderjit Dhillon: the better it is doing so. I want to try and decrease it right to 0, because there exists

165
00:20:45.450 --> 00:20:51.300
Inderjit Dhillon: a hyperplane. So eventually this set M should become the empty set.

166
00:20:52.040 --> 00:20:52.780
Inderjit Dhillon: Okay.

167
00:20:53.360 --> 00:21:02.010
Inderjit Dhillon: So now, how do we kind of optimize it? Well, let's look at You know we've we've been talking about training.

168
00:21:02.090 --> 00:21:07.300
Inderjit Dhillon: getting an objective function, and if you want to try and minimize the objective function

169
00:21:07.310 --> 00:21:15.130
Inderjit Dhillon: we use, we could potentially use gradient descent. right? So we look at the gradient. We look at the negative of the gradient.

170
00:21:15.450 --> 00:21:20.970
Inderjit Dhillon: right? And we move in that direction. For that we need the gradient. Okay. So what is the gradient?

171
00:21:21.620 --> 00:21:26.140
Inderjit Dhillon: There are 2 quantities that I have, which is W. Which is

172
00:21:26.240 --> 00:21:28.810
Inderjit Dhillon: the normal to the hyperplane.

173
00:21:30.500 --> 00:21:34.100
Inderjit Dhillon: and then I have w not, which is a scalar.

174
00:21:34.370 --> 00:21:36.340
Inderjit Dhillon: We just kind of like the

175
00:21:40.210 --> 00:21:43.540
Inderjit Dhillon: some people call it the biased term of the hyperplane.

176
00:21:48.440 --> 00:21:49.290
Inderjit Dhillon: Okay.

177
00:21:49.410 --> 00:21:53.870
Inderjit Dhillon: So what is the gradient of this? Well, I look at this quantity.

178
00:21:56.540 --> 00:22:00.490
Inderjit Dhillon: I need to take the gradient with respect to W. If I look at it.

179
00:22:00.700 --> 00:22:03.170
Inderjit Dhillon: W. Has this term.

180
00:22:05.380 --> 00:22:09.970
Inderjit Dhillon: and so the gradient is summation. I

181
00:22:10.700 --> 00:22:16.280
Inderjit Dhillon: over the misclassified points, and it is just Y. I Times X.

182
00:22:16.740 --> 00:22:18.990
Inderjit Dhillon: And of course there is a negative sign outside.

183
00:22:20.070 --> 00:22:23.040
Inderjit Dhillon: Okay. Similarly for W. Not

184
00:22:23.440 --> 00:22:35.160
Inderjit Dhillon: it is. I have the minus sign outside. and I will belongs to M. And it is well why I. Times w not is

185
00:22:35.440 --> 00:22:47.660
Inderjit Dhillon: the our term in D, which depends upon W. Not so only why I survive. Okay. If I want to write this and make it down into the classes.

186
00:22:47.850 --> 00:22:50.650
Inderjit Dhillon: then I can say that this is

187
00:22:51.520 --> 00:22:55.010
Inderjit Dhillon: formation of I bill that are

188
00:22:55.400 --> 00:22:59.080
Inderjit Dhillon: in M and and C. One

189
00:23:00.600 --> 00:23:03.950
Inderjit Dhillon: Y. I is plus one. So it is x I.

190
00:23:04.400 --> 00:23:08.240
Inderjit Dhillon: And for the points that belong to class 2

191
00:23:08.800 --> 00:23:10.870
Inderjit Dhillon: and are misclassified.

192
00:23:11.540 --> 00:23:15.610
Inderjit Dhillon: I have the My Yi is minus one. So this is

193
00:23:16.060 --> 00:23:17.430
Inderjit Dhillon: minus X.

194
00:23:19.120 --> 00:23:23.270
Inderjit Dhillon: Okay. So just to make sure that you're not confused on this.

195
00:23:26.020 --> 00:23:27.360
Inderjit Dhillon: and over here

196
00:23:27.480 --> 00:23:29.860
Inderjit Dhillon: it's essentially counting right.

197
00:23:30.260 --> 00:23:32.550
Inderjit Dhillon: Y. I is plus one

198
00:23:33.450 --> 00:23:37.950
Inderjit Dhillon: if it belongs to Class C one, and is misclassified right?

199
00:23:38.140 --> 00:23:50.670
Inderjit Dhillon: And if those number of points currently is n one. then this becomes minus n, one minus n 2. Okay. So just to clarify what n one and m 2 is where and I,

200
00:23:50.890 --> 00:23:53.940
Inderjit Dhillon: is the number

201
00:23:55.040 --> 00:23:57.890
Inderjit Dhillon: of misclassified

202
00:24:01.460 --> 00:24:02.790
Inderjit Dhillon: points.

203
00:24:04.880 --> 00:24:05.790
Inderjit Dhillon: And

204
00:24:11.520 --> 00:24:14.060
Inderjit Dhillon: okay. So now that I have a gradient.

205
00:24:14.390 --> 00:24:18.100
Inderjit Dhillon: I can essentially do gradient

206
00:24:18.760 --> 00:24:21.580
Inderjit Dhillon: descent. Okay.

207
00:24:21.780 --> 00:24:31.220
Inderjit Dhillon: But the perceptron does not actually do gradient descent. Okay, so let me first write the perceptron update rule. And remember, this is

208
00:24:31.300 --> 00:24:36.810
Inderjit Dhillon: a technique that was first developed in 1,962 so long time ago.

209
00:24:37.070 --> 00:24:41.050
Inderjit Dhillon: Okay, and so. perceptron.

210
00:24:42.780 --> 00:24:48.490
Inderjit Dhillon: I'll just give the update rule and then, that's connected to the gradient. We have above.

211
00:24:50.340 --> 00:24:57.200
Inderjit Dhillon: So what the perceptron does is it visits each Xi one at a time.

212
00:24:58.360 --> 00:25:08.730
Inderjit Dhillon: It looks at how. what the W. Transpose xi plus w not, is for that particular X I.

213
00:25:09.090 --> 00:25:20.320
Inderjit Dhillon: And if it is misclassified, it updates it. If it is not misclassified. it does not update it, and the update tool is W. W. Not

214
00:25:21.190 --> 00:25:24.670
Inderjit Dhillon: The new Ww. Is the old one.

215
00:25:26.230 --> 00:25:31.020
Inderjit Dhillon: Okay? No. A step size, Ada

216
00:25:43.810 --> 00:25:47.610
Inderjit Dhillon: Y. I. X. I. Why.

217
00:25:49.160 --> 00:25:58.050
Inderjit Dhillon: and if you look over here. it's essentially this, except it's looking at this particular misclassified point.

218
00:26:05.010 --> 00:26:07.270
Inderjit Dhillon: Okay. If

219
00:26:09.170 --> 00:26:11.040
Inderjit Dhillon: Xi is

220
00:26:18.750 --> 00:26:19.550
Inderjit Dhillon: okay.

221
00:26:20.540 --> 00:26:33.640
Inderjit Dhillon: so is this the gradient right now? Well, it's not the great full gradient, because the full gradient is summation over all I. Right. But this is the contribution of the gradient

222
00:26:33.760 --> 00:26:36.060
Inderjit Dhillon: from Xi. Right.

223
00:26:36.150 --> 00:26:41.400
Inderjit Dhillon: So this is what is called, and we touched upon it in the last lecture

224
00:26:41.670 --> 00:26:46.300
Inderjit Dhillon: or 2 lectures before it is stochastic radio descent.

225
00:26:48.790 --> 00:26:55.350
Inderjit Dhillon: So the perceptron is actually doing stochastic, gradient descent. which means it looks at

226
00:26:56.500 --> 00:27:01.460
Inderjit Dhillon: the Point X I. And Hey

227
00:27:01.760 --> 00:27:03.880
Inderjit Dhillon: X. I is

228
00:27:04.620 --> 00:27:06.230
Inderjit Dhillon: mis classified.

229
00:27:10.910 --> 00:27:12.070
Inderjit Dhillon: then

230
00:27:14.190 --> 00:27:16.210
Inderjit Dhillon: go in

231
00:27:16.360 --> 00:27:17.760
Inderjit Dhillon: direction

232
00:27:19.560 --> 00:27:21.430
Inderjit Dhillon: of the

233
00:27:22.560 --> 00:27:24.080
Inderjit Dhillon: negative.

234
00:27:25.290 --> 00:27:26.240
Inderjit Dhillon: Great.

235
00:27:29.300 --> 00:27:30.810
Inderjit Dhillon: Okay, but

236
00:27:32.980 --> 00:27:34.790
Inderjit Dhillon: but only

237
00:27:36.170 --> 00:27:37.980
Inderjit Dhillon: contributions

238
00:27:42.680 --> 00:27:44.730
Inderjit Dhillon: to gradient

239
00:27:46.420 --> 00:27:48.440
Inderjit Dhillon: by Xi.

240
00:27:49.650 --> 00:27:50.380
Inderjit Dhillon: Okay.

241
00:27:57.170 --> 00:28:03.390
Inderjit Dhillon: So here is the perceptron. Let me actually write the percept on pseudo code. It's actually very simple.

242
00:28:08.280 --> 00:28:10.040
Inderjit Dhillon: Do the code.

243
00:28:12.390 --> 00:28:15.120
Inderjit Dhillon: Okay. So what you do is you start

244
00:28:16.250 --> 00:28:19.130
Inderjit Dhillon: when some

245
00:28:20.510 --> 00:28:24.450
Inderjit Dhillon: W. And W. Not? Let's say random.

246
00:28:24.790 --> 00:28:26.680
Inderjit Dhillon: Okay. And then you repeat

247
00:28:29.660 --> 00:28:37.270
Inderjit Dhillon: the following steps: okay, so you do. for I equal to one to N.

248
00:28:37.500 --> 00:28:43.830
Inderjit Dhillon: I so remember assuming that you have x, I Y. I

249
00:28:45.810 --> 00:28:49.430
Inderjit Dhillon: from I equal to one to M

250
00:28:49.740 --> 00:28:51.030
Inderjit Dhillon: is the

251
00:28:59.550 --> 00:29:09.310
Inderjit Dhillon: okay good looks at all the points. Let's suppose in this order it could actually be, you know, some randomized order.

252
00:29:09.520 --> 00:29:19.790
Inderjit Dhillon: It could be, you know, cyclic order, but not necessarily one to one. But the first thing it does is it checks if the point is misclassified, so it says, if

253
00:29:20.360 --> 00:29:25.190
Inderjit Dhillon: y I w transpose x I plus w not

254
00:29:25.630 --> 00:29:28.330
Inderjit Dhillon: is less than 0. Them

255
00:29:28.960 --> 00:29:33.060
Inderjit Dhillon: make the following update, which is W. Gets

256
00:29:33.720 --> 00:29:36.300
Inderjit Dhillon: that's w plus

257
00:29:36.670 --> 00:29:39.600
Inderjit Dhillon: in the Y. I. Xi.

258
00:29:40.990 --> 00:29:46.070
Inderjit Dhillon: The we not also changes on w not plus either.

259
00:29:46.620 --> 00:29:47.490
Inderjit Dhillon: Why not?

260
00:29:48.610 --> 00:29:52.250
Inderjit Dhillon: Okay? So this is. And

261
00:29:54.290 --> 00:29:55.040
yeah.

262
00:29:57.540 --> 00:30:08.440
Inderjit Dhillon: if this quantity, the Y I. Times W. Transpose Xi plus W. Naught is greater than 0 means that the hyperplane is actually correctly classifying it. And so we don't need to do anything

263
00:30:09.100 --> 00:30:15.650
Inderjit Dhillon: and so. But if it is negative, then we make the updates. and then we and

264
00:30:19.380 --> 00:30:25.000
Inderjit Dhillon: okay. And this is what's called, you know, even in deep learning. Currently.

265
00:30:25.120 --> 00:30:32.400
Inderjit Dhillon: this is called one epoch. When you basically cycle through all the points from I equal to 1 2. And

266
00:30:32.510 --> 00:30:37.000
Inderjit Dhillon: so how many times does Perceptron go and do this?

267
00:30:37.270 --> 00:30:39.910
Inderjit Dhillon: Well, it does it till until

268
00:30:41.430 --> 00:30:42.560
Inderjit Dhillon: there

269
00:30:43.030 --> 00:30:45.200
Inderjit Dhillon: or no

270
00:30:45.850 --> 00:30:47.940
Inderjit Dhillon: misclassifications

271
00:30:52.440 --> 00:30:54.110
Inderjit Dhillon: or mystics

272
00:30:57.060 --> 00:30:58.680
Inderjit Dhillon: within the for

273
00:31:03.610 --> 00:31:07.260
Inderjit Dhillon: okay. So within the for loop you can have a count counter

274
00:31:07.420 --> 00:31:14.910
Inderjit Dhillon: which counts the number of mis classification. Then, If the number of misclassifications when you go through an entire epoch is 0, then you are done.

275
00:31:15.420 --> 00:31:19.500
Inderjit Dhillon: Okay? And this is the algorithm okay, so what does it output.

276
00:31:20.340 --> 00:31:25.300
Inderjit Dhillon: Okay, so the perceptron algorithm you can show by doing some

277
00:31:25.700 --> 00:31:27.120
Inderjit Dhillon: analysis

278
00:31:28.180 --> 00:31:30.380
Inderjit Dhillon: for Sept on algorithms

279
00:31:32.750 --> 00:31:34.400
Inderjit Dhillon: is guaranteed.

280
00:31:39.110 --> 00:31:42.280
Inderjit Dhillon: So you can actually show that it is guaranteed. There is a proof

281
00:31:42.590 --> 00:31:46.090
Inderjit Dhillon: that it is, it will find

282
00:31:46.870 --> 00:31:49.130
Inderjit Dhillon: a separating hyperplane

283
00:31:55.830 --> 00:31:58.940
Inderjit Dhillon: if the data

284
00:32:01.160 --> 00:32:03.280
Inderjit Dhillon: is linearly separable.

285
00:32:10.650 --> 00:32:18.190
Inderjit Dhillon: Okay. So that means if the data is like this. If it looks like this. where the

286
00:32:19.980 --> 00:32:31.930
Inderjit Dhillon: data points from one class can be separated from the data points from the second class by a linear surface. then the data is linearly separated. I think, in the homework you'll see that the first problem

287
00:32:31.940 --> 00:32:36.940
Inderjit Dhillon: is one where the data is not linearly separate. There is no hyperplane

288
00:32:37.040 --> 00:32:46.290
Inderjit Dhillon: that separates the data that belongs lies in these 2 concentric circles, as in the first problem in your homework. Okay.

289
00:32:47.420 --> 00:33:00.010
Inderjit Dhillon: So that's a good part that the perceptron algorithm is guaranteed to find a separating hyperplane of the data is linearly separable. But the perceptron, I mean it was. It. It has some drawbacks.

290
00:33:00.250 --> 00:33:03.710
Inderjit Dhillon: Okay. So the drawbacks

291
00:33:05.570 --> 00:33:06.760
Inderjit Dhillon: of perception.

292
00:33:09.740 --> 00:33:15.340
Inderjit Dhillon: Anybody wants to think what the possible drawbacks would be. I've kind of given a hint

293
00:33:15.500 --> 00:33:19.120
Inderjit Dhillon: one at least, for one of the drawbacks. I've already given a hint

294
00:33:19.390 --> 00:33:25.010
Inderjit Dhillon: as to what it might. Why, you know a case where

295
00:33:25.350 --> 00:33:28.800
Inderjit Dhillon: the percept or may not give a a good answer.

296
00:33:29.270 --> 00:33:32.780
Inderjit Dhillon: Anyone wants to wants to

297
00:33:32.940 --> 00:33:36.060
Inderjit Dhillon: say what possible drawbacks it might be of perceptron

298
00:33:43.550 --> 00:33:48.400
like when the data is not linearly separable. Then it goes into infinity loop

299
00:33:50.300 --> 00:33:52.800
Inderjit Dhillon: the chat window.

300
00:33:54.280 --> 00:34:06.340
Inderjit Dhillon: Yeah. So Margaret said, that, you know, if the the data is not linearly separable. then well, the model itself is not very good, right? But you know.

301
00:34:06.670 --> 00:34:08.480
Inderjit Dhillon: I could still

302
00:34:08.850 --> 00:34:11.409
Inderjit Dhillon: output

303
00:34:11.580 --> 00:34:16.889
Inderjit Dhillon: separating hyperplane, even if the data is not linearly separable.

304
00:34:16.949 --> 00:34:22.719
Inderjit Dhillon: Okay, so that actually is one drawback of perceptron as compared to other.

305
00:34:22.760 --> 00:34:27.330
Inderjit Dhillon: algorithm there are still time to return a separating hyperplane.

306
00:34:27.429 --> 00:34:32.440
Inderjit Dhillon: If the classes are not linearly separable, then the algorithm is actually will not converge.

307
00:34:32.630 --> 00:34:45.920
Inderjit Dhillon: It can actually even go into cycles, and the cycles may not be easy to detect. They might be, you know, quite long. So you know, when you are trying to write code they might not be easy to detect. So that's one drawback.

308
00:34:46.090 --> 00:34:54.730
Inderjit Dhillon: And then there are other drawbacks, too. So let me kind of list some of these drawbacks. and of course the idea is that we try and get something which is

309
00:34:54.820 --> 00:34:59.000
Inderjit Dhillon: better than the perceptron, and we will be able to do that. Okay.

310
00:34:59.260 --> 00:35:00.500
Inderjit Dhillon: So if

311
00:35:02.090 --> 00:35:13.180
Inderjit Dhillon: the data is linearly the preval. So even if the data is the only approval.

312
00:35:14.310 --> 00:35:15.130
Inderjit Dhillon: okay.

313
00:35:15.470 --> 00:35:22.510
Inderjit Dhillon: the output which is given by the algorithm which is that the hyperplane output

314
00:35:28.000 --> 00:35:31.320
Inderjit Dhillon: depends on the order of

315
00:35:31.630 --> 00:35:41.280
Inderjit Dhillon: on which the points are visited or presented to the out. So the hyperplane output depends on the order

316
00:35:42.680 --> 00:35:44.610
Inderjit Dhillon: and which

317
00:35:46.130 --> 00:35:47.480
Inderjit Dhillon: points

318
00:35:48.750 --> 00:35:53.420
Inderjit Dhillon: my and by points I mean the data, the training data, X Iy I

319
00:35:53.720 --> 00:35:55.780
Inderjit Dhillon: is presented

320
00:35:58.370 --> 00:36:00.090
Inderjit Dhillon: to the output.

321
00:36:03.490 --> 00:36:06.630
Inderjit Dhillon: and that might not be very good right? Because

322
00:36:06.760 --> 00:36:10.860
Inderjit Dhillon: suppose I just arbitrarily shuffle the points.

323
00:36:10.920 --> 00:36:14.330
Inderjit Dhillon: then I can actually get a very different

324
00:36:14.550 --> 00:36:17.700
Inderjit Dhillon: hyperplane, and there is no kind of guarantee

325
00:36:17.780 --> 00:36:20.750
Inderjit Dhillon: on what kind of hyperplane is output.

326
00:36:21.250 --> 00:36:27.170
Inderjit Dhillon: Second, the number of iterations can be actually quite large, or the number of steps.

327
00:36:37.840 --> 00:36:47.850
Inderjit Dhillon: and finally, what I mentioned above, which is, if the classes. if the 2 classes are

328
00:36:48.590 --> 00:36:50.890
Inderjit Dhillon: not linearly separable

329
00:36:56.400 --> 00:36:59.110
Inderjit Dhillon: them. the algorithm

330
00:37:03.270 --> 00:37:05.170
Inderjit Dhillon: might not converge.

331
00:37:11.240 --> 00:37:15.230
Inderjit Dhillon: Okay. And what can actually happen is

332
00:37:15.380 --> 00:37:18.140
Inderjit Dhillon: that cycles can develop

333
00:37:24.340 --> 00:37:26.390
Inderjit Dhillon: that are not easy to detect.

334
00:37:37.320 --> 00:37:44.970
Inderjit Dhillon: Okay, so we've now seen the perception algorithm. Conceptually, very, very simple, algorithm. Right? I mean, you basically can

335
00:37:45.080 --> 00:37:48.580
Inderjit Dhillon: write the code in, you know.

336
00:37:49.710 --> 00:37:53.830
Inderjit Dhillon: if 1015min and actually start experimenting with the

337
00:37:54.000 --> 00:37:55.700
Inderjit Dhillon: with the I was going to.

338
00:37:57.650 --> 00:38:00.440
Inderjit Dhillon: So this was developed in 1,962,

339
00:38:01.800 --> 00:38:05.060
Inderjit Dhillon: and then in the 19 nineties.

340
00:38:06.800 --> 00:38:11.500
Inderjit Dhillon: What people came up was support vector machines

341
00:38:19.380 --> 00:38:30.000
Inderjit Dhillon: and for a long time support vector machines were along with logistic regression, were the really best performing classification methods until

342
00:38:30.150 --> 00:38:39.430
Inderjit Dhillon: the you know. for the last 10 or 15 years, where deep learning has really surpassed

343
00:38:39.520 --> 00:38:43.990
Inderjit Dhillon: even support vector machines. Okay, but a very important

344
00:38:44.070 --> 00:38:51.820
Inderjit Dhillon: milestone in machine learning. So if I, if you remember, let me draw that picture again.

345
00:38:52.290 --> 00:38:58.920
Inderjit Dhillon: You have these pluses. and then you have these circles.

346
00:39:00.800 --> 00:39:12.570
Inderjit Dhillon: I have to see one. Let me see. What did I call them? The right one? I think I called C. One Yes, and the left one I call C, 2, c. One.

347
00:39:12.600 --> 00:39:17.290
Inderjit Dhillon: Why, why, I is plus one. This is C 2

348
00:39:17.710 --> 00:39:20.510
Inderjit Dhillon: Y. I is plus

349
00:39:20.960 --> 00:39:22.430
Inderjit Dhillon: sorry minus one

350
00:39:24.120 --> 00:39:25.360
chitrank: professor.

351
00:39:25.930 --> 00:39:26.780
Inderjit Dhillon: Okay.

352
00:39:26.790 --> 00:39:29.430
Inderjit Dhillon: So if I think about it right.

353
00:39:29.750 --> 00:39:31.640
Inderjit Dhillon: my

354
00:39:32.820 --> 00:39:41.360
Inderjit Dhillon: for some. The only guarantee it gives me is that it will give a separating hyper plan.

355
00:39:41.970 --> 00:39:47.840
Inderjit Dhillon: and that's a pretty hyperplane may depend upon the order in which points are presented to the out.

356
00:39:48.290 --> 00:39:50.680
Inderjit Dhillon: which means that, for example.

357
00:39:50.950 --> 00:39:55.670
Inderjit Dhillon: you know the perceptron's surface may actually be something like this.

358
00:39:57.960 --> 00:39:59.610
Inderjit Dhillon: Is that a good surface

359
00:40:01.280 --> 00:40:08.600
Inderjit Dhillon: to separate these points? By? I mean, ideally, we would want something which it's like. It is in the middle

360
00:40:12.390 --> 00:40:16.330
Inderjit Dhillon: kind of separates maximally the 2 classes.

361
00:40:16.370 --> 00:40:18.190
Inderjit Dhillon: right? I mean. In some sense

362
00:40:18.290 --> 00:40:25.460
Inderjit Dhillon: this is the maximal separation between the 2 classes, whereas here. You know the with this

363
00:40:25.720 --> 00:40:30.450
Inderjit Dhillon: green line that might be output by the

364
00:40:30.540 --> 00:40:35.820
Inderjit Dhillon: it actually might not be very good. Right? So if you consider a point, let's say I have a new point

365
00:40:36.060 --> 00:40:42.020
Inderjit Dhillon: which I do not buy. you know. Let me just call it X.

366
00:40:43.480 --> 00:40:59.560
Inderjit Dhillon: Well. if I use the green separating hyperplane. It will say that this point is plus one, or belongs to C one. but it looks like it should actually belong to C. 2. Right? I think the point will be more obvious if I did something like this.

367
00:41:05.000 --> 00:41:05.590
Inderjit Dhillon: Oh.

368
00:41:08.200 --> 00:41:10.570
Inderjit Dhillon: if X is something like this.

369
00:41:15.700 --> 00:41:16.560
Inderjit Dhillon: Okay.

370
00:41:18.990 --> 00:41:28.580
Inderjit Dhillon: So you know, I intuitively that X should belong to C one. But this is but percept or might not do it. Okay.

371
00:41:28.810 --> 00:41:32.280
Inderjit Dhillon: So let's see what we can do.

372
00:41:32.740 --> 00:41:35.170
Inderjit Dhillon: So remember that the signed

373
00:41:37.770 --> 00:41:39.060
Inderjit Dhillon: distance

374
00:41:41.370 --> 00:41:43.060
Inderjit Dhillon: of X

375
00:41:44.260 --> 00:41:46.090
Inderjit Dhillon: to the hyperplane.

376
00:41:48.870 --> 00:42:03.500
Inderjit Dhillon: Okay, that's given by W. Transpose X minus W. Transpose X 0 divided by norm of W. And since W. Transpose x 0 plus w not equal to 0.

377
00:42:03.740 --> 00:42:09.380
Inderjit Dhillon: This is W. Transpose x plus w naught divided by

378
00:42:09.460 --> 00:42:15.560
Inderjit Dhillon: just repeating what we had done before. Okay. so in particular

379
00:42:16.420 --> 00:42:22.030
Inderjit Dhillon: Y, I. Times W. Transpose x plus w not

380
00:42:22.500 --> 00:42:28.160
Inderjit Dhillon: divided by Norm of W. That is the distance

381
00:42:29.870 --> 00:42:30.920
Inderjit Dhillon: of

382
00:42:31.970 --> 00:42:33.520
Inderjit Dhillon: training. Point

383
00:42:37.510 --> 00:42:40.160
Inderjit Dhillon: xi. Do the hyper.

384
00:42:46.660 --> 00:42:59.140
Inderjit Dhillon: Well. if I look at the above example above picture, when the exit is close to class, c. One, but it's misclassified if the perceptron is, you know, leads to kind of a bad

385
00:43:00.490 --> 00:43:10.240
Inderjit Dhillon: separating hyperplane. right? We may say, okay, Can we put the requirement that each of these distances is greater than a certain threshold?

386
00:43:10.830 --> 00:43:13.230
Inderjit Dhillon: Okay, so that's what we do. Suppose

387
00:43:17.220 --> 00:43:19.520
Inderjit Dhillon: we put the requirement

388
00:43:24.060 --> 00:43:25.840
Inderjit Dhillon: that each

389
00:43:27.480 --> 00:43:30.900
Inderjit Dhillon: of these distances

390
00:43:33.700 --> 00:43:36.740
Inderjit Dhillon: is greater than c.

391
00:43:39.220 --> 00:43:46.080
Inderjit Dhillon: Some value greater than let's say, some value, c. Which means that what i'm saying is that why I

392
00:43:46.640 --> 00:43:52.070
Inderjit Dhillon: times w transpose x plus w not divided by norm of W.

393
00:43:52.460 --> 00:44:00.440
Inderjit Dhillon: Is greater than equal to c. Which implies. and why I don't stop. You transpose X plus w

394
00:44:01.110 --> 00:44:03.800
Inderjit Dhillon: Sorry time to do it too quickly.

395
00:44:06.270 --> 00:44:08.100
Inderjit Dhillon: That's w not

396
00:44:12.480 --> 00:44:15.710
Inderjit Dhillon: is greater than or equal to C. Times norm of top.

397
00:44:17.220 --> 00:44:18.020
Inderjit Dhillon: Okay.

398
00:44:19.020 --> 00:44:20.610
Inderjit Dhillon: so in

399
00:44:21.360 --> 00:44:27.560
Inderjit Dhillon: and i'll clarify what I mean. But later let me call them as linear support vector machines.

400
00:44:28.850 --> 00:44:31.190
Inderjit Dhillon: So in linear support vector

401
00:44:33.090 --> 00:44:34.520
Inderjit Dhillon: machines.

402
00:44:37.270 --> 00:44:39.300
Inderjit Dhillon: The goal is

403
00:44:40.600 --> 00:44:41.590
Inderjit Dhillon: 2

404
00:44:42.660 --> 00:44:47.090
Inderjit Dhillon: maximize. I think. Okay.

405
00:44:47.420 --> 00:44:50.970
Inderjit Dhillon: And so what I'm gonna do is I can write it as

406
00:44:51.650 --> 00:44:54.420
Inderjit Dhillon: I want to maximize

407
00:44:56.000 --> 00:45:04.050
Inderjit Dhillon: over all choices of the hyperplane, which is given by W. Not this quantity, c. That's that.

408
00:45:07.080 --> 00:45:19.480
Inderjit Dhillon: Why I. Times W. Transpose x I plus w naught is greater than or equal to C. Times. Now. so same criterion as this

409
00:45:20.290 --> 00:45:34.460
Inderjit Dhillon: same criterion. But i'm now saying that I want to try and maximize. See? Because I want to basically try and make sure that each point is kind of far away from the separating hyperplane.

410
00:45:35.250 --> 00:45:44.110
Inderjit Dhillon: Now, you know, there is one thing that you should note that, you know W. Transpose x plus w, Not when I write W. Transpose x plus w not equal to 0.

411
00:45:44.230 --> 00:45:54.970
Inderjit Dhillon: I can actually just scale W by any number, and that equation will still be valid right? So I can change W. To 100 times. W. W. Not to 100 times. W. Not

412
00:45:55.120 --> 00:45:58.460
Inderjit Dhillon: but my question or the hyperplane will domain the same.

413
00:45:58.570 --> 00:46:10.160
Inderjit Dhillon: So there is a little bit of ambiguity. So what we can do is you can fix C. Times norm of W. Equal to one. Okay. since

414
00:46:10.800 --> 00:46:12.820
Inderjit Dhillon: me and

415
00:46:13.870 --> 00:46:15.520
Inderjit Dhillon: arbitrarily

416
00:46:17.750 --> 00:46:18.950
Inderjit Dhillon: scale

417
00:46:20.040 --> 00:46:25.630
Inderjit Dhillon: W. And Don't, you know. By doing this, i'm actually anchoring

418
00:46:25.750 --> 00:46:34.300
Inderjit Dhillon: w and W not. And if I do that I get that C is equal to one divided by the norm of W.

419
00:46:34.870 --> 00:46:40.570
Inderjit Dhillon: Okay. So then I can say that this criterion that I've written over here

420
00:46:42.380 --> 00:46:46.600
Inderjit Dhillon: becomes maximize

421
00:46:49.680 --> 00:46:52.120
Inderjit Dhillon: W. W. Not

422
00:46:53.510 --> 00:46:58.190
Inderjit Dhillon: I have one divided by normal W.

423
00:46:58.990 --> 00:47:00.890
Inderjit Dhillon: Such that

424
00:47:02.770 --> 00:47:12.820
Inderjit Dhillon: Y. I. W. Transpose x I plus W. Naught is greater than well, c. Times long of W is one.

425
00:47:13.150 --> 00:47:16.370
Inderjit Dhillon: and so I do it, for I equal to one to

426
00:47:16.670 --> 00:47:19.560
Inderjit Dhillon: cool, and I think i'm calling it.

427
00:47:20.350 --> 00:47:25.820
Inderjit Dhillon: Capital M. I think I may have called it little, and before. So

428
00:47:26.700 --> 00:47:28.090
Inderjit Dhillon: let me say.

429
00:47:35.700 --> 00:47:40.710
Inderjit Dhillon: Okay. And now this is kind of a little awkward. It's minute maximizing.

430
00:47:41.310 --> 00:47:44.180
Inderjit Dhillon: you know, one divided by W.

431
00:47:48.890 --> 00:47:52.130
Inderjit Dhillon: Okay. So I can just write it as

432
00:47:52.720 --> 00:47:54.250
Inderjit Dhillon: minimize

433
00:47:56.730 --> 00:47:59.160
Inderjit Dhillon: them. Do. W. Not

434
00:47:59.860 --> 00:48:05.470
Inderjit Dhillon: if i'm maximizing one divided by W. Normal. W. I can minimize

435
00:48:05.870 --> 00:48:09.280
Inderjit Dhillon: this Okay, and I can.

436
00:48:09.290 --> 00:48:18.610
Inderjit Dhillon: Instead of doing this, I can minimize the 2 Norms Square right? It's identical. And by the way, I mean, this was the 2 norms, so I don't need to

437
00:48:18.750 --> 00:48:20.250
Inderjit Dhillon: gonna change it

438
00:48:20.940 --> 00:48:30.690
Inderjit Dhillon: right. But if i'm maximizing normal W. It's equivalent to my sorry if i'm minimizing norm of W. It is equivalent to minimizing the

439
00:48:30.930 --> 00:48:33.140
Inderjit Dhillon: square norm of W.

440
00:48:33.760 --> 00:48:34.550
Inderjit Dhillon: Okay.

441
00:48:34.800 --> 00:48:38.960
Inderjit Dhillon: And so this is my criterion, which is.

442
00:48:39.690 --> 00:48:41.030
Inderjit Dhillon: That's that

443
00:48:45.470 --> 00:48:46.670
Inderjit Dhillon: such that

444
00:48:47.250 --> 00:48:53.700
Inderjit Dhillon: Y. I times w transpose x I plus w naught

445
00:48:54.100 --> 00:48:58.290
Inderjit Dhillon: is greater than or equal to one. for I equal to

446
00:48:58.940 --> 00:49:04.410
Inderjit Dhillon: 1 2 3, and so this is

447
00:49:05.130 --> 00:49:07.200
Inderjit Dhillon: the support vector machine follow.

448
00:49:08.450 --> 00:49:15.120
Inderjit Dhillon: Okay, so many times support Vector machines are called Svm.

449
00:49:16.890 --> 00:49:20.880
Inderjit Dhillon: As we M for support vector machines.

450
00:49:21.420 --> 00:49:24.710
Inderjit Dhillon: and this is known as the

451
00:49:26.780 --> 00:49:28.950
Inderjit Dhillon: Svm: Problem.

452
00:49:32.450 --> 00:49:39.920
Inderjit Dhillon: Okay, now look at it and say, okay, now, how do I try to find the optimal W. And W. Not?

453
00:49:41.840 --> 00:49:50.190
Inderjit Dhillon: This is a little bit different than some of the problems that we've seen before. because we actually have a constraint.

454
00:49:53.210 --> 00:49:56.310
Inderjit Dhillon: Oh, I'm: Sorry I

455
00:50:01.700 --> 00:50:10.540
Inderjit Dhillon: Okay. I'm sorry I saw a message just now that wanted to ask a question. I apologize. I think my audio might have been off

456
00:50:10.910 --> 00:50:13.870
chitrank: volume. Let me know you think, could you?

457
00:50:14.080 --> 00:50:16.930
I can ask that later towards the end of the class.

458
00:50:17.470 --> 00:50:18.260
Inderjit Dhillon: Okay.

459
00:50:20.710 --> 00:50:26.860
Inderjit Dhillon: So we have this Svm problem. And you know, this is actually

460
00:50:27.380 --> 00:50:33.290
Inderjit Dhillon: constrained optimization problem, right? So you have an objective which is this

461
00:50:33.630 --> 00:50:34.920
Inderjit Dhillon: over here.

462
00:50:36.040 --> 00:50:37.290
Inderjit Dhillon: and then

463
00:50:38.530 --> 00:50:40.620
Inderjit Dhillon: you have this constraint.

464
00:50:41.630 --> 00:50:47.310
Inderjit Dhillon: And what this is saying is that you have an objective, and you have a constraint.

465
00:50:47.720 --> 00:50:51.240
Inderjit Dhillon: So this is a constrained optimization problem.

466
00:50:55.570 --> 00:51:05.260
Inderjit Dhillon: If you think about all the other problems we've had, they haven't been constrained optimization problems. They have been actually unconstrained optimization problems.

467
00:51:06.710 --> 00:51:11.170
Inderjit Dhillon: although when we, for for example, did regularization, you can actually convert

468
00:51:11.360 --> 00:51:16.250
Inderjit Dhillon: one form of the problem to other right with the Lagrange multiplier.

469
00:51:16.760 --> 00:51:20.450
Inderjit Dhillon: But what we will see, and there will be some value to doing this

470
00:51:20.520 --> 00:51:24.820
Inderjit Dhillon: is that when you have a constraint optimization problem.

471
00:51:24.970 --> 00:51:28.780
Inderjit Dhillon: you can call the original problem the primal problem.

472
00:51:31.740 --> 00:51:40.240
Inderjit Dhillon: Okay? And then you can derive by techniques in the you know, in optimization, what's called the dual of the problem.

473
00:51:40.600 --> 00:51:41.360
Inderjit Dhillon: Okay.

474
00:51:41.580 --> 00:51:46.650
Inderjit Dhillon: So that's what we will do is, you know.

475
00:51:46.950 --> 00:51:49.910
Inderjit Dhillon: for you know this is the primer problem.

476
00:51:50.910 --> 00:51:56.130
Inderjit Dhillon: and I still Haven't told you about how to solve this right. So this is not as simple

477
00:51:56.170 --> 00:51:59.080
Inderjit Dhillon: as solving the least squares problem.

478
00:52:01.560 --> 00:52:07.810
Inderjit Dhillon: but you have to develop some techniques to be able to solve it, and there are many, many techniques in the literature

479
00:52:07.830 --> 00:52:14.050
Inderjit Dhillon: to try and solve this problem. As I said, support vector machines, you know, in the

480
00:52:14.130 --> 00:52:28.870
Inderjit Dhillon: till, about the mid 2,000, right? So in the until about 2,010 or slightly earlier. They were the best performing, or at least one of the 2 best performing classifiers

481
00:52:30.240 --> 00:52:41.600
Inderjit Dhillon: for the classification problem. Okay. So so just to kind of give you a picture, right? So remember I have these pluses.

482
00:52:45.100 --> 00:52:46.820
Inderjit Dhillon: And now I have these.

483
00:52:46.890 --> 00:52:50.560
Inderjit Dhillon: you know. So the second one is the circles.

484
00:52:52.670 --> 00:52:55.720
Inderjit Dhillon: Okay. what?

485
00:52:56.000 --> 00:53:05.230
Inderjit Dhillon: What support vector machines give you is they basically give you a classifier which has what's called the largest margin.

486
00:53:05.940 --> 00:53:06.620
Inderjit Dhillon: Okay.

487
00:53:06.810 --> 00:53:10.370
Inderjit Dhillon: you can show that you know, if I draw

488
00:53:10.790 --> 00:53:12.220
Inderjit Dhillon: things which are

489
00:53:13.650 --> 00:53:16.570
Inderjit Dhillon: over here, things which are

490
00:53:17.990 --> 00:53:23.120
Inderjit Dhillon: this hyperplane which is over here, that this distance

491
00:53:24.480 --> 00:53:28.700
Inderjit Dhillon: is one divided by Norm of top 2.

492
00:53:29.820 --> 00:53:33.800
Inderjit Dhillon: Okay. And this is

493
00:53:34.410 --> 00:53:50.120
Inderjit Dhillon: 2 divided by Norm of and by minimizing the norm of W. Subject to these constraints. And remember the constraints say that the points are correctly classified. Right? So this is so just as a reminder. This is

494
00:53:50.400 --> 00:53:55.110
Inderjit Dhillon: W. Right. and this is the hyperplane

495
00:53:59.790 --> 00:54:04.830
Inderjit Dhillon: hyperplane which is W. Transpose x plus w. Not

496
00:54:07.380 --> 00:54:12.220
Inderjit Dhillon: okay. So it gives the kind of

497
00:54:12.470 --> 00:54:13.990
Inderjit Dhillon: hyperplane

498
00:54:17.410 --> 00:54:19.210
Inderjit Dhillon: which is with

499
00:54:20.240 --> 00:54:21.810
Inderjit Dhillon: maximum margin.

500
00:54:26.260 --> 00:54:33.370
Inderjit Dhillon: so as in the picture above it' never given. Give this a green line

501
00:54:33.910 --> 00:54:40.260
Inderjit Dhillon: as a hyperplane. It'll give the hyperplane that maximally separates the 2 classes.

502
00:54:40.550 --> 00:54:44.600
Inderjit Dhillon: So it gets rid of these problems that perceptron have.

503
00:54:44.760 --> 00:54:49.210
Inderjit Dhillon: So the perceptron has this problem right that the hybrid plane output

504
00:54:49.440 --> 00:54:58.880
Inderjit Dhillon: depends on the order in which data is presented. Well, the support vector machine, even though it's a harder optimization problem.

505
00:54:59.040 --> 00:55:05.890
Inderjit Dhillon: it is still a quadratic optimization problem. So quadratic optimization problems are automatic constraint

506
00:55:06.600 --> 00:55:08.500
Inderjit Dhillon: with

507
00:55:09.180 --> 00:55:11.250
Inderjit Dhillon: linear inequalities.

508
00:55:11.520 --> 00:55:19.050
Inderjit Dhillon: Right and there is a globally optimal solution that exists for those problems. So no matter, you know, you can actually get

509
00:55:20.060 --> 00:55:30.490
Inderjit Dhillon: algorithm with process either one or 2 points at a time. but the algorithm will give you the same separating hyperplane

510
00:55:30.800 --> 00:55:40.500
Inderjit Dhillon: every time. And then, of course, you can try and optimize it, so that the number of iterations is not too large. and then we will see that

511
00:55:40.710 --> 00:55:53.130
Inderjit Dhillon: when the classes are not linearly separable, so, for example, if it happens that you know there is a point plus over here. Let's suppose it's an outline, or let's suppose there was a mistake in labeling

512
00:55:53.210 --> 00:55:56.050
Inderjit Dhillon: the points. Then you can have outliers.

513
00:55:56.230 --> 00:56:01.360
Inderjit Dhillon: But there's the version of the problem, not the problem that I have given so far.

514
00:56:01.390 --> 00:56:07.580
Inderjit Dhillon: because over here the algorithm is insisting that each of the points

515
00:56:07.830 --> 00:56:12.200
Inderjit Dhillon: is correctly classified, and that is this constraint.

516
00:56:13.590 --> 00:56:25.250
Inderjit Dhillon: But what you can do, and we'll show that later is you can add something called slack variables. so that you can say that you know you actually have some slack and satisfying this.

517
00:56:25.850 --> 00:56:29.440
Inderjit Dhillon: each of the linear inequality constraints. So

518
00:56:30.220 --> 00:56:42.690
Inderjit Dhillon: it actually is able to kind of get rid of that third constraint also. which is which talks about the the the the third constraint, which says

519
00:56:42.790 --> 00:56:47.610
Inderjit Dhillon: that if the classes are not linearly separable, it will not

520
00:56:48.100 --> 00:57:05.330
Inderjit Dhillon: give give the a good hyperplane. It'll always actually, in some sense, be able to give a hyperplane, even if the classes are not linearly the triple. Of course, what that means is it'll do not get 0 training together right. But as we saw

521
00:57:05.950 --> 00:57:11.300
Inderjit Dhillon: earlier that you know, the goal is not to minimize the error

522
00:57:11.400 --> 00:57:12.800
Inderjit Dhillon: on the training set.

523
00:57:12.930 --> 00:57:16.330
Inderjit Dhillon: The goal is to generalize to new points.

524
00:57:18.530 --> 00:57:31.910
Reed Zimmermann: Okay, so would you mind explaining again why minimizing the norm of W squared like results in you having this line that's maximally separated, because it seems like the intuition is you're minimizing the

525
00:57:32.060 --> 00:57:36.000
Reed Zimmermann: slope or something Which is it's not making that much sense to me.

526
00:57:37.550 --> 00:57:41.370
Inderjit Dhillon: Okay, so let me see. So over here.

527
00:57:46.170 --> 00:57:49.570
Inderjit Dhillon: So let's go through this right again. So

528
00:57:50.410 --> 00:58:00.010
Inderjit Dhillon: what we said is that suppose we put the requirement that each of these distances is greater than C. So so you what that read or

529
00:58:00.300 --> 00:58:01.040
Reed Zimmermann: yeah.

530
00:58:01.130 --> 00:58:06.880
Inderjit Dhillon: Yeah. So so, Reid, you did understand till over here, right that

531
00:58:08.170 --> 00:58:10.400
Reed Zimmermann: what this says. Yeah.

532
00:58:11.080 --> 00:58:16.250
Inderjit Dhillon: okay. And from here we said, okay, we are going to try and maximize C,

533
00:58:17.300 --> 00:58:21.080
Inderjit Dhillon: which means that we are going to try and make sure that

534
00:58:21.540 --> 00:58:24.600
Inderjit Dhillon: the points have, you know.

535
00:58:25.030 --> 00:58:29.360
Inderjit Dhillon: a distance which is greater than C from the hyperplane.

536
00:58:30.620 --> 00:58:32.720
Inderjit Dhillon: Okay. And

537
00:58:34.960 --> 00:58:45.050
Inderjit Dhillon: this point says that since we can arbitrarily scales W. And W not, there's kind of one extra range of freedom, one extra degree of freedom.

538
00:58:45.100 --> 00:58:51.180
Inderjit Dhillon: We remove that degree of freedom by fixing the C. Times norm of w equals one.

539
00:58:52.430 --> 00:59:03.690
Inderjit Dhillon: which means that maximize, c. Becomes, since C is equal to one divided by norm of W. It becomes maximizing one divided by the norm of 2,

540
00:59:04.810 --> 00:59:10.400
Inderjit Dhillon: maximizing one divided by Norm of W. Since Norm of W is a positive quantity.

541
00:59:10.440 --> 00:59:14.140
Inderjit Dhillon: is the same as minimizing the norm of W.

542
00:59:15.680 --> 00:59:16.410
Reed Zimmermann: I see.

543
00:59:16.580 --> 00:59:20.990
Inderjit Dhillon: And minimizing norm of W. Is the same as minimizing the norm of the square.

544
00:59:22.360 --> 00:59:23.050
Reed Zimmermann: Okay.

545
00:59:23.550 --> 00:59:24.260
Inderjit Dhillon: Okay.

546
00:59:24.360 --> 00:59:25.140
Reed Zimmermann: thanks.

547
00:59:25.240 --> 00:59:31.910
Inderjit Dhillon: Okay. Good. I hope that clear things up. I think now is a good time for you to ask the question.

548
00:59:32.560 --> 00:59:44.010
chitrank: Oh, sorry! So my question was actually on the perception. And I meant to us that if we were to like, calculate the the gradient of a batch of

549
00:59:45.260 --> 00:59:53.040
chitrank: misclassifications that would help to resolve some of the problems like lesser tuition. So

550
00:59:53.100 --> 00:59:56.930
chitrank: and would it even lead to conversions or anything at all?

551
00:59:57.390 --> 01:00:07.030
Inderjit Dhillon: Yeah. So I think what you're talking about is that you know in some sense, if I look at this algorithm right, I think I mentioned stochastic, gradient descent.

552
01:00:07.760 --> 01:00:22.630
Inderjit Dhillon: This is looking at stochastic, gradient descent, and it is basically looking at just one individual point. Right? So the question is, can we actually generalize the perceptron algorithm because I mean, the perceptron algorithm was presented as

553
01:00:22.770 --> 01:00:27.190
Inderjit Dhillon: given over here. Okay, as written over here.

554
01:00:28.340 --> 01:00:30.520
Inderjit Dhillon: That is how the algorithm was presented.

555
01:00:30.810 --> 01:00:39.110
Inderjit Dhillon: Okay, and you know it was a long time, you know. It was long time ago, and even then i'm not even sure if they.

556
01:00:39.450 --> 01:00:49.630
Inderjit Dhillon: when the original method was presented. I haven't actually done the original paper I must have meant. I don't know if they presented it in the in the framework of doing stochastic gradient descent.

557
01:00:50.900 --> 01:00:51.600
Inderjit Dhillon: Okay.

558
01:00:51.640 --> 01:00:55.260
Inderjit Dhillon: But I think we are the heart of your question is.

559
01:00:55.290 --> 01:01:01.550
Inderjit Dhillon: okay. What if I now change my stochastic, gradient descent which goes through each individual point

560
01:01:01.910 --> 01:01:09.510
Inderjit Dhillon: right to something which is called. There are many bad, stochastic, gradient descent. Right that you look over. Look at a

561
01:01:09.520 --> 01:01:11.170
Inderjit Dhillon: subset of points.

562
01:01:12.460 --> 01:01:15.230
Inderjit Dhillon: and then you basically do

563
01:01:17.290 --> 01:01:24.820
Inderjit Dhillon: mini bad, stochastic, gradient descent. So definitely you can change the algorithm to do, meaning that stochastic, gradient descent.

564
01:01:27.750 --> 01:01:33.190
Inderjit Dhillon: And then your question is, or do you kind of change the

565
01:01:33.360 --> 01:01:36.130
Inderjit Dhillon: output and the behavior of the algorithm

566
01:01:37.300 --> 01:01:42.160
Inderjit Dhillon: Okay, maybe we'll actually come back to that later on when we talk about the Std.

567
01:01:42.960 --> 01:01:44.200
chitrank: Okay.

568
01:01:47.790 --> 01:01:50.160
Inderjit Dhillon: Okay, any other questions.

569
01:02:00.960 --> 01:02:15.240
Inderjit Dhillon: Okay. So if there are no other questions kind of let me move on to the next part, right? And the next part i'll actually talk about and let me actually motivate. Why, I will be going through some.

570
01:02:15.590 --> 01:02:16.580
Inderjit Dhillon: you know.

571
01:02:17.700 --> 01:02:32.650
Inderjit Dhillon: optimization in the in this, the remainder of this lecture and the next lecture. Okay. So this is the like, I said. What you see on the screen is, you know, in this rectangular box

572
01:02:32.910 --> 01:02:36.850
Inderjit Dhillon: is the primal support vector machine problem. Okay?

573
01:02:37.150 --> 01:02:49.220
Inderjit Dhillon: And by looking at you know the optimization problem and more depth. We'll actually be able to get what's called the dual of this

574
01:02:49.290 --> 01:02:52.560
Inderjit Dhillon: problem. So there'll be a primal Svm problem.

575
01:02:52.830 --> 01:03:00.250
Inderjit Dhillon: and then we will be able to derive the dual Svm problem. And what that will allow us to do

576
01:03:01.020 --> 01:03:06.010
Inderjit Dhillon: is to understand this in more greater depth.

577
01:03:06.240 --> 01:03:14.770
Inderjit Dhillon: We'll actually end up getting insight into how the support factor machine works

578
01:03:15.270 --> 01:03:22.000
Inderjit Dhillon: right, and the reason it is called the support vector machine is kind of it's kind of supporting. You know these

579
01:03:22.310 --> 01:03:23.110
Inderjit Dhillon: these

580
01:03:23.930 --> 01:03:37.810
Inderjit Dhillon: 2 kind of hyperplanes which are on either side of the separating hyperplane kind of support these points, and we'll actually make that packed very concrete by looking at the door problem

581
01:03:38.260 --> 01:03:42.910
Inderjit Dhillon: and the other advantage. So it will give us more insight into the problem. But

582
01:03:43.030 --> 01:03:44.680
Inderjit Dhillon: even more than that.

583
01:03:45.920 --> 01:03:53.010
Inderjit Dhillon: it will allow us to have another way of looking at problems in machine learning which are

584
01:03:53.070 --> 01:03:59.870
Inderjit Dhillon: called colonel methods. And by looking at the dual problem we'll actually see

585
01:03:59.980 --> 01:04:08.110
Inderjit Dhillon: that this is one way to apply, or no, for example, kernel support vector machines.

586
01:04:08.150 --> 01:04:25.200
Inderjit Dhillon: So what i'm saying right now might actually not make that much sense, so that we kind of dive into it. and then we'll come back to it. Okay. So the support vector machine is a special case of a general

587
01:04:27.780 --> 01:04:29.600
Inderjit Dhillon: constrained optimization problem.

588
01:04:29.980 --> 01:04:35.950
Inderjit Dhillon: Nice. Sometimes I try and write too quickly. So

589
01:04:37.400 --> 01:04:38.820
Inderjit Dhillon: general

590
01:04:40.510 --> 01:04:43.650
Inderjit Dhillon: constrained optimization problem.

591
01:04:47.570 --> 01:04:55.340
Inderjit Dhillon: So the material i'm teaching from now until you know, maybe the end of next lecture is

592
01:04:55.570 --> 01:04:59.160
Inderjit Dhillon: from the book on Convex optimization by

593
01:04:59.590 --> 01:05:05.370
Inderjit Dhillon: Boyd and Vandenberg. Okay, it's a optimization textbook.

594
01:05:06.150 --> 01:05:13.550
Inderjit Dhillon: So here is the general problem. I am trying to minimize my objective, which I write as F. Naught. X.

595
01:05:14.830 --> 01:05:18.160
Inderjit Dhillon: X. Is the variable. and

596
01:05:18.400 --> 01:05:25.420
Inderjit Dhillon: my constraints are one. Write it as fi. X. Is less than equal to 0.

597
01:05:25.450 --> 01:05:38.850
Inderjit Dhillon: I is equal to one to M. And I have inequality constraints with less than equal to, and I can also have equality constraints.

598
01:05:40.270 --> 01:05:43.580
Inderjit Dhillon: So H. I. X is equal to 0.

599
01:05:43.910 --> 01:05:46.740
Inderjit Dhillon: I is equal to 1, 2,

600
01:05:46.810 --> 01:05:50.930
Inderjit Dhillon: Me. So M. Inequality constraints

601
01:05:51.110 --> 01:05:54.430
Inderjit Dhillon: and p equality constraints. Okay.

602
01:05:54.490 --> 01:05:56.370
Inderjit Dhillon: So just to clarify here.

603
01:05:57.640 --> 01:06:00.620
Inderjit Dhillon: X is our

604
01:06:01.640 --> 01:06:05.250
Inderjit Dhillon: Okay. X is an N. Dimensional. Vector

605
01:06:05.340 --> 01:06:10.370
Inderjit Dhillon: Okay. So this I will call as problem one.

606
01:06:10.800 --> 01:06:14.170
Inderjit Dhillon: And this is also the My: no problem.

607
01:06:20.210 --> 01:06:21.050
Inderjit Dhillon: Okay.

608
01:06:21.350 --> 01:06:24.640
Inderjit Dhillon: So just to kind of clarify, okay.

609
01:06:26.850 --> 01:06:35.680
Inderjit Dhillon: if X belongs to our end, each of my fi's, including my objective. Okay. And then in the in the quality constraints

610
01:06:36.290 --> 01:06:39.430
Inderjit Dhillon: they are. I'm: sorry while I put I here.

611
01:06:44.500 --> 01:06:45.380
Inderjit Dhillon: Okay.

612
01:06:45.490 --> 01:06:50.120
Inderjit Dhillon: They are a function from Rm.

613
01:06:51.270 --> 01:06:55.770
Inderjit Dhillon: Who are the real valued function

614
01:06:56.300 --> 01:07:00.000
Inderjit Dhillon: that maps are M. To R. And H. I.

615
01:07:00.660 --> 01:07:01.610
Inderjit Dhillon: All

616
01:07:02.560 --> 01:07:05.940
Inderjit Dhillon: functions that map R. And 2 are also

617
01:07:06.700 --> 01:07:07.500
Inderjit Dhillon: okay.

618
01:07:09.550 --> 01:07:18.510
Inderjit Dhillon: Well, that is the most general form of a constrained optimization problem. Not that I Haven't said anything really about, if not

619
01:07:18.750 --> 01:07:21.750
Inderjit Dhillon: Fi or H. I.

620
01:07:22.850 --> 01:07:23.470
Inderjit Dhillon: Okay.

621
01:07:24.710 --> 01:07:31.430
Inderjit Dhillon: So in optimization theory. what you can do is when you are given such a problem.

622
01:07:32.010 --> 01:07:37.110
Inderjit Dhillon: Then we're gonna try and opt analyze this. Okay, so you can form

623
01:07:37.820 --> 01:07:41.230
Inderjit Dhillon: what is called the lagrangian.

624
01:07:44.050 --> 01:07:44.720
Okay?

625
01:07:45.000 --> 01:07:58.390
Inderjit Dhillon: And the lagrangian is now defined as the function which is our end calls or M. Calls Rp.

626
01:07:59.040 --> 01:07:59.970
Inderjit Dhillon: You are

627
01:08:01.540 --> 01:08:11.870
Inderjit Dhillon: okay. But but what is it? So it takes Now the original problem only had X as an input, a as a as the variable.

628
01:08:12.060 --> 01:08:15.140
Inderjit Dhillon: But the lagrangian has X.

629
01:08:15.860 --> 01:08:28.750
Inderjit Dhillon: Lambda and me as variable. And what are these Lambda and muse. Well, these are the Lagrange Lagrange

630
01:08:29.460 --> 01:08:32.330
Inderjit Dhillon: parameters or variables.

631
01:08:33.950 --> 01:08:34.990
Inderjit Dhillon: and

632
01:08:35.439 --> 01:08:38.640
Inderjit Dhillon: the Lagrangian is f naught x plus

633
01:08:40.760 --> 01:08:41.890
Inderjit Dhillon: plus

634
01:08:42.810 --> 01:08:45.950
Inderjit Dhillon: lambda. I. F. I. X.

635
01:08:46.040 --> 01:08:48.439
Inderjit Dhillon: I is equal to one to m

636
01:08:49.240 --> 01:08:53.490
Inderjit Dhillon: plus summation of I equals one to p

637
01:08:54.250 --> 01:08:58.250
Inderjit Dhillon: me. I H. I X.

638
01:08:58.910 --> 01:09:01.920
Inderjit Dhillon: Okay. So just to kind of

639
01:09:02.490 --> 01:09:14.120
Inderjit Dhillon: make it clear. What is happening is that I have F naught. X. And I'm: basically taking Lambda I. Times. Fi. X:

640
01:09:14.240 --> 01:09:16.370
Inderjit Dhillon: Okay. And that is the term over here.

641
01:09:17.770 --> 01:09:19.899
Inderjit Dhillon: Lambda I. Fi: X:

642
01:09:21.180 --> 01:09:23.819
Inderjit Dhillon: Okay, so. And over here

643
01:09:24.130 --> 01:09:26.260
Inderjit Dhillon: it is mute.

644
01:09:27.180 --> 01:09:31.640
Inderjit Dhillon: That is Mu I H. I X.

645
01:09:32.240 --> 01:09:38.630
Inderjit Dhillon: So each of the Lambda Eyes i'm going to erase that because I don't want you to get confused data off. Oh, wow, what did I do?

646
01:09:41.740 --> 01:09:46.380
Inderjit Dhillon: Okay. I don't want you to get confused. So i'm actually going to erase this.

647
01:09:53.700 --> 01:09:58.500
Inderjit Dhillon: Okay, but basically Lambda I is the Lagrange

648
01:09:58.810 --> 01:10:01.740
Inderjit Dhillon: parameter for

649
01:10:02.420 --> 01:10:05.380
Inderjit Dhillon: the it. Inequality constraints.

650
01:10:05.660 --> 01:10:11.230
Inderjit Dhillon: And me, I is the Lagrange parameter for the it

651
01:10:12.840 --> 01:10:21.020
Inderjit Dhillon: equality constraint. And that's why the lagrangian is a function. This is X. Rm.

652
01:10:21.290 --> 01:10:23.030
Inderjit Dhillon: There are M.

653
01:10:24.210 --> 01:10:28.250
Inderjit Dhillon: Inequality constraints so lambda, one for Lambda, N.

654
01:10:28.890 --> 01:10:32.440
Inderjit Dhillon: And there are so so

655
01:10:33.630 --> 01:10:39.930
Inderjit Dhillon: this is X. Lambda is this? And then I have me

656
01:10:41.760 --> 01:10:49.510
Inderjit Dhillon: okay. And there are p equality constraints. Okay, so that's the lagrangian. So let me just write out

657
01:10:50.820 --> 01:10:54.080
Inderjit Dhillon: what I just said so. Where

658
01:10:54.260 --> 01:10:56.780
Inderjit Dhillon: Lambda I is.

659
01:11:00.910 --> 01:11:06.070
Inderjit Dhillon: I think I said parameter and the Lagrange multiplier.

660
01:11:09.670 --> 01:11:15.930
Inderjit Dhillon: because it multiplies the I in the quality constraint for it

661
01:11:17.050 --> 01:11:18.710
Inderjit Dhillon: inequality.

662
01:11:21.080 --> 01:11:22.320
Inderjit Dhillon: constraint.

663
01:11:26.550 --> 01:11:29.960
Inderjit Dhillon: and mu. I is

664
01:11:34.870 --> 01:11:36.770
Inderjit Dhillon: multiplier.

665
01:11:38.480 --> 01:11:43.560
Inderjit Dhillon: or I it. Equality, constraint.

666
01:11:49.230 --> 01:11:55.860
Inderjit Dhillon: okay. and the lambda and the meal. And you are now starting to see why

667
01:11:55.920 --> 01:12:01.700
Inderjit Dhillon: we have the primal and something called the dual problem. They are also called dual variables.

668
01:12:05.800 --> 01:12:11.840
Inderjit Dhillon: and we will actually see why the word dual is used. Okay.

669
01:12:12.190 --> 01:12:16.690
Inderjit Dhillon: So then we can define something called the Lagrange

670
01:12:19.650 --> 01:12:21.200
Inderjit Dhillon: dual function.

671
01:12:25.820 --> 01:12:29.780
Inderjit Dhillon: Okay. So we can basically take L.

672
01:12:30.300 --> 01:12:31.390
Inderjit Dhillon: X

673
01:12:31.960 --> 01:12:41.580
Inderjit Dhillon: and meal. And what we can do is we can, instead of taking the

674
01:12:42.340 --> 01:12:47.340
Inderjit Dhillon: instead of you know, we can actually take the look at all the X's.

675
01:12:47.930 --> 01:12:51.340
Inderjit Dhillon: and we can take the minimum or infam

676
01:12:53.540 --> 01:12:56.370
Inderjit Dhillon: over X. What if I

677
01:12:57.280 --> 01:13:00.130
Inderjit Dhillon: takes the minimum over

678
01:13:01.150 --> 01:13:05.420
Inderjit Dhillon: X, then I basically get a function of lambda and meal.

679
01:13:12.030 --> 01:13:15.200
Inderjit Dhillon: Okay, which means that this is the

680
01:13:15.630 --> 01:13:19.070
Inderjit Dhillon: No. I can write what L is from above.

681
01:13:20.090 --> 01:13:26.200
Inderjit Dhillon: Right so from above I got. That is the internal over X of

682
01:13:26.720 --> 01:13:32.330
Inderjit Dhillon: F naught x plus summation of I equals one through M.

683
01:13:33.070 --> 01:13:36.430
Inderjit Dhillon: Lambda, I. F. I. X plus

684
01:13:37.130 --> 01:13:39.460
Inderjit Dhillon: mu, I H. I

685
01:13:48.390 --> 01:13:49.210
Inderjit Dhillon: Okay.

686
01:13:50.220 --> 01:14:00.550
Inderjit Dhillon: So i'm going to actually stop. Now. We have just 1min left, but what we will see is all the Lagrange dual function.

687
01:14:01.210 --> 01:14:06.980
Inderjit Dhillon: Okay, is related to the original optimization problem.

688
01:14:07.580 --> 01:14:15.440
Inderjit Dhillon: So there's a duality that exists. And when there's something called the the Duality Gap is 0.

689
01:14:15.460 --> 01:14:27.990
Inderjit Dhillon: Then you can actually solve either the primal problem or the dual problem. And sometimes there's a big advantage to solving the dual problem. It's actually sometimes simpler. It gives more insights.

690
01:14:28.230 --> 01:14:32.140
Inderjit Dhillon: So this is where we will pick off next time.

691
01:14:32.440 --> 01:14:39.100
Inderjit Dhillon: So, just to kind of summarize today, what we did was we started off

692
01:14:39.660 --> 01:14:44.720
Inderjit Dhillon: talking about the perceptron. We saw that it was a pretty simple method.

693
01:14:44.760 --> 01:14:46.550
Inderjit Dhillon: It had some drawbacks.

694
01:14:46.740 --> 01:14:51.050
Inderjit Dhillon: We overcome these drawbacks, but use kind of the same

695
01:14:51.410 --> 01:15:03.870
Inderjit Dhillon: separating hyperplane principle. We talked about support vector machines. We delight what the objective for a support vector machine is.

696
01:15:04.210 --> 01:15:07.150
Inderjit Dhillon: Okay. And now we are looking at

697
01:15:07.300 --> 01:15:14.870
Inderjit Dhillon: kind of more insight into the support vector machine. So we are looking in depth into this constraint optimization problem.

698
01:15:16.040 --> 01:15:22.130
Inderjit Dhillon: Okay. So that kind of concludes today's lecture any questions.

699
01:15:26.390 --> 01:15:30.800
Inderjit Dhillon: So remember the homework has been assigned, and the homework is due

700
01:15:30.840 --> 01:15:38.730
Inderjit Dhillon: next next Friday. Okay. So next time we will continue off, and

701
01:15:38.750 --> 01:15:44.380
Inderjit Dhillon: I think i'll be able to wrap up talking about support vector machines in the next class.

702
01:15:45.520 --> 01:15:50.200
Inderjit Dhillon: Okay. thank you all. See you on Wednesday. But.

WEBVTT

1
00:01:19.700 --> 00:01:20.890
Inderjit Dhillon: Okay.

2
00:01:21.960 --> 00:01:24.000
Inderjit Dhillon: Good afternoon. Everybody.

3
00:01:27.180 --> 00:01:29.680
Inderjit Dhillon: Let's get started

4
00:01:32.700 --> 00:01:36.450
Inderjit Dhillon: any questions for me before we start or for the PA.

5
00:01:52.130 --> 00:01:59.220
Inderjit Dhillon: Okay. So if not, let's start. we're going to continue where we left off yesterday.

6
00:02:04.770 --> 00:02:05.590
Inderjit Dhillon: Okay.

7
00:02:06.890 --> 00:02:09.009
Inderjit Dhillon: So

8
00:02:09.190 --> 00:02:25.620
Inderjit Dhillon: if you remember what we we're doing yesterday that's or 2 day ago, let's recap first. We talked about a very classical algorithm for classification problem, binary classification. This was the perceptron algorithm

9
00:02:25.710 --> 00:02:36.920
Inderjit Dhillon: proposed nearly 60 years ago, a big landmark. and we saw that perceptron is essentially stochastic, gradient descent on a particular kind of

10
00:02:38.290 --> 00:02:42.390
Inderjit Dhillon: but misclassification criteria.

11
00:02:42.610 --> 00:02:51.270
Inderjit Dhillon: and it has a very simple algorithm but there are some disadvantages to the receptor on

12
00:02:51.350 --> 00:02:53.080
Inderjit Dhillon: in that

13
00:02:53.300 --> 00:02:58.790
Inderjit Dhillon: it could actually, you know, not necessarily give a good hyperplane.

14
00:02:58.960 --> 00:03:00.540
Inderjit Dhillon: And then we tried to

15
00:03:00.630 --> 00:03:11.020
Inderjit Dhillon: define what notion of a good hyperplane is in some sense it is a hyperplane that is maximally far apart from the 2 classes.

16
00:03:11.030 --> 00:03:15.780
Inderjit Dhillon: So we made that concrete. and we showed that

17
00:03:17.430 --> 00:03:31.170
Inderjit Dhillon: if we the support vector machine problem is essentially a quadratic optimization problem. So a quadratic objective function. subject to linear inequality constraints.

18
00:03:31.310 --> 00:03:35.530
Inderjit Dhillon: and it ends up, giving us the hyperplane with maximum margin.

19
00:03:36.210 --> 00:03:48.200
Inderjit Dhillon: and then we wanted to kind of understand what Svm's do, what support vector machines do, and one way to do that is to and dive deeper into

20
00:03:48.380 --> 00:03:49.290
Inderjit Dhillon: the

21
00:03:49.320 --> 00:03:57.010
Inderjit Dhillon: what's called the primal support vector machine problem, which is what we wrote over here in these rectangular boxes.

22
00:03:57.270 --> 00:04:04.960
Inderjit Dhillon: and then we are going to take a little deep to, to generally talk about general constraint, optimization problems.

23
00:04:07.930 --> 00:04:15.490
Inderjit Dhillon: So here we decide. You know, we define like canonical constraint, optimization problem, which is minimize your function. F. Not

24
00:04:15.840 --> 00:04:23.220
Inderjit Dhillon: x. Subject to M. Inequality constraints. If I. X is less than or equal to 0 and be

25
00:04:23.420 --> 00:04:27.570
Inderjit Dhillon: equality constraints. H. I. X. Equal to 0. Okay.

26
00:04:27.700 --> 00:04:38.920
Inderjit Dhillon: So X belongs to Rm. Then we can define a lagrangian which essentially uses lagrange multipliers for each of the inequality constraints. These are called lambda eyes.

27
00:04:39.030 --> 00:04:50.540
Inderjit Dhillon: and new eyes are the Lagrange multiplier for the big quality constraints. and these are called dual variables, and it will become 3, or why they are called to a variables.

28
00:04:51.030 --> 00:04:53.860
Inderjit Dhillon: And then we define what is called

29
00:04:54.350 --> 00:05:01.280
Inderjit Dhillon: Lagrange dual function, which now will only depend, which will be a function only of the dual variables.

30
00:05:01.410 --> 00:05:05.400
Inderjit Dhillon: and we'll in some sense get rid of the private variables

31
00:05:05.430 --> 00:05:08.930
Inderjit Dhillon: by taking the infam over all X.

32
00:05:11.890 --> 00:05:20.920
Inderjit Dhillon: So now this is where we were yesterday or or again sorry on Monday. And now let's continue this treatment.

33
00:05:21.110 --> 00:05:34.740
Inderjit Dhillon: And just to give you a preview of what is to come. By doing this, we'll actually be able to get quite a lot of insight into what the support vector machine problem actually finds. Okay, and the decision surface.

34
00:05:34.860 --> 00:05:38.330
Inderjit Dhillon: it will become clear, is actually a ends up, being a

35
00:05:38.450 --> 00:05:54.860
Inderjit Dhillon: linear combination of what's called called the support vectors and the support vectors are called so because they actually support the hyperplane. These are vectors from class one as well as vectors from or data points from class to

36
00:05:55.920 --> 00:05:57.460
Inderjit Dhillon: So

37
00:05:57.470 --> 00:06:02.230
Inderjit Dhillon: So that's what's to come. But first we need to look at this

38
00:06:02.830 --> 00:06:14.460
Inderjit Dhillon: constrained optimization machinery, and just just reminding you that the material that i'm teaching is actually from a convex optimization book by Stephen Boyd and Levin Vanderberg.

39
00:06:14.730 --> 00:06:18.070
Inderjit Dhillon: So Lambda and mu.

40
00:06:19.640 --> 00:06:21.070
Inderjit Dhillon: You are

41
00:06:22.490 --> 00:06:25.200
Inderjit Dhillon: dual, feasible. So these are

42
00:06:26.490 --> 00:06:37.290
Inderjit Dhillon: dual variables, I already said. But we are saying that this is feasible. So feasible means that the constraint is satisfied so generally, when we say that X is feasible.

43
00:06:37.300 --> 00:06:45.000
Inderjit Dhillon: it will mean that you know this constraint. If you are given a particular x.

44
00:06:45.380 --> 00:07:01.430
Inderjit Dhillon: then all the Fi X's are less than equal to 0, and all the H. I. X's are equal to 0. That's what we mean by a feasible solution. And of course the optimal solution would be optimal, primal solution would be one which minimizes F naught. X.

45
00:07:01.430 --> 00:07:09.140
Inderjit Dhillon: While being feasible while satisfying the linear inequality and linear or sorry equality

46
00:07:09.280 --> 00:07:16.120
Inderjit Dhillon: and inequality constraints. The fi is in this treatment actually do not have to be linear.

47
00:07:17.160 --> 00:07:21.640
Inderjit Dhillon: So. What we say is that Lambda and Mu are dual, feasible. If

48
00:07:22.320 --> 00:07:28.110
Inderjit Dhillon: the constraints that correspond to the inequality constraints are greater than equal to 0,

49
00:07:28.340 --> 00:07:36.840
Inderjit Dhillon: so the Lambdas are greater than or equal to 0. Okay. and there is no restriction. There is no fee, a constraint on the

50
00:07:36.950 --> 00:07:48.040
Inderjit Dhillon: meals which are the lagrange multipliers for the inequality constraints. But then we do want to have a qualification which is that Lambda Mu is not.

51
00:07:48.120 --> 00:07:51.480
Inderjit Dhillon: Oh, sorry! G. Of Lambda M. Is not unbounded.

52
00:07:51.590 --> 00:07:55.370
Inderjit Dhillon: and that it is greater than equal to minus infinity.

53
00:07:56.300 --> 00:08:10.190
Inderjit Dhillon: So let's see. Now let's write down a couple of facts i'm not necessarily going to go through all the proofs over here, but you know the main main ones. But fact one is the G. Lambda Mu

54
00:08:10.560 --> 00:08:13.150
Inderjit Dhillon: is less than equal to P. Star.

55
00:08:13.930 --> 00:08:16.290
Inderjit Dhillon: Okay, what is P. Star? Well.

56
00:08:16.320 --> 00:08:21.860
Inderjit Dhillon: P. Star is the optimal value of this equation One.

57
00:08:23.600 --> 00:08:28.010
Inderjit Dhillon: Okay. So P. Star is going to be the F. Naught X star

58
00:08:28.020 --> 00:08:35.970
Inderjit Dhillon: where x star is the optimal solution. So let me write it down. So G. Lambda is less than equal to P. Star for any

59
00:08:38.520 --> 00:08:41.220
Inderjit Dhillon: dual usable

60
00:08:43.220 --> 00:08:47.040
Inderjit Dhillon: lambda meal where

61
00:08:48.000 --> 00:08:50.140
Inderjit Dhillon: the star is

62
00:08:51.940 --> 00:08:54.580
Inderjit Dhillon: optimal value.

63
00:08:56.090 --> 00:08:57.180
Inderjit Dhillon: of course.

64
00:08:58.310 --> 00:09:00.080
Inderjit Dhillon: Okay, Which means that.

65
00:09:00.270 --> 00:09:03.750
Inderjit Dhillon: And this means that we star

66
00:09:04.070 --> 00:09:08.090
Inderjit Dhillon: is equal to F naught. X star.

67
00:09:08.620 --> 00:09:22.640
Inderjit Dhillon: And since we are saying that it is optimal value of of one X star is also a feasible solution, which means that if I of X star is less than or equal to 0, and H. I of X star is equal to 0,

68
00:09:25.650 --> 00:09:27.470
Inderjit Dhillon: and then fact 2.

69
00:09:29.540 --> 00:09:41.170
Inderjit Dhillon: So think about what fact one says. Okay. fact. One says that the Lagrange dual function is upper, bounded by P. Star.

70
00:09:42.300 --> 00:09:45.870
Inderjit Dhillon: So if I now try to maximize G. Lab

71
00:09:46.980 --> 00:09:53.140
Inderjit Dhillon: for Lambda and mail over all Lambda and Mu the maximum value it can reach us. P. Star.

72
00:09:54.430 --> 00:09:58.220
Inderjit Dhillon: And what we are going to say is that strong duality holds

73
00:09:58.400 --> 00:10:04.020
Inderjit Dhillon: when maximum value of G. Lambda, M. For feasible

74
00:10:04.820 --> 00:10:09.980
Inderjit Dhillon: Lambda and Mu is actually equal to P. Star.

75
00:10:10.740 --> 00:10:20.950
Inderjit Dhillon: So that means you. While you are minimizing the primal. you can actually equivalently maximize the dual. and you actually get the same value

76
00:10:21.130 --> 00:10:24.030
Inderjit Dhillon: in if strong duality calls. Okay.

77
00:10:25.240 --> 00:10:28.720
Inderjit Dhillon: So let me write that down. So

78
00:10:28.890 --> 00:10:33.100
Inderjit Dhillon: back to is that if there exists dual.

79
00:10:34.300 --> 00:10:35.440
Inderjit Dhillon: feasible

80
00:10:36.800 --> 00:10:47.690
Inderjit Dhillon: Lambda, Star. New Star. and all that means is that Lambda Star is greater than or equal to 0,

81
00:10:49.900 --> 00:10:52.750
Inderjit Dhillon: and if I have primal

82
00:10:54.720 --> 00:10:55.880
Inderjit Dhillon: Reusable

83
00:10:57.890 --> 00:10:59.210
Inderjit Dhillon: X. Star

84
00:11:00.370 --> 00:11:07.750
Inderjit Dhillon: x star Oops. Sorry too much too close to the edge. Let me see if I can get in there like star

85
00:11:09.000 --> 00:11:10.400
Inderjit Dhillon: such that

86
00:11:11.960 --> 00:11:16.930
Inderjit Dhillon: G. Of Lambda Star. new star

87
00:11:17.570 --> 00:11:23.160
Inderjit Dhillon: equals to P. Star, which amount, remember, is equal to F naught. X star.

88
00:11:24.020 --> 00:11:25.350
Inderjit Dhillon: Then

89
00:11:26.620 --> 00:11:28.970
Inderjit Dhillon: strong duality

90
00:11:33.590 --> 00:11:36.470
Inderjit Dhillon: is said to hold.

91
00:11:39.060 --> 00:11:41.260
Inderjit Dhillon: Okay, so just like, I said.

92
00:11:41.370 --> 00:11:49.160
Inderjit Dhillon: if you're trying to minimize this one. Sorry over here, if i'm trying to minimize one.

93
00:11:49.290 --> 00:11:58.910
Inderjit Dhillon: and X star is the optimal and the corresponding F. Not x star is P. Star. Another way of getting that is actually to form the dual

94
00:11:59.250 --> 00:12:03.660
Inderjit Dhillon: and maximize G. Of Lambda Me. So that's why

95
00:12:04.010 --> 00:12:17.320
Inderjit Dhillon: we say that this is the dual problem. And now the using of the word dual should be somewhat obvious that you have a problem, and in some sense it's a dual, but you can actually get

96
00:12:17.400 --> 00:12:28.830
Inderjit Dhillon: to in some sense the solution of the solution value of the final. So if I maximize lambda so over Lambda and Mu

97
00:12:29.570 --> 00:12:31.670
Inderjit Dhillon: g of Lambda mu

98
00:12:32.810 --> 00:12:36.490
Inderjit Dhillon: such that lambda is greater than or equal to 0.

99
00:12:39.920 --> 00:12:44.210
Inderjit Dhillon: So this is the dual problem, and if strong duality holds.

100
00:12:44.470 --> 00:12:50.460
Inderjit Dhillon: Then I will basically this maximum value, or G. Of Lambda Star Mu Star

101
00:12:50.480 --> 00:12:54.760
Inderjit Dhillon: will actually be equal to P. Star. That's what this says.

102
00:12:56.480 --> 00:12:58.340
Inderjit Dhillon: Okay, so this is

103
00:12:58.460 --> 00:13:03.750
Inderjit Dhillon: theory in constrained optimization, general constraint, optimization.

104
00:13:06.160 --> 00:13:14.250
Inderjit Dhillon: Now let's see this. It actually has so interesting implications. So suppose

105
00:13:17.020 --> 00:13:18.770
Inderjit Dhillon: strong duality

106
00:13:21.880 --> 00:13:23.000
Inderjit Dhillon: holds

107
00:13:25.460 --> 00:13:28.000
Inderjit Dhillon: for Lambda Star

108
00:13:28.220 --> 00:13:31.250
Inderjit Dhillon: new star next door

109
00:13:32.570 --> 00:13:33.690
Inderjit Dhillon: them.

110
00:13:36.010 --> 00:13:45.550
Inderjit Dhillon: Okay, let's examine it. We have F, not X star. That's what we said is P. Star. and from above from over here.

111
00:13:46.760 --> 00:13:48.640
Inderjit Dhillon: Right? We have

112
00:13:49.380 --> 00:13:50.490
Inderjit Dhillon: that

113
00:13:51.080 --> 00:13:55.120
Inderjit Dhillon: this is equal to G. Of Lambda Star.

114
00:13:55.140 --> 00:13:56.150
Inderjit Dhillon: you still.

115
00:13:58.430 --> 00:14:02.550
Inderjit Dhillon: Now, what was G. Of Lambda Star, me Star. Okay.

116
00:14:03.570 --> 00:14:08.190
Inderjit Dhillon: It's equal to P. Star. right, and G. Of Lambda Mu

117
00:14:08.520 --> 00:14:10.780
Inderjit Dhillon: is from here.

118
00:14:15.450 --> 00:14:18.730
Inderjit Dhillon: It is equal to the infim of

119
00:14:19.340 --> 00:14:22.590
Inderjit Dhillon: the Lagrange tool function with respect to X,

120
00:14:22.940 --> 00:14:28.460
Inderjit Dhillon: so I can write that as infam. So i'm basically just copying it from above.

121
00:14:28.950 --> 00:14:31.430
Inderjit Dhillon: I have interim over X

122
00:14:32.580 --> 00:14:36.270
Inderjit Dhillon: L. Of X. But at the optimal

123
00:14:37.600 --> 00:14:41.330
Inderjit Dhillon: right? Because i'm looking at G. Of Lambda Star and New Star.

124
00:14:41.860 --> 00:14:45.150
Inderjit Dhillon: Okay. Now, if I just expand this out

125
00:14:45.300 --> 00:14:49.860
Inderjit Dhillon: right, I expand this out. I have infirm of X.

126
00:14:51.520 --> 00:14:54.810
Inderjit Dhillon: L. The dual function is f naught.

127
00:14:54.850 --> 00:14:55.830
Inderjit Dhillon: X

128
00:14:56.220 --> 00:15:00.400
Inderjit Dhillon: last summation of I equals one through M.

129
00:15:00.940 --> 00:15:03.010
Inderjit Dhillon: Lambda I. Star

130
00:15:04.090 --> 00:15:06.100
Inderjit Dhillon: F. I. X

131
00:15:06.160 --> 00:15:11.130
Inderjit Dhillon: plus summation mu star I.

132
00:15:11.520 --> 00:15:16.560
Inderjit Dhillon: H. I. X. All I've done is Basically, copy in from above.

133
00:15:17.930 --> 00:15:21.470
Inderjit Dhillon: Yeah, I just copied the definition which is given over here.

134
00:15:22.510 --> 00:15:23.380
Inderjit Dhillon: Okay.

135
00:15:24.520 --> 00:15:26.040
Inderjit Dhillon: but over here

136
00:15:26.140 --> 00:15:31.300
Inderjit Dhillon: it is at the Dual optimal at Lambda Star and New Star.

137
00:15:31.360 --> 00:15:41.030
Inderjit Dhillon: Okay? Well, since this is the infam. this must be less than equal to the value at any other X in particular at X Star.

138
00:15:41.720 --> 00:15:44.810
Inderjit Dhillon: So if I write X star over here.

139
00:15:45.860 --> 00:15:48.490
Inderjit Dhillon: right, this is less than equal to this.

140
00:15:54.360 --> 00:15:59.170
Inderjit Dhillon: Okay, so just writing the above. but substituting

141
00:16:02.080 --> 00:16:03.960
Inderjit Dhillon: X star for X.

142
00:16:04.320 --> 00:16:15.300
Inderjit Dhillon: Okay, it's. It's the interim overall X. So that means it must be less than equal to the value of what's in the brackets, or you know, for a particular x, which is X star.

143
00:16:15.660 --> 00:16:18.450
Inderjit Dhillon: Now let's look at this. Okay.

144
00:16:18.610 --> 00:16:26.080
Inderjit Dhillon: I have Lambda I Star Lambda. I star is dual feasible. So this is greater than or equal to 0.

145
00:16:27.980 --> 00:16:30.900
Inderjit Dhillon: Okay? And then I have F. I. X star.

146
00:16:31.640 --> 00:16:37.000
Inderjit Dhillon: and this is less than equal to 0. Right? X star is

147
00:16:37.190 --> 00:16:42.460
Inderjit Dhillon: dual, feasible. X star is a primal, feasible, a private feasible. Yes.

148
00:16:42.870 --> 00:16:48.580
Inderjit Dhillon: so I have something which is less than equal to 0 times greater than equal to 0.

149
00:16:48.640 --> 00:16:55.910
Inderjit Dhillon: So this is this: each lambda I star Times, F. I. X star is now less than equals.

150
00:16:59.300 --> 00:17:02.330
Inderjit Dhillon: Now I look over here, and clearly.

151
00:17:03.050 --> 00:17:10.190
Inderjit Dhillon: This term equals 0, right? Because it is primal, feasible. So this whole term is 0.

152
00:17:12.310 --> 00:17:15.280
Inderjit Dhillon: So the right hand side is F naught x star.

153
00:17:15.520 --> 00:17:19.040
Inderjit Dhillon: and then something negative, and then something 0,

154
00:17:19.599 --> 00:17:23.700
Inderjit Dhillon: as a result of which this is less than equal to, if not next.

155
00:17:26.940 --> 00:17:30.470
Inderjit Dhillon: But if you look at about, we started with F, one X star.

156
00:17:31.980 --> 00:17:34.330
Inderjit Dhillon: and we ended with, if not external.

157
00:17:36.470 --> 00:17:41.420
Inderjit Dhillon: So what this says is, F. Naught x star is less than equal to F. Not X star. But of course.

158
00:17:41.890 --> 00:17:51.940
Inderjit Dhillon: if not, x stop the this must hold with equality. Okay. So that means that both of these must be not inequalities, but inequalities.

159
00:17:52.560 --> 00:17:56.270
Inderjit Dhillon: So the above

160
00:17:58.150 --> 00:18:00.670
Inderjit Dhillon: to inequalities

161
00:18:04.370 --> 00:18:06.410
Inderjit Dhillon: must hold

162
00:18:08.210 --> 00:18:10.530
Inderjit Dhillon: with hey quality.

163
00:18:14.140 --> 00:18:17.290
Inderjit Dhillon: which means so what is the implication of the first one?

164
00:18:18.530 --> 00:18:22.080
Inderjit Dhillon: Okay. So this says that X star

165
00:18:22.290 --> 00:18:24.740
Inderjit Dhillon: is actually so. If I look over here

166
00:18:26.390 --> 00:18:27.560
Inderjit Dhillon: and here.

167
00:18:28.600 --> 00:18:35.860
Inderjit Dhillon: this says that if this is an equality, it means that the infam is actually achieved at X Star.

168
00:18:39.100 --> 00:18:39.850
Inderjit Dhillon: Okay.

169
00:18:40.150 --> 00:18:45.920
Inderjit Dhillon: So X Star. Must we minimizer

170
00:18:48.420 --> 00:18:56.520
Inderjit Dhillon: off L. X. Lambda Star new stuff. So at the

171
00:18:57.080 --> 00:19:00.140
Inderjit Dhillon: dual variables that are optimal.

172
00:19:01.140 --> 00:19:07.420
Inderjit Dhillon: If I consider the Lagrange Dual function X star must be the minimizer of it.

173
00:19:08.040 --> 00:19:11.550
Inderjit Dhillon: And then, if I look at this last part.

174
00:19:12.370 --> 00:19:14.760
Inderjit Dhillon: this must hold with equality.

175
00:19:16.310 --> 00:19:18.530
Inderjit Dhillon: And what that means is that

176
00:19:20.310 --> 00:19:26.160
Inderjit Dhillon: remember, this was less than equal to 0. Well, this means that this should be actually equal to 0.

177
00:19:26.660 --> 00:19:34.420
Inderjit Dhillon: So this means that Lambda, I. Star F. I. X. Star must be equal to 0,

178
00:19:38.040 --> 00:19:49.230
Inderjit Dhillon: and this is a sum. But of course each term has the same sign right, which is less than or equal to 0, which means each of the individual terms must be equal to 0,

179
00:19:51.450 --> 00:19:55.030
Inderjit Dhillon: for I equals 1, 2 through M.

180
00:20:04.360 --> 00:20:05.260
Inderjit Dhillon: Okay.

181
00:20:06.550 --> 00:20:12.080
Inderjit Dhillon: So what does that mean? So this means if this multiplication is equal to

182
00:20:12.210 --> 00:20:15.000
Inderjit Dhillon: 0. So what this means that if

183
00:20:16.000 --> 00:20:31.900
Inderjit Dhillon: Lambda I star is greater than 0, remember it is greater than equal to 0, but it suppose it is strictly greater than 0. Then this says that F. Of I. X star must be equal to 0.

184
00:20:32.870 --> 00:20:38.960
Inderjit Dhillon: Okay. And remember, the feasible set is Fi. X is less than equal to 0.

185
00:20:39.330 --> 00:20:47.640
Inderjit Dhillon: So what we say is that that inequality constraint is active because it actually holds with equality.

186
00:20:48.360 --> 00:20:51.740
Inderjit Dhillon: Okay, and what? And the secondly, it says that if

187
00:20:52.510 --> 00:20:55.320
Inderjit Dhillon: F. I. Of X star

188
00:20:56.220 --> 00:20:59.500
Inderjit Dhillon: equal to 0, which is the second term over here

189
00:21:00.010 --> 00:21:04.150
Inderjit Dhillon: right sorry if I. F. Of X star is less than 0.

190
00:21:05.340 --> 00:21:13.500
Inderjit Dhillon: So strictly in the quality. then the corresponding lagrange multiplier must actually be equal to 0.

191
00:21:17.000 --> 00:21:25.840
Inderjit Dhillon: So it actually gives a nice characterization that that certain inequality constraints will hold with inequality. These are called the active constraints.

192
00:21:26.020 --> 00:21:30.560
Inderjit Dhillon: and there the Lagrange multiplier will be greater than people.

193
00:21:31.160 --> 00:21:40.880
Inderjit Dhillon: But if there is slack in some inequality constraint, and it doesn't hold with it doesn't hold tightly.

194
00:21:40.990 --> 00:21:44.690
Inderjit Dhillon: then that means that the

195
00:21:44.870 --> 00:21:55.170
Inderjit Dhillon: constraint is not active, and the corresponding, like launch multiplier, is actually equal to 0. So there is something called.

196
00:21:57.710 --> 00:22:00.650
Inderjit Dhillon: Let me just write it

197
00:22:02.660 --> 00:22:07.100
Inderjit Dhillon: tucker condition, and this is really called Kkt.

198
00:22:08.390 --> 00:22:11.070
Inderjit Dhillon: Okay. So it's the Kkt conditions.

199
00:22:13.380 --> 00:22:18.260
Inderjit Dhillon: These provide a certificate of optimality

200
00:22:22.550 --> 00:22:25.010
Inderjit Dhillon: certificate of optimality.

201
00:22:27.530 --> 00:22:28.820
Inderjit Dhillon: When.

202
00:22:29.960 --> 00:22:42.780
Inderjit Dhillon: so far, all we have what we have done is looked at. General. If not, we haven't said anything about what kind of functions, if nots are. or the fis or the h i's.

203
00:22:43.040 --> 00:22:45.030
Inderjit Dhillon: But now, if you say that if

204
00:22:45.640 --> 00:22:48.230
Inderjit Dhillon: this problem is convex.

205
00:22:49.360 --> 00:23:07.390
Inderjit Dhillon: which is certainly the case in the Svm problem, the support vector machine problem. Okay. So the Kkt conditions provide a certificate of optimality. When problem one is convex certificate of optimality. Let's. Let's actually just dive into it, and then we can discuss it.

206
00:23:07.680 --> 00:23:12.080
Inderjit Dhillon: So let's see. these are the Kkt conditions

207
00:23:15.190 --> 00:23:16.230
Inderjit Dhillon: for

208
00:23:17.560 --> 00:23:22.700
Inderjit Dhillon: X Star and the Dual Variables, Lambda Star and New Star.

209
00:23:30.930 --> 00:23:43.040
Inderjit Dhillon: So optimality conditions, right? What does that mean? So we are given some X star. We are given some lambda to our new star. How can we tell that this gives us an optimal solution? Well.

210
00:23:43.410 --> 00:23:46.050
Inderjit Dhillon: they must be X star.

211
00:23:46.450 --> 00:23:58.730
Inderjit Dhillon: Both the dual variables and the primal variables must be feasible. So what is feasibility mean? Well, Feasibility means that fi effects are

212
00:23:59.100 --> 00:24:03.240
Inderjit Dhillon: must be less than equal to 0. I equals one to

213
00:24:03.680 --> 00:24:04.730
Inderjit Dhillon: U. M.

214
00:24:05.640 --> 00:24:10.180
Inderjit Dhillon: And H. I of X star must be equal to 0.

215
00:24:10.240 --> 00:24:16.870
Inderjit Dhillon: I equals 1 2. Okay. So this is

216
00:24:18.980 --> 00:24:20.490
Inderjit Dhillon: primal, feasible.

217
00:24:22.850 --> 00:24:29.890
Inderjit Dhillon: Okay, X Star is probably more feasible if these conditions are there. and then we also must have dual feasibility.

218
00:24:32.340 --> 00:24:35.450
Inderjit Dhillon: Okay, so that's one of the conditions, obviously, for

219
00:24:36.810 --> 00:24:40.340
Inderjit Dhillon: you know, so being a certificate of optimality.

220
00:24:44.090 --> 00:24:49.720
Inderjit Dhillon: so the Lagrange multipliers for the inequality constraints must be

221
00:24:50.030 --> 00:24:55.300
Inderjit Dhillon: greater than equal to 0, and then we have

222
00:24:55.410 --> 00:24:56.830
Inderjit Dhillon: this part

223
00:24:59.560 --> 00:25:10.910
Inderjit Dhillon: as a certificate of our feasibility. Okay, which is that Lambda I star Times, F of I. X star must be equal to 0.

224
00:25:11.440 --> 00:25:19.870
Inderjit Dhillon: I equals 1, 2 through M. And this is called complementary slightness.

225
00:25:28.030 --> 00:25:37.360
Inderjit Dhillon: Okay. And if you think about the name, complimentary slackness, what it's, it means is that when there is slack in my

226
00:25:37.670 --> 00:25:46.630
Inderjit Dhillon: inequality constraints Fi. So that means if I, if I of X star is less than 0, then Lambda, I star is equal to 0.

227
00:25:47.290 --> 00:25:49.340
Inderjit Dhillon: Okay. But if

228
00:25:49.510 --> 00:25:52.040
Inderjit Dhillon: Lambda I star is greater than 0,

229
00:25:52.350 --> 00:25:56.380
Inderjit Dhillon: then the corresponding inequality constraint must be tight.

230
00:25:56.650 --> 00:26:01.480
Inderjit Dhillon: which means that there is no slack. and that

231
00:26:01.500 --> 00:26:06.380
Inderjit Dhillon: that inequality, constraint, and, you know, is

232
00:26:06.460 --> 00:26:15.620
Inderjit Dhillon: is active, and is actually holds within with equality. And then the final condition is that X star

233
00:26:16.260 --> 00:26:19.540
Inderjit Dhillon: must be equal to our men

234
00:26:20.510 --> 00:26:21.520
Inderjit Dhillon: of

235
00:26:21.870 --> 00:26:25.930
Inderjit Dhillon: L. X. Lambda Star mixed up.

236
00:26:26.200 --> 00:26:30.460
Inderjit Dhillon: Okay, so these all these things are. We have talked a little bit about above.

237
00:26:30.650 --> 00:26:33.700
Inderjit Dhillon: But this is just saying that these

238
00:26:33.890 --> 00:26:35.960
Inderjit Dhillon: things i'm sorry.

239
00:26:37.100 --> 00:26:39.180
Inderjit Dhillon: These cool.

240
00:26:41.630 --> 00:26:43.370
Inderjit Dhillon: the dual and

241
00:26:44.050 --> 00:26:47.190
Inderjit Dhillon: primal variables must satisfy

242
00:26:47.430 --> 00:26:51.600
Inderjit Dhillon: this, and these are the corresponding conditions.

243
00:26:52.000 --> 00:26:56.950
Inderjit Dhillon: Actually let me add one more part to it. As to what this means.

244
00:27:00.060 --> 00:27:05.700
Inderjit Dhillon: Okay, this is over X. And this means that

245
00:27:07.600 --> 00:27:12.490
Inderjit Dhillon: if I take the gradient of X. 0, X star

246
00:27:15.280 --> 00:27:23.360
Inderjit Dhillon: gradient of F of I. And evaluated at X Star. And if I look at this entire expression.

247
00:27:34.670 --> 00:27:36.690
Inderjit Dhillon: this must be equal to 0.

248
00:27:45.980 --> 00:28:00.630
Inderjit Dhillon: Okay? So that concludes this slight kind of diversion that I've made to look at. You know, general problems. Okay? Well, this is a treatment which is for any

249
00:28:04.630 --> 00:28:06.660
Inderjit Dhillon: any problem with this form.

250
00:28:11.930 --> 00:28:13.860
Inderjit Dhillon: And then at the end

251
00:28:14.720 --> 00:28:28.770
Inderjit Dhillon: we said that, hey? You know, Kitty, conditions when problem one is convex or these conditions. So, having taken that detail, let's go back to my Svm problem

252
00:28:30.900 --> 00:28:44.890
Inderjit Dhillon: and let us see what we talked about in the beginning, that the reason for taking this detail is to get some insight into You know what this maximum margin hyperplane that we sought for the support vector machine problem.

253
00:28:45.070 --> 00:28:45.940
Inderjit Dhillon: you know.

254
00:28:45.980 --> 00:28:48.490
Inderjit Dhillon: Can we get more insight into it, and we will.

255
00:28:48.810 --> 00:28:54.250
Inderjit Dhillon: So this is the primal problem which is minimize.

256
00:28:55.010 --> 00:28:59.030
Inderjit Dhillon: I'm just gonna put a half doesn't. Just simplify things a little bit

257
00:28:59.450 --> 00:29:08.940
Inderjit Dhillon: over W. W. Not such that y I w transpose xi plus w naught

258
00:29:09.310 --> 00:29:12.020
Inderjit Dhillon: is greater than or equal to one, for

259
00:29:12.460 --> 00:29:18.460
Inderjit Dhillon: I equals 1, 2 through. Let's just assume there are endpoints capital.

260
00:29:19.430 --> 00:29:24.040
Inderjit Dhillon: And now remember that my my general

261
00:29:26.220 --> 00:29:32.600
Inderjit Dhillon: form of the problems was such that I had any quality constraints. Okay.

262
00:29:33.060 --> 00:29:40.470
Inderjit Dhillon: less than equal to 0. F. Of I. X is less than or equal to 0. So let's put this in that form right.

263
00:29:40.560 --> 00:29:43.090
Inderjit Dhillon: and the way to write that in that form is

264
00:29:43.430 --> 00:29:50.160
Inderjit Dhillon: one minus y. I times W. Transpose Xi plus W. Naught

265
00:29:50.270 --> 00:29:52.520
Inderjit Dhillon: is less than equals 0.

266
00:29:56.610 --> 00:29:59.080
Inderjit Dhillon: So these 2, of course, are just identical.

267
00:30:02.090 --> 00:30:05.300
Inderjit Dhillon: Okay. But this is in the form of

268
00:30:07.400 --> 00:30:10.160
Inderjit Dhillon: this is like F. Of 5,

269
00:30:11.270 --> 00:30:17.460
Inderjit Dhillon: you know. W. Remember, our variable is W. Now W. Equal to not equal to 0. But

270
00:30:19.430 --> 00:30:23.650
Inderjit Dhillon: this is F. Of I. W. Is less than equal to 0. Okay.

271
00:30:23.720 --> 00:30:33.540
Inderjit Dhillon: So now let's check through the machinery, right? Basically we got the machinery for trying to get to the dual problem. So let's look at what the lagrangian is.

272
00:30:37.610 --> 00:30:48.810
Inderjit Dhillon: Okay. So remember the lagrangian is function of W. And then the also all the primal variables here, Remember, we have split them into W and W not.

273
00:30:49.180 --> 00:30:52.910
Inderjit Dhillon: and then let me call the dual variables alpha

274
00:30:54.030 --> 00:31:00.490
Inderjit Dhillon: So Alpha I is a dual variable for training. Point X. I. That corresponds to the

275
00:31:00.860 --> 00:31:08.710
Inderjit Dhillon: constraint. Fi w is less than or equal to 0. So what is the

276
00:31:13.100 --> 00:31:26.390
Inderjit Dhillon: it's half of w square plus summation of I equals one through n I have Al 5, one minus y. I

277
00:31:26.890 --> 00:31:30.360
Inderjit Dhillon: W. Transpose x I plus W. Not

278
00:31:33.460 --> 00:31:35.380
Inderjit Dhillon: just give me 1min.

279
00:31:48.230 --> 00:31:51.410
Inderjit Dhillon: Sorry. I just closed the door because there was some noise outside.

280
00:31:52.140 --> 00:31:53.920
Inderjit Dhillon: Okay, so

281
00:31:54.840 --> 00:31:57.620
Inderjit Dhillon: this is my lagrangian. Okay?

282
00:31:58.210 --> 00:32:00.870
Inderjit Dhillon: And remember, we thought that the

283
00:32:01.030 --> 00:32:11.110
Inderjit Dhillon: you know, if in this case I can take the minimum with respect to the through W.

284
00:32:12.530 --> 00:32:14.040
Inderjit Dhillon: So gradient of

285
00:32:19.070 --> 00:32:30.890
Inderjit Dhillon: so gradient of L. With respect to W. And W. Not let me see what happens when this is set to 0.

286
00:32:44.210 --> 00:32:52.490
Inderjit Dhillon: So I have W. So i'm taking the gradient. Now gradient of half of normal W. Squared is just W.

287
00:32:52.990 --> 00:32:58.710
Inderjit Dhillon: And then from the summation i'll see that what I get is summation of I

288
00:32:59.080 --> 00:33:01.030
Inderjit Dhillon: equals one through N.

289
00:33:01.330 --> 00:33:04.950
Inderjit Dhillon: Well, alpha it does not pick up, you know.

290
00:33:06.490 --> 00:33:13.160
Inderjit Dhillon: is not a function of W. So I have alpha I y iw transpose Xi. So that's where W. Occurs.

291
00:33:13.480 --> 00:33:15.920
Inderjit Dhillon: so my gradient is

292
00:33:16.430 --> 00:33:18.930
Inderjit Dhillon: minus Alpha I

293
00:33:18.950 --> 00:33:22.160
Inderjit Dhillon: Why, I excellent.

294
00:33:23.500 --> 00:33:33.400
Inderjit Dhillon: and this equals 0, which implies that W. Is equal to summation of I equals one through N.

295
00:33:33.860 --> 00:33:42.560
Inderjit Dhillon: Alpha I Y. I X. I. So you actually now see something interesting, right that my W.

296
00:33:42.920 --> 00:33:46.720
Inderjit Dhillon: When I optimize this, remember for

297
00:33:47.350 --> 00:33:56.420
Inderjit Dhillon: W. To be primal, optimal. It actually must also minimize the like around you.

298
00:33:57.560 --> 00:34:02.540
Inderjit Dhillon: So w the support. Vector so w the hyperplane.

299
00:34:02.670 --> 00:34:09.230
Inderjit Dhillon: the normal to the hyperfame. is actually a linear combination of the excise.

300
00:34:10.310 --> 00:34:23.739
Inderjit Dhillon: And remember that why I is plus one for class, one and minus one for class 2. So it's basically Alpha x i. Plus if Xi. Belongs to class one

301
00:34:23.900 --> 00:34:27.429
Inderjit Dhillon: and minus Alpha I. X. I.

302
00:34:27.469 --> 00:34:38.429
Inderjit Dhillon: If Xi. Belongs to class 2, of course we need to figure out exactly what the Alphas are. Now, if I take the gradient with respect to W. Not

303
00:34:39.350 --> 00:34:45.110
Inderjit Dhillon: let's see what happens. Well, there's nothing. The first term is not a function of W. Not.

304
00:34:45.320 --> 00:34:48.570
Inderjit Dhillon: and the only thing that's a function of W. Not is over here.

305
00:34:49.870 --> 00:34:52.230
Inderjit Dhillon: and so we get

306
00:34:52.940 --> 00:34:59.660
Inderjit Dhillon: summation of I equals one through m minus alpha I

307
00:35:00.160 --> 00:35:03.620
Inderjit Dhillon: y I equal to. Which means that

308
00:35:04.130 --> 00:35:17.900
Inderjit Dhillon: summation of Alpha I Y. I equals 0. I equals 1 3. Again, Something interesting and basically say is that if I look at the summation of Alpha I for the

309
00:35:18.420 --> 00:35:30.200
Inderjit Dhillon: this is like a balancing constraint. If I look at the summation of the Alpha I is for class, one that's equal to the summation of Alpha I for class 2 because y I equal to one for class, one.

310
00:35:30.340 --> 00:35:33.520
Inderjit Dhillon: and why I go to negative one for class 2.

311
00:35:34.260 --> 00:35:36.790
Inderjit Dhillon: Okay. So already we are starting to see some.

312
00:35:37.240 --> 00:35:39.830
Inderjit Dhillon: you know, some consequences

313
00:35:39.930 --> 00:35:45.020
Inderjit Dhillon: of what the or some outcomes of what the

314
00:35:45.190 --> 00:35:54.530
Inderjit Dhillon: the hyperplane actually looks like by going deeper into the problem. Now, if I look at the dual function.

315
00:35:56.440 --> 00:36:01.130
Inderjit Dhillon: remember, the dual function is the dual variable is only alpha.

316
00:36:02.020 --> 00:36:03.740
Inderjit Dhillon: and that is

317
00:36:03.770 --> 00:36:06.890
Inderjit Dhillon: info. Ww.

318
00:36:12.290 --> 00:36:13.050
Inderjit Dhillon: Okay.

319
00:36:13.190 --> 00:36:21.280
Inderjit Dhillon: So now I can my G of Alpha and I can start substituting my My L. My W's.

320
00:36:21.710 --> 00:36:24.730
Inderjit Dhillon: Okay. So Remember, this is L.

321
00:36:26.980 --> 00:36:30.080
Inderjit Dhillon: Okay, so it's in of

322
00:36:31.030 --> 00:36:33.000
Inderjit Dhillon: Ww: not.

323
00:36:34.770 --> 00:36:40.690
Inderjit Dhillon: and it's half of W. Square. But W. Is this form over here.

324
00:36:43.810 --> 00:36:46.960
Inderjit Dhillon: So I copy that. So it's basically half of

325
00:36:47.690 --> 00:36:55.340
Inderjit Dhillon: summation of I equals one through M. Alpha I Y. I X. I square.

326
00:36:56.440 --> 00:37:03.250
Inderjit Dhillon: So just bear with me. There's a little bit of algebra. Okay, so then I have plus summation again. I'm looking at this

327
00:37:03.590 --> 00:37:10.010
Inderjit Dhillon: this part over here I have summation of I equals one through n

328
00:37:10.380 --> 00:37:13.010
Inderjit Dhillon: I have Alpha I

329
00:37:13.920 --> 00:37:17.960
Inderjit Dhillon: I guess the alpha I multiply by one, so I can leave that by itself.

330
00:37:18.220 --> 00:37:25.690
Inderjit Dhillon: and then I have minus. I equals one through, and alpha I y i.

331
00:37:26.460 --> 00:37:28.040
Inderjit Dhillon: and then I have.

332
00:37:29.220 --> 00:37:40.620
Inderjit Dhillon: I can write it as I'm actually going to separate these 2 terms. I have X. I transpose W. Remember W. Transpose. Xi. Is the same as Xi. Transpose W. But W. Is of the form

333
00:37:41.280 --> 00:37:48.740
Inderjit Dhillon: summation of Let me just call it Alpha J. Yj. Next day, because I've already used an I,

334
00:37:52.420 --> 00:37:55.430
Inderjit Dhillon: and then I have the last term which is from here.

335
00:37:57.080 --> 00:38:03.290
Inderjit Dhillon: which is minus summation of I equals one through N.

336
00:38:03.980 --> 00:38:07.610
Inderjit Dhillon: Alpha I Y. I W. Not.

337
00:38:10.950 --> 00:38:14.680
Inderjit Dhillon: Okay. Now W. Naught is a constant.

338
00:38:16.430 --> 00:38:20.530
Inderjit Dhillon: and I have summation of Alpha Iy I, and from here

339
00:38:22.090 --> 00:38:26.110
Inderjit Dhillon: I can see that this means that this quantity

340
00:38:26.830 --> 00:38:28.310
Inderjit Dhillon: is equal to 0.

341
00:38:31.060 --> 00:38:37.510
Inderjit Dhillon: So let me simplify things a little bit again. My my apologies for there's a little bit of

342
00:38:38.090 --> 00:38:46.700
Inderjit Dhillon: algebra over here. So let me write the Alpha term first. I equals one to N alpha I minus.

343
00:38:46.820 --> 00:38:51.880
Inderjit Dhillon: and then what you will see is that this term is plus half.

344
00:38:52.770 --> 00:38:56.050
Inderjit Dhillon: and this is minus one of the same thing.

345
00:38:56.740 --> 00:38:59.860
Inderjit Dhillon: You know this is, you know, if you expand this out.

346
00:39:00.030 --> 00:39:07.230
Inderjit Dhillon: all the terms that you'll be get will be of the form alpha I alpha j y I yj x. I fans for the x J.

347
00:39:07.700 --> 00:39:20.840
Inderjit Dhillon: For you. When you combine these 2 you are just left with minus one half of that minus one half I equal to one through and J equals one through n

348
00:39:21.210 --> 00:39:25.500
Inderjit Dhillon: alpha I alpha j y I y day

349
00:39:26.080 --> 00:39:28.150
Inderjit Dhillon: excite, transpose x 2,

350
00:39:30.460 --> 00:39:33.900
Inderjit Dhillon: and remember that the as we the dual.

351
00:39:35.180 --> 00:39:40.620
Inderjit Dhillon: So in this case, as we do, all says that I want to maximize

352
00:39:42.280 --> 00:39:43.620
Inderjit Dhillon: G of L. For

353
00:39:43.760 --> 00:39:48.700
Inderjit Dhillon: overall Alpha. Okay, and what is G of Alpha? Let me just write it over here.

354
00:39:49.160 --> 00:39:58.380
Inderjit Dhillon: U of Alpha is what's given over here above summation of Alpha I I equals one minus one half

355
00:39:58.890 --> 00:40:04.210
Inderjit Dhillon: information. I equals one through and J equals one through, and

356
00:40:04.380 --> 00:40:13.750
Inderjit Dhillon: l 5 alpha J. Y. I. Y. Xi transpose x 2. So remember alpha I alpha of skaters

357
00:40:14.090 --> 00:40:17.560
Inderjit Dhillon: that are, you know we are maximizing over

358
00:40:17.820 --> 00:40:31.620
Inderjit Dhillon: y eyes are given plus one for class, one minus one for class 2. Okay. And so this is such that the dual variables are greater than equal to 0. So Alpha is greater than or equal to 0.

359
00:40:31.880 --> 00:40:34.450
Inderjit Dhillon: I equals one to N.

360
00:40:34.710 --> 00:40:40.070
Inderjit Dhillon: And then we have the balancing constraint that came from here. which is that

361
00:40:41.260 --> 00:40:44.470
Inderjit Dhillon: formation of I equals one through m

362
00:40:44.690 --> 00:40:48.110
Inderjit Dhillon: Alpha I y I a 1 0.

363
00:40:48.870 --> 00:40:54.230
Inderjit Dhillon: This is the S. 3 M. Do.

364
00:40:57.390 --> 00:41:04.870
Inderjit Dhillon: Okay, so quite a bit. I I've done quite a bit right. We started with the primal problem

365
00:41:06.770 --> 00:41:09.770
Inderjit Dhillon: which we derived. This was the primal problem

366
00:41:09.870 --> 00:41:12.080
Inderjit Dhillon: in the rectangles.

367
00:41:13.660 --> 00:41:15.660
Inderjit Dhillon: We looked at the dual

368
00:41:19.550 --> 00:41:30.540
Inderjit Dhillon: and the dual is basically maximizing something. and the variable has changed to the dual variable earlier. It was the primal variable W.

369
00:41:30.740 --> 00:41:33.600
Inderjit Dhillon: Now it is a dual variable alpha I

370
00:41:35.820 --> 00:41:43.990
Inderjit Dhillon: Any questions so far. I know that I've gone through quite a lot of algebra. but now we'll actually be

371
00:41:44.160 --> 00:41:47.450
Inderjit Dhillon: able to start getting some insight into what is going on.

372
00:41:49.780 --> 00:41:53.140
Inderjit Dhillon: If you have questions. Yeah, please go ahead.

373
00:41:53.270 --> 00:42:05.880
chitrank: So I think I did not understand. Like I I was not able to follow through all of that. So just for the Svm example. How did you come up with the dual function that you have written like?

374
00:42:07.710 --> 00:42:12.380
chitrank: Is it like by definition? Or did you design the dual function by yourself.

375
00:42:13.300 --> 00:42:20.730
Inderjit Dhillon: No, no, I didn't design my dual function. I basically just went through the for the machinery. Right? So so

376
00:42:21.570 --> 00:42:25.790
Inderjit Dhillon: okay, so let me kind of again repeat the different steps. Right? So

377
00:42:27.040 --> 00:42:32.290
Inderjit Dhillon: what we did over here right is to look at a general optimization problem.

378
00:42:33.810 --> 00:42:44.880
Inderjit Dhillon: if not minimize, if not X such that Fi. X is less than or equal to 0. H. I. X. Equals 0. And then we from this object called on the ground, Jim.

379
00:42:46.710 --> 00:42:52.260
Inderjit Dhillon: Okay. we form this thing called the Lagrange dual function.

380
00:42:53.510 --> 00:42:56.820
Inderjit Dhillon: and from that we derive the dual problem.

381
00:42:58.430 --> 00:43:03.250
Inderjit Dhillon: So the dual problem is G. Maximize ge of Lambda Mail

382
00:43:04.000 --> 00:43:09.050
Inderjit Dhillon: and G is infam over X of the lagrangian.

383
00:43:11.640 --> 00:43:16.600
Inderjit Dhillon: and dual feasibility is when the dual function dual variables are greater than or equal to 0.

384
00:43:17.870 --> 00:43:20.310
Inderjit Dhillon: I basically went through the same

385
00:43:22.110 --> 00:43:34.300
Inderjit Dhillon: machinery. right and in between. We also said that hey X Star. the primal optimal. must satisfy this for optimality.

386
00:43:35.390 --> 00:43:41.030
Inderjit Dhillon: which means that the gradient of F not x star. This term must be equal to 0.

387
00:43:42.490 --> 00:43:47.060
chitrank: So your you're most of

388
00:43:47.110 --> 00:43:51.790
Inderjit Dhillon: yeah, my cursor. I don't even know. Let me see. Is this the

389
00:43:56.870 --> 00:43:57.750
Inderjit Dhillon: Yeah.

390
00:43:59.190 --> 00:44:00.510
chitrank: Yeah.

391
00:44:00.550 --> 00:44:03.930
Inderjit Dhillon: Yeah, I guess I don't have a cursor with on the ipad.

392
00:44:03.950 --> 00:44:09.350
chitrank: I think they should be like a pointer or something on your writing card.

393
00:44:09.480 --> 00:44:18.190
Inderjit Dhillon: This is a software called notability. And I oh, yeah, okay, yeah, okay, thanks.

394
00:44:18.350 --> 00:44:21.450
Inderjit Dhillon: So okay, let me. Now, let's say like.

395
00:44:21.900 --> 00:44:25.990
Inderjit Dhillon: So we looked at a general problem.

396
00:44:26.690 --> 00:44:28.970
chitrank: Hmm. Convex: yeah.

397
00:44:29.120 --> 00:44:38.790
Inderjit Dhillon: With inequality constraints and equality constraints. This is a general problem, right? Our Svm problem was actually a little bit simpler. It had

398
00:44:39.960 --> 00:44:47.750
Inderjit Dhillon: the quadratic optimum. So this corresponds to F naught. Yeah, I understood that. Yeah.

399
00:44:48.840 --> 00:44:54.010
Inderjit Dhillon: And what we did is we went through like, you know, forming what's called the lagrangian.

400
00:44:54.680 --> 00:44:55.300
Hmm.

401
00:44:56.380 --> 00:45:01.690
Inderjit Dhillon: Got to.

402
00:45:02.060 --> 00:45:04.860
Inderjit Dhillon: So this is. And then that's how we got the dual problem.

403
00:45:06.340 --> 00:45:16.420
Inderjit Dhillon: So now what we did was we basically just follow that recipe. I did not. There is no kind of it's not like I did invented something new. I just followed that recipe.

404
00:45:17.630 --> 00:45:19.520
Inderjit Dhillon: So this is my primal problem.

405
00:45:21.160 --> 00:45:24.380
Inderjit Dhillon: I wrote it as an inequality constraint.

406
00:45:26.160 --> 00:45:28.010
Inderjit Dhillon: I form the lagrangian.

407
00:45:28.810 --> 00:45:29.500
Hmm.

408
00:45:30.020 --> 00:45:35.510
Inderjit Dhillon: I know what W. Starm, it says must satisfy that came about. You know, from the

409
00:45:36.450 --> 00:45:37.720
Inderjit Dhillon: from over here

410
00:45:40.440 --> 00:45:41.960
Inderjit Dhillon: right of the gradient.

411
00:45:45.140 --> 00:45:46.770
Inderjit Dhillon: I set them to 0.

412
00:45:47.020 --> 00:45:47.590
Hmm.

413
00:45:47.940 --> 00:45:50.150
Inderjit Dhillon: I got the form of W. Star.

414
00:45:51.060 --> 00:45:51.640
chitrank: Hmm.

415
00:45:52.160 --> 00:46:02.980
Inderjit Dhillon: And when I take the derivative with respect to gradient. With respect to W. Not. it gives me this balancing constraint. Okay. So then, I said, okay, the dual function

416
00:46:04.710 --> 00:46:06.390
Inderjit Dhillon: is the infam.

417
00:46:07.430 --> 00:46:09.700
chitrank: Hmm. Off the ground, Jim.

418
00:46:10.050 --> 00:46:11.990
Inderjit Dhillon: With respect to W. W. Not

419
00:46:12.990 --> 00:46:13.730
Hmm.

420
00:46:13.830 --> 00:46:18.890
Inderjit Dhillon: Okay. So I substituted W: not from here both.

421
00:46:18.930 --> 00:46:19.560
chitrank: Hmm.

422
00:46:19.700 --> 00:46:21.860
Inderjit Dhillon: Right? So this is

423
00:46:22.020 --> 00:46:24.340
Inderjit Dhillon: half of norm of W. Square.

424
00:46:24.410 --> 00:46:25.070
chitrank: Hmm.

425
00:46:25.430 --> 00:46:26.310
Inderjit Dhillon: Okay.

426
00:46:26.920 --> 00:46:29.800
chitrank: And then I

427
00:46:32.000 --> 00:46:37.450
Inderjit Dhillon: half of W. Square right? And then this is plus

428
00:46:37.960 --> 00:46:41.840
Inderjit Dhillon: alpha I right? I'm fine

429
00:46:43.080 --> 00:46:48.940
Inderjit Dhillon: an alpha I Y. I w transpose Xi. No, I Why, I

430
00:46:50.310 --> 00:46:55.410
chitrank: I can. I can calculate that later. So you are saying that.

431
00:46:55.940 --> 00:47:10.460
chitrank: like we found out the I think this solution or the form of the the dwell problem, and then say, so. You just found out the the dual version of Svm. That's what we are trying to do Exactly. So this is yeah, exactly. They are

432
00:47:10.770 --> 00:47:15.810
Inderjit Dhillon: in some sense equivalent, like I said right in this case.

433
00:47:16.320 --> 00:47:19.970
Inderjit Dhillon: and you can either solve the primal problem, or you can solve the dual problem.

434
00:47:21.220 --> 00:47:23.740
chitrank: Okay. So in the

435
00:47:23.860 --> 00:47:24.660
chitrank: and

436
00:47:24.720 --> 00:47:30.900
chitrank: and what is that? You are trying to conclude that the primal problem is difficult to or like is one

437
00:47:30.920 --> 00:47:39.710
chitrank: version of optimization easier to compute, or something

438
00:47:40.110 --> 00:47:41.810
Inderjit Dhillon: which I haven't done as yet.

439
00:47:41.930 --> 00:47:46.620
chitrank: I mean, in some sense we already have a few insights. Right? We have this insight

440
00:47:46.890 --> 00:47:51.090
Inderjit Dhillon: that the W. Star is actually of this form.

441
00:47:54.990 --> 00:48:06.800
Inderjit Dhillon: Okay, by just looking at this problem, you won't be able to say that W. Star is like this, right? If I just look at this problem, where is it? If I just look at this problem right?

442
00:48:06.960 --> 00:48:07.560
chitrank: Hmm.

443
00:48:08.270 --> 00:48:10.870
Inderjit Dhillon: I don't know what form W. Will take.

444
00:48:11.220 --> 00:48:18.210
Inderjit Dhillon: but it turns out that by doing this analysis I can say that W's will be of this form.

445
00:48:18.490 --> 00:48:30.920
Inderjit Dhillon: Okay, which means it will actually be a linear combination of all the training points and all the things on class. One will have the same sign. and the things in class 2 will have negative signs.

446
00:48:31.670 --> 00:48:34.870
Inderjit Dhillon: Okay, remember that the Alphas are greater than or equal to 0.

447
00:48:35.510 --> 00:48:38.070
chitrank: Okay, because there are dual variables.

448
00:48:38.630 --> 00:48:49.360
Inderjit Dhillon: Then we also have the balancing constraint, which means that the summation of Alpha I. For class one has to be the same as the summation of L 5 for Class 2.

449
00:48:49.490 --> 00:48:50.720
That's what this means.

450
00:48:51.290 --> 00:48:53.720
Inderjit Dhillon: right? Because the Y is a plus one and minus one.

451
00:48:53.790 --> 00:48:54.640
chitrank: Okay.

452
00:48:54.820 --> 00:49:06.270
Inderjit Dhillon: Okay. So we've already gotten some insights and we'll see that we'll get a little bit more insight into what the problem is. And then the another big advantage of the dual problem is

453
00:49:06.340 --> 00:49:19.610
Inderjit Dhillon: that the dual problem is actually expressed in term, and i'll come to that. It's actually it's basically, so so you know, Obviously, you know a little bit about of machine learning. But

454
00:49:19.840 --> 00:49:27.460
Inderjit Dhillon: let me repeat it for the rest of the class. Right is then you express it as inner products between Xi and X. J.

455
00:49:27.530 --> 00:49:42.270
Inderjit Dhillon: Right. So, instead of taking inner products between Xi and next day I can map xi. X to a higher dimensional feature space. and then invoke something called the kernel methods, and we can actually solve this problem in the

456
00:49:42.370 --> 00:49:43.590
Inderjit Dhillon: kernel space.

457
00:49:44.640 --> 00:49:51.290
Inderjit Dhillon: And i'll come to that. I like, explain what I mean, but that is another advantage of looking at the dual and

458
00:49:51.700 --> 00:50:00.220
Inderjit Dhillon: and most most of the software that is out there for Svm's actually ends up solving this, do a problem.

459
00:50:05.880 --> 00:50:13.770
Inderjit Dhillon: Okay? And then, of course, there are some cases where the primer might be easier, but it depends upon, for example, the number. So you can see over here

460
00:50:13.980 --> 00:50:24.990
Inderjit Dhillon: that you know this problem as this form, the other problem has the pro a form above. and in some cases it might be actually be able to solve this problem.

461
00:50:26.140 --> 00:50:29.020
Inderjit Dhillon: And now we'll come to the complementary slightness also.

462
00:50:30.280 --> 00:50:32.940
Inderjit Dhillon: Okay, so did you.

463
00:50:34.050 --> 00:50:36.450
Inderjit Dhillon: Did you have any more questions, or should I move on?

464
00:50:36.540 --> 00:50:50.630
chitrank: Yeah, I I understood somewhat. So yeah, and see that that's the reason I actually went through this whole machinery right? You might say. Hey, you know why is sorry about that yet. I think Professor is like.

465
00:50:51.000 --> 00:50:56.200
Inderjit Dhillon: you know, in the peak is just going into all this math. We don't need all this math.

466
00:50:58.240 --> 00:51:01.770
chitrank: but what it does is it gives you insights.

467
00:51:01.830 --> 00:51:07.660
Inderjit Dhillon: because without doing this math I cannot. Actually, you know, I can just show you the dual problem, but you will not know where it came from.

468
00:51:07.840 --> 00:51:12.220
chitrank: Okay, so that's the of, you know. I'm just trying to give you

469
00:51:12.390 --> 00:51:15.300
Inderjit Dhillon: like exactly how the dual problem comes.

470
00:51:16.410 --> 00:51:17.500
chitrank: Thanks so much.

471
00:51:17.520 --> 00:51:19.080
Inderjit Dhillon: of course, of course.

472
00:51:19.240 --> 00:51:32.050
Inderjit Dhillon: Okay. So now let's see what you know. There's something called. I I mentioned something called complimentary like this, and that'll actually give us another, you know, really kind of nice intuition behind what is happening.

473
00:51:32.990 --> 00:51:37.720
Inderjit Dhillon: Okay, so remember complementaries like this. So let me repeat it.

474
00:51:38.740 --> 00:51:40.970
Inderjit Dhillon: You'd say that at optimality

475
00:51:45.670 --> 00:51:47.140
Inderjit Dhillon: Alpha I.

476
00:51:47.360 --> 00:51:54.190
Inderjit Dhillon: Both the primal and the dual variables. Alpha I. W not must satisfy

477
00:51:59.310 --> 00:52:05.490
Inderjit Dhillon: this problem, which is that the Lagrange multiplier alpha I. Times.

478
00:52:05.690 --> 00:52:14.130
Inderjit Dhillon: F. Of W, which is one minus y. I. W transpose xi plus w not

479
00:52:15.700 --> 00:52:18.370
Inderjit Dhillon: equal to 0 for all.

480
00:52:24.090 --> 00:52:28.440
Inderjit Dhillon: So what does that mean? What is complementary slightness, complementary slightness is

481
00:52:29.120 --> 00:52:31.670
Inderjit Dhillon: that if Alpha I

482
00:52:31.880 --> 00:52:35.670
Inderjit Dhillon: is greater than 0, then

483
00:52:36.740 --> 00:52:44.200
Inderjit Dhillon: if I of W. Star, or if I of W at the optimal W, must be equal to 0, which means that

484
00:52:44.720 --> 00:52:51.480
Inderjit Dhillon: why I w transpose x I plus w not equals one.

485
00:52:52.650 --> 00:52:54.070
Inderjit Dhillon: and if

486
00:52:55.690 --> 00:53:02.200
Inderjit Dhillon: y I w transpose xi plus w naught is greater than one.

487
00:53:03.040 --> 00:53:06.610
Inderjit Dhillon: Them Alpha I must be 0.

488
00:53:08.690 --> 00:53:19.260
Inderjit Dhillon: Okay. And if you remember what my W. Is. W. Is equal to Alpha I Y. I I equals one to one.

489
00:53:21.000 --> 00:53:24.750
Inderjit Dhillon: What does this mean? Geometrically, geometrically? It means.

490
00:53:25.080 --> 00:53:34.770
Inderjit Dhillon: let me kind of you know, draw the same kind of picture that I had before. Well, I have You don't know. You know this is task 2.

491
00:53:36.710 --> 00:53:38.650
Inderjit Dhillon: This is class one.

492
00:53:43.020 --> 00:53:47.710
Inderjit Dhillon: and my optimal W. Is going to be something like this.

493
00:53:50.820 --> 00:53:56.930
Inderjit Dhillon: And if you think about it? The separating sorry, the supporting hyperplane was like this.

494
00:53:59.140 --> 00:54:00.730
Inderjit Dhillon: So this is the margin.

495
00:54:03.880 --> 00:54:04.780
Inderjit Dhillon: Okay.

496
00:54:06.310 --> 00:54:10.760
Inderjit Dhillon: So what happens is that all the points that are on this boundary

497
00:54:15.320 --> 00:54:23.370
Inderjit Dhillon: there, Alpha I is actually greater than 0 because they satisfy this with equality.

498
00:54:28.040 --> 00:54:29.720
Inderjit Dhillon: Similarly, these ones.

499
00:54:31.780 --> 00:54:33.940
Inderjit Dhillon: these points are on the boundary.

500
00:54:35.500 --> 00:54:37.540
Inderjit Dhillon: and let me just change the color

501
00:54:39.250 --> 00:54:42.310
Inderjit Dhillon: and all these other points right?

502
00:54:42.640 --> 00:54:52.580
Inderjit Dhillon: This point, this point, this point, this point. all the ones that are not at the boundary. What are their Alpha eyes?

503
00:54:57.460 --> 00:54:58.840
Inderjit Dhillon: Well, they do not

504
00:55:00.670 --> 00:55:02.620
Inderjit Dhillon: there in this case.

505
00:55:04.120 --> 00:55:09.460
Inderjit Dhillon: because they are not first against the hyperplane. So all these must be equal to 0.

506
00:55:13.470 --> 00:55:14.360
Inderjit Dhillon: Okay.

507
00:55:16.220 --> 00:55:22.360
Inderjit Dhillon: So that's why there is this name: support vector machines, these points.

508
00:55:23.130 --> 00:55:26.830
Inderjit Dhillon: or these vectors, because they are vectors. Right. Xi's.

509
00:55:27.200 --> 00:55:30.550
Inderjit Dhillon: They are what I call the support vectors

510
00:55:32.000 --> 00:55:35.040
Inderjit Dhillon: because they are supporting the hyperplane.

511
00:55:38.820 --> 00:55:44.780
Inderjit Dhillon: And since all the Alpha eyes of the non Support vectors are 0,

512
00:55:45.820 --> 00:55:50.840
Inderjit Dhillon: that means that this is a linear combination

513
00:55:55.480 --> 00:55:59.860
Inderjit Dhillon: of only the support vectors all the other.

514
00:56:04.130 --> 00:56:12.800
Inderjit Dhillon: But in some sense. if most of the points are not support vectors. that means there. Alphas are equal to 0.

515
00:56:14.140 --> 00:56:14.970
Inderjit Dhillon: Okay.

516
00:56:15.300 --> 00:56:17.980
Inderjit Dhillon: So then, W. Is.

517
00:56:18.750 --> 00:56:19.620
Inderjit Dhillon: you know.

518
00:56:19.860 --> 00:56:23.300
Inderjit Dhillon: is a sparse linear combination.

519
00:56:23.650 --> 00:56:34.960
Inderjit Dhillon: and it's a linear combination of only the support vectors. So from one. The data set only a few will be the support vectors. and only those alpha to be equal to

520
00:56:35.930 --> 00:56:37.620
Inderjit Dhillon: will be greater than 0.

521
00:56:41.160 --> 00:56:47.190
Inderjit Dhillon: And this is, you know. Remember, this was 2 divided by Norm of W. This is also called the margin.

522
00:56:50.460 --> 00:56:57.310
Inderjit Dhillon: so sometimes the 4 vectors are all support vector machines. They are also served as maximum margin type of thing.

523
00:57:00.230 --> 00:57:11.460
Inderjit Dhillon: Okay? Questions. Now, I know Chitlin had some questions, but to other some of the questions I still there. So I understood that the submission of Alpha why I should be 0. But

524
00:57:11.730 --> 00:57:13.970
chitrank: I do not understand. Why is that

525
00:57:14.210 --> 00:57:19.890
chitrank: so? You not summation for all data points Alpha Y, I should be 0 right because that.

526
00:57:20.270 --> 00:57:27.510
chitrank: But how do we come to this thing? How did we get this? Sorry in the dwell?

527
00:57:27.900 --> 00:57:29.420
chitrank: If you move a little up?

528
00:57:31.080 --> 00:57:35.740
chitrank: Yeah. So in the zoom dual problem you have written that Sorry.

529
00:57:36.600 --> 00:57:37.340
No.

530
00:57:37.720 --> 00:57:46.520
Inderjit Dhillon: yeah. I I I think I did understand this. So Can you explain why? So Remember that we did did the general part, and we talked about

531
00:57:46.990 --> 00:57:48.890
Inderjit Dhillon: complementary slightness.

532
00:57:54.490 --> 00:57:56.160
Inderjit Dhillon: So add the optimal.

533
00:57:56.910 --> 00:58:02.210
Inderjit Dhillon: the product of the lagrange multiplier times, the constraint must be equal to 0.

534
00:58:02.920 --> 00:58:05.940
Inderjit Dhillon: And we kind of saw that from what we did above.

535
00:58:07.400 --> 00:58:09.240
Inderjit Dhillon: when I did this derivation.

536
00:58:09.420 --> 00:58:10.950
chitrank: Okay, I I

537
00:58:11.070 --> 00:58:12.040
chitrank: Yeah.

538
00:58:12.080 --> 00:58:17.310
chitrank: I: yeah, yeah, I okay, yeah, I got that. Yeah. So what that means is

539
00:58:17.410 --> 00:58:22.380
Inderjit Dhillon: So so here is the rule of some right. If you have an if strong duality holds.

540
00:58:23.460 --> 00:58:27.160
Inderjit Dhillon: you have an inequality constraint. Okay.

541
00:58:28.370 --> 00:58:31.670
Inderjit Dhillon: if the inequality constraint is not

542
00:58:32.210 --> 00:58:33.700
Inderjit Dhillon: met with equality.

543
00:58:35.430 --> 00:58:39.090
Inderjit Dhillon: and at the optimal the Lagrange multiplier will be 0.

544
00:58:42.210 --> 00:58:45.960
Inderjit Dhillon: On the other hand, if the Lagrange multiplier is greater than 0,

545
00:58:47.840 --> 00:58:54.270
Inderjit Dhillon: then since Lambda, I. Star F. I. X star must be equal to 0. That means F. I. X star must be equal to 0.

546
00:58:54.470 --> 00:59:00.930
chitrank: That means that that inequality constraint must be met with. It must be equality, constraint.

547
00:59:00.960 --> 00:59:03.850
Inderjit Dhillon: Okay. So which means

548
00:59:04.480 --> 00:59:12.240
Inderjit Dhillon: which basically in the Svm problem, it translates to exactly this that what is in the

549
00:59:12.460 --> 00:59:18.700
Inderjit Dhillon: So it's actually pretty neat if you think about it right?

550
00:59:18.900 --> 00:59:19.520
Hmm.

551
00:59:19.720 --> 00:59:20.890
chitrank: It makes sense.

552
00:59:22.050 --> 00:59:24.980
Inderjit Dhillon: Okay, Anybody else who has questions.

553
00:59:35.540 --> 00:59:44.530
Inderjit Dhillon: Okay, If not, let's move on to the next part. So it's very rare when you have like an optimization problem that you.

554
00:59:44.700 --> 00:59:54.460
Inderjit Dhillon: you know, have the When you have a you know, slightly difficult classification problem, you know. Maybe there will be some outliers, maybe so on. Some of the

555
00:59:54.540 --> 00:59:58.860
Inderjit Dhillon: pluses will be here, and some of the All will be here right

556
00:59:59.000 --> 01:00:02.730
Inderjit Dhillon: So far we've only tackled the case where

557
01:00:03.060 --> 01:00:10.230
Inderjit Dhillon: you know things are nicely, linearly separable. But real problems are not nice and linear separable.

558
01:00:10.690 --> 01:00:18.030
Inderjit Dhillon: Okay, real problems might have this form right? There might be like, maybe we like some pluses over here.

559
01:00:18.410 --> 01:00:20.400
Inderjit Dhillon: But then there are some places here.

560
01:00:21.120 --> 01:00:25.360
Inderjit Dhillon: There may be some. This is the other class.

561
01:00:28.660 --> 01:00:30.490
Inderjit Dhillon: and there may be some. You also.

562
01:00:34.920 --> 01:00:35.750
Inderjit Dhillon: Okay

563
01:00:36.160 --> 01:00:41.200
Inderjit Dhillon: over here. If you look back to your you know

564
01:00:42.460 --> 01:00:48.270
Inderjit Dhillon: regression problems and so on, right? When we said that the training error is 0,

565
01:00:49.150 --> 01:00:52.040
Inderjit Dhillon: right? That's what this would achieve

566
01:00:52.440 --> 01:00:53.740
Inderjit Dhillon: over here.

567
01:00:54.920 --> 01:00:56.780
Inderjit Dhillon: If all the points

568
01:00:58.040 --> 01:00:59.840
Inderjit Dhillon: we're literally separable.

569
01:01:00.680 --> 01:01:04.120
Inderjit Dhillon: then all the points would be feasible.

570
01:01:07.160 --> 01:01:11.290
Inderjit Dhillon: Okay, all the the W's will lead to feasibility. right?

571
01:01:11.670 --> 01:01:16.700
Inderjit Dhillon: And basically there will be no error on any of the training points.

572
01:01:18.200 --> 01:01:20.260
Inderjit Dhillon: But that's generally not going to be possible.

573
01:01:21.750 --> 01:01:31.600
Inderjit Dhillon: So the question is, and I mentioned this as a drawback of the perceptron method that you know it can actually cycle through the cycles can actually be hard to detect.

574
01:01:33.440 --> 01:01:37.860
Inderjit Dhillon: So what happens in the Svm case? Well, so far it has the same problem.

575
01:01:38.860 --> 01:01:45.620
Inderjit Dhillon: It'll basically say that the problem is not feasible. That's it. And it'll just end, and you will not get a W. Out of it.

576
01:01:46.530 --> 01:01:52.540
Inderjit Dhillon: Okay. So now let's see how we can. you know, fairly in a straightforward manner.

577
01:01:52.680 --> 01:01:57.110
Inderjit Dhillon: kind of extend the Svm formulation

578
01:01:57.240 --> 01:02:02.410
Inderjit Dhillon: to accommodate these points that may not be perfectly classified.

579
01:02:02.960 --> 01:02:04.620
Inderjit Dhillon: Okay. So recall

580
01:02:08.630 --> 01:02:10.350
Inderjit Dhillon: the original

581
01:02:12.770 --> 01:02:14.020
Inderjit Dhillon: separable

582
01:02:16.780 --> 01:02:18.530
Inderjit Dhillon: Svm formulations.

583
01:02:26.240 --> 01:02:27.220
Inderjit Dhillon: Excuse me.

584
01:02:28.310 --> 01:02:41.680
Inderjit Dhillon: The original formulation was, you know I, Max, over w not remember we had this quantity called C such that y I w transpose x. I plus w naught

585
01:02:41.870 --> 01:02:44.950
Inderjit Dhillon: is greater than or equal to C. Times norm of.

586
01:02:46.860 --> 01:02:49.920
Inderjit Dhillon: So in order to

587
01:02:57.100 --> 01:02:58.360
Inderjit Dhillon: Hello.

588
01:03:02.400 --> 01:03:03.210
Inderjit Dhillon: you know.

589
01:03:05.060 --> 01:03:07.340
Inderjit Dhillon: non-separable data sets

590
01:03:14.330 --> 01:03:17.940
Inderjit Dhillon: what we will do is we will relax the above inequality.

591
01:03:24.280 --> 01:03:27.360
Inderjit Dhillon: and we relax it to have what's called a slack.

592
01:03:28.810 --> 01:03:31.510
Inderjit Dhillon: So we'll say that, hey? You know, you can actually

593
01:03:32.320 --> 01:03:35.990
Inderjit Dhillon: violate the linear require inequality a little bit.

594
01:03:36.680 --> 01:03:39.580
Inderjit Dhillon: So instead of this.

595
01:03:41.340 --> 01:03:43.020
Inderjit Dhillon: we'll say that. Okay.

596
01:03:44.320 --> 01:03:51.470
Inderjit Dhillon: what I will accommodate is W. Transpose Xi plus w not is greater than or equal to C

597
01:03:51.660 --> 01:03:54.720
Inderjit Dhillon: times one minus. I I

598
01:03:54.730 --> 01:04:00.530
Inderjit Dhillon: times norm of. and we'll say that the I is greater than equal to 0.

599
01:04:01.590 --> 01:04:14.810
Inderjit Dhillon: And if you think about what this means is this is basically saying that, hey? You know. i'm adding a slack of I. I am actually willing to live with some violation of

600
01:04:15.250 --> 01:04:21.610
Inderjit Dhillon: this being on one side of the hyperplane. Okay.

601
01:04:22.250 --> 01:04:32.830
Inderjit Dhillon: So if you then proceed as before, where we say, you know, fix of C. Times norm of W equals one. We get the non

602
01:04:33.990 --> 01:04:35.570
Inderjit Dhillon: linearly

603
01:04:36.790 --> 01:04:38.120
Inderjit Dhillon: approval.

604
01:04:40.100 --> 01:04:48.790
Inderjit Dhillon: Svm. Formulation. and so the primal will be minimize.

605
01:04:50.420 --> 01:04:52.810
Inderjit Dhillon: W. W. Not

606
01:04:53.450 --> 01:05:00.840
Inderjit Dhillon: sorry, so I have. Now I want to still limit this right. I don't want this to this to be unbounded.

607
01:05:01.420 --> 01:05:05.140
Inderjit Dhillon: So this kind of comes into the problem formulation.

608
01:05:05.380 --> 01:05:10.850
Inderjit Dhillon: So now, in addition to W, not being the primal variables, I also have size.

609
01:05:11.700 --> 01:05:15.600
Inderjit Dhillon: so I minimize off of W Square.

610
01:05:16.840 --> 01:05:28.080
Inderjit Dhillon: and my constraint then becomes Y. I times w transpose x I plus w Naught is greater than equal to one minus. I

611
01:05:29.270 --> 01:05:32.680
Inderjit Dhillon: I equals 1, 2 through n.

612
01:05:33.020 --> 01:05:38.700
Inderjit Dhillon: and then I have. Phi. I is greater than equal to 0 from here.

613
01:05:41.440 --> 01:05:43.260
Inderjit Dhillon: and then I want to limit

614
01:05:43.500 --> 01:05:50.340
Inderjit Dhillon: this I I so I can add a constraint which is summation of I I, for I equals one through M.

615
01:05:50.520 --> 01:05:53.620
Inderjit Dhillon: Is some less than equal to some constant

616
01:05:54.830 --> 01:05:59.820
Inderjit Dhillon: And think of that constant as kind of like a for regularization parameter

617
01:06:00.780 --> 01:06:06.930
Inderjit Dhillon: just like before you had a regularization parameter in you know regression.

618
01:06:06.960 --> 01:06:13.420
Inderjit Dhillon: classification, regression, least squares, regression, logistic regression that is used for classification.

619
01:06:14.780 --> 01:06:19.600
Inderjit Dhillon: Okay. So you can write it like this.

620
01:06:21.510 --> 01:06:26.140
Inderjit Dhillon: Many people. When you look at textbooks. They'll actually take this part

621
01:06:27.120 --> 01:06:37.120
Inderjit Dhillon: and put it over here with a parameter like the constant over here. So usually you will see this as the Svm primal

622
01:06:37.470 --> 01:06:41.530
Inderjit Dhillon: when you open textbooks. Okay, as minimum

623
01:06:41.690 --> 01:06:45.580
Inderjit Dhillon: minimize

624
01:06:46.630 --> 01:06:50.150
Inderjit Dhillon: for W. Square plus

625
01:06:50.720 --> 01:06:54.630
Inderjit Dhillon: regularization parameter. let me summation of

626
01:06:54.690 --> 01:07:03.960
Inderjit Dhillon: by. I I equals one from N. Okay. And then the constraints one minus. I I

627
01:07:03.970 --> 01:07:15.330
Inderjit Dhillon: i'm writing it as F. Of W. Is less than F. Of W. Is less than or equal to 0. Y. I. W. Transpose X I plus w not.

628
01:07:15.940 --> 01:07:21.040
Inderjit Dhillon: It's less than equal to 0. I equals 1, 2 through, and

629
01:07:22.560 --> 01:07:26.490
Inderjit Dhillon: and then I I is greater than or equal to 0.

630
01:07:29.220 --> 01:07:31.590
Inderjit Dhillon: Okay. So this is really the

631
01:07:32.970 --> 01:07:36.460
Inderjit Dhillon: Svm problem primal problem that you want to solve.

632
01:07:38.300 --> 01:07:40.900
Inderjit Dhillon: Okay, so that has the slack variables.

633
01:07:44.210 --> 01:07:45.140
Inderjit Dhillon: Hello.

634
01:07:45.450 --> 01:07:48.270
Inderjit Dhillon: Any questions on this.

635
01:07:49.600 --> 01:07:55.950
Inderjit Dhillon: So by doing this, now we can accommodate any data set. The data set does not have to be linearly separable.

636
01:08:04.620 --> 01:08:06.250
Inderjit Dhillon: So any questions

637
01:08:24.439 --> 01:08:28.130
Inderjit Dhillon: let me also write the dual problem for this without deriving it.

638
01:08:31.399 --> 01:08:33.140
Inderjit Dhillon: Okay. So so

639
01:08:34.370 --> 01:08:41.450
Inderjit Dhillon: if you feel brave enough. you can go through all the steps that we had before.

640
01:08:41.560 --> 01:08:45.899
Inderjit Dhillon: for the you know, incorporating the slack variables

641
01:08:46.340 --> 01:08:55.380
Inderjit Dhillon: and try to derive the dual. And this is what you will get. The Svm tool will be maximize over Alpha

642
01:08:57.210 --> 01:08:58.430
Inderjit Dhillon: actually understood that.

643
01:08:58.800 --> 01:09:01.540
Inderjit Dhillon: Maximize over over

644
01:09:02.180 --> 01:09:03.330
Inderjit Dhillon: how far.

645
01:09:04.760 --> 01:09:10.680
Inderjit Dhillon: and you can now see. You know with what I wrote before, how how it is different. I equals one through n

646
01:09:11.310 --> 01:09:20.170
Inderjit Dhillon: minus one half formation of I equals one through N. K. Equals one through. And

647
01:09:21.050 --> 01:09:26.090
Inderjit Dhillon: now I also J. Y. I Y. Day

648
01:09:27.250 --> 01:09:29.750
Inderjit Dhillon: xi transpose x

649
01:09:32.020 --> 01:09:33.130
Inderjit Dhillon: jade

650
01:09:35.420 --> 01:09:40.220
Inderjit Dhillon: 0 is less than or equal to Alpha I is less than or equal to gamma.

651
01:09:40.490 --> 01:09:42.140
Inderjit Dhillon: I plus one

652
01:09:42.290 --> 01:09:44.970
Inderjit Dhillon: to and

653
01:09:45.770 --> 01:09:50.189
Inderjit Dhillon: summation of alpha I Y: one equals 0.

654
01:09:50.729 --> 01:09:52.529
Inderjit Dhillon: My.

655
01:09:54.069 --> 01:09:56.590
Inderjit Dhillon: this becomes the dual.

656
01:10:00.120 --> 01:10:04.090
Inderjit Dhillon: Okay. it's basically the non

657
01:10:05.390 --> 01:10:08.470
Inderjit Dhillon: linearly. no problem

658
01:10:10.390 --> 01:10:11.600
Inderjit Dhillon: history. And do

659
01:10:14.210 --> 01:10:15.660
Inderjit Dhillon: then you leave

660
01:10:17.720 --> 01:10:18.950
Inderjit Dhillon: no problem.

661
01:10:23.240 --> 01:10:27.020
Inderjit Dhillon: Can you see how different this is from the earlier one that we have.

662
01:10:28.990 --> 01:10:30.190
Inderjit Dhillon: Thank you

663
01:10:32.810 --> 01:10:40.810
Inderjit Dhillon: earlier. Anybody wants to say whether it's the identical or slightly different. Just go back to.

664
01:10:41.550 --> 01:10:44.450
Inderjit Dhillon: I guess you guys don't have them out. So

665
01:10:46.550 --> 01:10:48.190
Inderjit Dhillon: if I

666
01:10:49.110 --> 01:10:50.810
look over here

667
01:10:55.780 --> 01:10:57.040
Inderjit Dhillon: this part

668
01:10:58.370 --> 01:11:03.630
Inderjit Dhillon: summation of Alpha I minus this, what is that? The same as here.

669
01:11:05.050 --> 01:11:09.290
Inderjit Dhillon: Summation of Alpha I minus. Oh, yeah, it's the same, exactly the same.

670
01:11:10.390 --> 01:11:13.710
Inderjit Dhillon: And I have summation of Alpha I,

671
01:11:15.640 --> 01:11:17.400
Inderjit Dhillon: and that will also.

672
01:11:19.820 --> 01:11:22.740
Inderjit Dhillon: Here at Alpha I is greater than 0,

673
01:11:24.810 --> 01:11:27.690
Inderjit Dhillon: and here I have. Oh.

674
01:11:28.730 --> 01:11:35.620
Inderjit Dhillon: i'll fly is greater than they grow. But then they have these additional constraints that the Alpha eyes are less than equal to normal.

675
01:11:36.410 --> 01:11:42.280
Inderjit Dhillon: So you can see that this is regularization. That's the only difference, you know, between

676
01:11:42.730 --> 01:11:50.330
Inderjit Dhillon: the Svm. Dual non dimly separable and the linearly separable. You should always use this.

677
01:11:51.490 --> 01:11:53.420
There's no need to use the other one.

678
01:11:53.980 --> 01:11:59.800
Inderjit Dhillon: because the other one might just come back and say there's no feasible solution. If your data is not.

679
01:12:05.540 --> 01:12:07.240
Inderjit Dhillon: i'll give you an exercise.

680
01:12:08.770 --> 01:12:11.380
Inderjit Dhillon: I will leave it to you. Exercise

681
01:12:12.760 --> 01:12:14.020
Inderjit Dhillon: Derive

682
01:12:15.670 --> 01:12:16.990
Inderjit Dhillon: the above

683
01:12:21.880 --> 01:12:23.660
Inderjit Dhillon: from the recipe that people.

684
01:12:25.130 --> 01:12:29.850
Professor Kenya explain, like intuitively what Alpha I is

685
01:12:29.950 --> 01:12:32.980
chitrank: into the like? What's its geometric interpretation?

686
01:12:33.240 --> 01:12:37.320
Inderjit Dhillon: Yeah, I mean, if you think about it, Alpha a. Is the Lagrange multiplier right

687
01:12:38.150 --> 01:12:40.400
Inderjit Dhillon: of the it

688
01:12:41.450 --> 01:12:43.450
Inderjit Dhillon: linear inequality constraint.

689
01:12:45.380 --> 01:12:48.920
Inderjit Dhillon: And that's basically saying that that training Point Xi.

690
01:12:50.140 --> 01:12:52.480
Inderjit Dhillon: How far is it from the hyperplane?

691
01:12:54.960 --> 01:12:58.210
Inderjit Dhillon: So if Alpha I

692
01:13:02.470 --> 01:13:04.240
Inderjit Dhillon: is a support vector

693
01:13:05.750 --> 01:13:07.580
Inderjit Dhillon: and Alpha is greater than 0.

694
01:13:10.970 --> 01:13:18.930
Inderjit Dhillon: If Alpha, if it's not on the supporting hyperplane. then also I equal to 0,

695
01:13:22.740 --> 01:13:25.180
so it's like a threshold right.

696
01:13:25.880 --> 01:13:30.840
Inderjit Dhillon: But now, when when in the nonlinearly separable case, if you have points which

697
01:13:30.860 --> 01:13:34.040
Inderjit Dhillon: kind of cross over right here.

698
01:13:34.310 --> 01:13:36.930
Inderjit Dhillon: you know, the hyperplane will be something like this.

699
01:13:38.620 --> 01:13:45.410
Inderjit Dhillon: Okay. So you will have supporting hyperplane supporting hyperplane. These points also

700
01:13:48.250 --> 01:13:50.080
Inderjit Dhillon: will be support vectors.

701
01:13:55.160 --> 01:13:59.470
chitrank: Oh, you meant the misclassified points.

702
01:14:02.340 --> 01:14:04.800
Inderjit Dhillon: Okay, the other ones are the ones which are 0,

703
01:14:06.300 --> 01:14:13.080
Inderjit Dhillon: even the misclassified points. I have to think a little bit, because you know they the side. I should also be

704
01:14:13.260 --> 01:14:15.310
Inderjit Dhillon: it also reads the same quantity.

705
01:14:16.750 --> 01:14:19.550
Inderjit Dhillon: So now I forget what the okay here.

706
01:14:25.600 --> 01:14:29.300
Inderjit Dhillon: So the misclassified points actually might be

707
01:14:30.500 --> 01:14:32.910
Inderjit Dhillon: might be capped at Gamma.

708
01:14:35.960 --> 01:14:41.700
Inderjit Dhillon: Yeah, I think that's true. The misclassified points are going to be. Have the value, Gamma.

709
01:14:49.830 --> 01:14:56.040
Inderjit Dhillon: I haven't derived that. So that's something that you can figure out right through the complementary slightness conditions.

710
01:14:58.060 --> 01:14:58.880
chitrank: Okay.

711
01:15:02.800 --> 01:15:05.260
chitrank: Okay, yeah. I got that. Yeah.

712
01:15:06.750 --> 01:15:09.890
Inderjit Dhillon: Okay, Any questions.

713
01:15:22.930 --> 01:15:25.500
Inderjit Dhillon: And just so that, you know, i'm your

714
01:15:25.980 --> 01:15:34.520
Inderjit Dhillon: I can. Actually, i'm not actually going to go through the full lectures. But

715
01:15:35.470 --> 01:15:36.540
Inderjit Dhillon: let's see.

716
01:15:39.490 --> 01:15:40.140
Inderjit Dhillon: Hmm.

717
01:15:43.290 --> 01:15:45.220
Inderjit Dhillon: Let me show you something

718
01:15:55.250 --> 01:15:57.220
Inderjit Dhillon: something.

719
01:16:10.190 --> 01:16:23.670
Inderjit Dhillon: Oh, I was just going to show you something, but maybe i'll just show you next time. So you know I do have notes where I've gone through all the details of deriving the dual problem for the nonlinearly separable. Svm.

720
01:16:25.550 --> 01:16:32.170
Inderjit Dhillon: But I don't think i'm going to cover it, and over that detail. I I'll just leave it as an exercise to you.

721
01:16:35.000 --> 01:16:39.050
Inderjit Dhillon: So that comes. That's the end of today's lecture.

722
01:16:40.360 --> 01:16:55.570
Inderjit Dhillon: and I think that's where we'll include Svm: I'll talk a little bit about, you know kernel methods, since we are at a point where we can discuss kernel methods a little bit. but I know that there have been several kind of discussions about projects in this class.

723
01:16:55.740 --> 01:17:02.780
Inderjit Dhillon: I know you have homework 2 that you're doing, but next week we'll now start discussing projects in quite a bit of detail.

724
01:17:03.030 --> 01:17:12.440
Inderjit Dhillon: So we have prepared kind of a list of projects that you could do. I know that some of you might have. You know, projects in mind.

725
01:17:12.700 --> 01:17:15.280
Inderjit Dhillon: Projects will be in.

726
01:17:15.590 --> 01:17:23.910
Inderjit Dhillon: you know there'll be no project that will be done just by one person. The minimum size of a project is going to be, You know, 2 people

727
01:17:24.800 --> 01:17:36.100
Inderjit Dhillon: ideal size of the team that does. The project will be 3 right, and if you are 2 or maximum 4, we'll you'll have to talk to us to.

728
01:17:36.320 --> 01:17:45.920
Inderjit Dhillon: you know, in some sense get approval. So what I'm thinking is that we'll spend a lecture next week talking about

729
01:17:46.240 --> 01:17:50.030
Inderjit Dhillon: possible projects that you can do both inulation. I

730
01:17:50.310 --> 01:18:05.270
Inderjit Dhillon: possibly on Monday. and then you'll have those, and you can either select from one of the projects we have, or you can write up your own description of a project that you will do obviously along, you know, having a team of 2 to

731
01:18:05.360 --> 01:18:09.960
Inderjit Dhillon: and like, I said, I did it 3. Any questions about that.

732
01:18:14.740 --> 01:18:26.420
Inderjit Dhillon: Okay, no problem. So we'll we'll. We'll discuss this more on on Monday, and I think Meanwhile, what we will do is Neil and I will kind of send over. You. Send over to you the list of projects.

733
01:18:26.440 --> 01:18:29.960
Inderjit Dhillon: and you can pre read them before Monday's class.

734
01:18:31.110 --> 01:18:41.210
Inderjit Dhillon: Okay, Thank you for i'm sorry I haven't niggled today. So thanks for listening, and I will see you on Monday. Thank you, bye.

WEBVTT

1
00:01:44.920 --> 00:01:45.860
Nilesh Gupta: Hey, everyone.

2
00:01:46.240 --> 00:01:51.680
Nilesh Gupta: So today is more of a discussion class rather than usual lecture class.

3
00:01:51.860 --> 00:01:54.680
Nilesh Gupta: So in this class

4
00:01:54.770 --> 00:02:00.180
Nilesh Gupta: me and Professor in that it will go over some of those suggestions for your final project.

5
00:02:00.460 --> 00:02:07.160
Nilesh Gupta: and we'll try to explain like, Go over briefly over, like all of the suggestions that we have.

6
00:02:07.550 --> 00:02:13.270
Nilesh Gupta: And yeah and like, if you have any questions, or if you want

7
00:02:13.370 --> 00:02:22.110
Nilesh Gupta: like, want to discuss something regarding that, you can do that and this one. And so

8
00:02:22.330 --> 00:02:27.300
Nilesh Gupta: yeah, keep in mind that this is just suggestion. You are

9
00:02:27.460 --> 00:02:40.500
Nilesh Gupta: free free to and actually like, encourage to choose your own project and maybe decide something of your own. But if you, if you are struggling to find anything, or if you want to

10
00:02:41.630 --> 00:02:47.340
Nilesh Gupta: kind of, have some ideas from us like These are some of the suggestions

11
00:02:47.480 --> 00:02:53.280
Nilesh Gupta: I sent the this document, you know, I think, last Friday.

12
00:02:53.500 --> 00:02:59.640
Nilesh Gupta: so I hope, like you have at least on a high level gone over the topics that it has.

13
00:03:00.140 --> 00:03:01.140
Nilesh Gupta: So

14
00:03:01.560 --> 00:03:08.750
Nilesh Gupta: Yeah. But before I begin that discussion there were a few things I wanted to mention regarding the homework.

15
00:03:08.970 --> 00:03:10.310
Nilesh Gupta: this homework, too.

16
00:03:10.660 --> 00:03:12.910
Nilesh Gupta: They were like

17
00:03:13.030 --> 00:03:20.480
Nilesh Gupta: quite a lot of people in the office are today, and unfortunately I wouldn't. I wasn't able to attend to all of them.

18
00:03:20.590 --> 00:03:31.050
Nilesh Gupta: But i'll try to post some guidelines regarding to debugging specifically the question. 3 of this homework to

19
00:03:31.070 --> 00:03:34.180
Nilesh Gupta: which I I ask you to implement

20
00:03:34.240 --> 00:03:40.380
Nilesh Gupta: the neural network and back propagation algorithm and numbering so.

21
00:03:40.680 --> 00:03:50.040
Nilesh Gupta: And i'll try to post some guidelines on piazza regarding to that and other than that, like few common things that I've seen people making mistakes is

22
00:03:50.080 --> 00:04:01.310
Nilesh Gupta: the wrong softmax implementation there like for the whole matrix. the softmax denominator is the summed over

23
00:04:01.380 --> 00:04:14.430
Nilesh Gupta: all the elements in the matrix. So keep in mind, like the softmax, is defined only for one data point, like for a data point. If you have a labels, then in the denominator. You want to sum only over those

24
00:04:14.580 --> 00:04:23.430
Nilesh Gupta: scores of the and labels for that particular data point. So you don't want to sum over all of your a 2 matrix.

25
00:04:23.620 --> 00:04:26.520
Nilesh Gupta: So that's one common mistake.

26
00:04:26.890 --> 00:04:44.880
Nilesh Gupta: And one suggestion was that when you are writing back propagation, try to go in the reverse order of the equations that you have written that like First try to compute the gradient for Z 2. Then with with the helper, If you have greeting for Z 2 you can compute created for

27
00:04:45.310 --> 00:04:53.730
Nilesh Gupta: W, 2 and b 2. Then compute the gradient for a one from a one you can go to z one, and from Z when you can go to W one and B one.

28
00:04:54.260 --> 00:05:04.070
Nilesh Gupta: So that's how you want to do the back propagation thing, and one more common issue was the the gradients for bias

29
00:05:04.160 --> 00:05:12.500
Nilesh Gupta: for the biased terms, the sentence in the equation. The bias term is getting duplicated for

30
00:05:13.200 --> 00:05:20.840
Nilesh Gupta: all of the data points that you have in your Mini batch. And then it's being added to all of the data points.

31
00:05:20.860 --> 00:05:28.500
Nilesh Gupta: So when you are computing the gradient, you want to do the same operation. They want to like sum over all the data points

32
00:05:28.730 --> 00:05:30.420
Nilesh Gupta: of the gradient feedback.

33
00:05:31.760 --> 00:05:34.510
Nilesh Gupta: So yeah.

34
00:05:35.110 --> 00:05:47.760
Nilesh Gupta: and feel like I think it's there in the assignment problem statement, too. But, like I'm just repeating it, feel free to use torch, to rebuild your code

35
00:05:47.990 --> 00:05:52.580
Nilesh Gupta: like you can try to initialize the same weights

36
00:05:52.700 --> 00:06:08.600
Nilesh Gupta: from bite or in by torch, and then to use the by torches backward, auto deaf engine to compute the gradients, and then compare the regions to the gradients that you're receiving in your numb by code.

37
00:06:09.050 --> 00:06:09.900
Nilesh Gupta: And

38
00:06:10.130 --> 00:06:14.330
Nilesh Gupta: yeah, and you can just check in gradients for each other variables.

39
00:06:14.800 --> 00:06:19.070
Nilesh Gupta: and that will be quite helpful if you are starting to debug your code.

40
00:06:20.250 --> 00:06:22.790
Nilesh Gupta: So yeah, that's

41
00:06:23.150 --> 00:06:29.340
Nilesh Gupta: the some of the general instructions for debugging this homework. And specifically the

42
00:06:29.760 --> 00:06:42.750
Nilesh Gupta: the question 3 of this number. One more thing I wanted to add is like when you are implementing question 3, and you are initializing your weights in random values from 0 to one

43
00:06:42.780 --> 00:06:49.040
Nilesh Gupta: you are expected to get. Oh. worse than your Pi touch code.

44
00:06:49.540 --> 00:06:58.110
Nilesh Gupta: So it's it's kind of normal to get worse results like it. It. The your network, should it's still should be training.

45
00:06:58.230 --> 00:07:06.830
Nilesh Gupta: But you won't get the same results that you were getting with by touch. But when you change the initializations as mentioned in the last part of the

46
00:07:06.980 --> 00:07:09.460
Nilesh Gupta: of course, like I think it's in question 4.

47
00:07:09.520 --> 00:07:19.130
Nilesh Gupta: When you change the initialization, then you will, with some of the initializations, you will see that, like it's, the network is starts getting

48
00:07:19.190 --> 00:07:28.600
Nilesh Gupta: the accuracy which is similar to the accuracy that you are getting with 5 torch. which kind of highlights the importance of neural network initializations.

49
00:07:28.700 --> 00:07:31.670
Nilesh Gupta: And you can also read more about that on Internet.

50
00:07:31.960 --> 00:07:39.640
Nilesh Gupta: But yeah, I just wanted to let you know that it's kind of expected that when you are initializing weights between 0 to one

51
00:07:39.930 --> 00:07:49.170
Nilesh Gupta: randomly in your number, your implementation, then the loss will be higher, and the accuracy that will be you will get will be lower than your.

52
00:07:50.380 --> 00:07:51.360
Nilesh Gupta: So

53
00:07:52.610 --> 00:07:54.850
Nilesh Gupta: that's it. Regarding the homework.

54
00:07:56.210 --> 00:08:07.070
Nilesh Gupta: If there are any like general questions about that maybe I can take a few minutes to discuss that now. Otherwise i'll just go over the talking that I said

55
00:08:09.770 --> 00:08:16.550
Chitrank Gupta: so in Aly. Why, why do you say that it is expected to get poorer results on? Is it because of like the

56
00:08:16.900 --> 00:08:20.280
the exponential thing that

57
00:08:21.210 --> 00:08:22.530
Chitrank Gupta: so

58
00:08:24.270 --> 00:08:25.130
Chitrank Gupta: in the

59
00:08:25.190 --> 00:08:29.480
Nilesh Gupta: neural network training is very brittle with respect to the initialization

60
00:08:29.570 --> 00:08:36.340
Nilesh Gupta: that you are choosing like if you are, if your initializations are not right, the outputs like.

61
00:08:36.470 --> 00:08:37.970
Chitrank Gupta: since the like

62
00:08:38.140 --> 00:08:40.659
Nilesh Gupta: it, there is a lot of possibility that, like

63
00:08:40.740 --> 00:08:48.920
Nilesh Gupta: you will get dead neurons, or like the activation that you are getting is saturated like, for example, like when we are aing sigmoid, and if you are

64
00:08:49.030 --> 00:08:54.180
Nilesh Gupta: sigmoid, is always in one region, or like it's in the saturated region.

65
00:08:54.300 --> 00:08:56.040
Nilesh Gupta: The gradient that you will get

66
00:08:56.070 --> 00:09:01.440
Nilesh Gupta: will won't be enough to like, take the weights into the right direction.

67
00:09:01.770 --> 00:09:08.640
Chitrank Gupta: so like that's why. But my question was, it like, what does that pi toch? Does different.

68
00:09:08.970 --> 00:09:12.820
Nilesh Gupta: So by touch, like there is something called initialization

69
00:09:12.920 --> 00:09:21.340
Nilesh Gupta: and timing timing initialization, I think, by torch, by default, uses climbing initialization.

70
00:09:21.420 --> 00:09:29.250
Nilesh Gupta: which is something very similar to the initialization that I have given. We have given in the question, for

71
00:09:29.510 --> 00:09:35.520
Chitrank Gupta: I want that we have some implementation. No, no, no, there is no implementation. Difference is that the

72
00:09:35.540 --> 00:09:38.720
Nilesh Gupta: kind of the range of the random values that it chooses

73
00:09:38.880 --> 00:09:40.540
Nilesh Gupta: that makes difference.

74
00:09:40.730 --> 00:09:42.630
Chitrank Gupta: Thank you. Yeah.

75
00:09:43.680 --> 00:09:48.800
Nilesh Gupta: And after this homework is done, I think i'll just post a nice blog

76
00:09:48.830 --> 00:09:54.620
Nilesh Gupta: about these initializations. It's interactive, too, so you can play around that.

77
00:09:54.820 --> 00:10:00.260
Nilesh Gupta: And if you Google, it you'll find it. So you don't have to wait. But yeah, I encourage all of you to

78
00:10:01.330 --> 00:10:04.490
Nilesh Gupta: take a look at your network initialization

79
00:10:04.870 --> 00:10:06.590
Nilesh Gupta: and how it affects the train.

80
00:10:07.950 --> 00:10:08.760
Call it

81
00:10:13.300 --> 00:10:22.330
Nilesh Gupta: So you Han posted one question. The bias terms so usually the bias terms don't matter that much.

82
00:10:22.830 --> 00:10:30.460
Nilesh Gupta: I mean the initializations of the bias terms don't matter that much, since they usually are not multiplied by the input.

83
00:10:30.610 --> 00:10:35.840
Nilesh Gupta: So like it's, it's fine to keep the bias terms initialized to be 0 or a small value.

84
00:10:36.100 --> 00:10:42.200
Nilesh Gupta: but the the weights of your neutral networks are kind. The initialization of them is kind of important.

85
00:10:46.600 --> 00:10:47.290
Nilesh Gupta: Okay.

86
00:10:48.880 --> 00:10:51.840
Nilesh Gupta: So let me share my screen.

87
00:11:06.430 --> 00:11:07.210
Nilesh Gupta: Yes.

88
00:11:07.360 --> 00:11:10.930
Nilesh Gupta: So the first project

89
00:11:11.320 --> 00:11:21.440
Nilesh Gupta: that you can potentially do is exploring sparsity in neural networks. So, as we all know, the internal networks are computationally expensive.

90
00:11:21.530 --> 00:11:28.050
Nilesh Gupta: and they require a large amount of computational resources in general like if you go to bigger networks.

91
00:11:28.170 --> 00:11:33.250
Nilesh Gupta: So one approach to address this challenge is to introduce.

92
00:11:33.810 --> 00:11:40.740
Nilesh Gupta: and by sparsity what I mean is that, like for a particular input, only a small number of neurons are active

93
00:11:40.900 --> 00:11:42.420
Nilesh Gupta: as a

94
00:11:43.630 --> 00:11:52.620
Nilesh Gupta: like, instead of like having dense connections with the like. The input or hidden layers like there are sparse connections so.

95
00:11:52.700 --> 00:12:03.580
Nilesh Gupta: and the in in your neural network. And this is sparsity, can reduce the computational cost and the memory requirements of your neural network.

96
00:12:04.630 --> 00:12:12.490
Nilesh Gupta: Sorry without, and sometimes without sacrificing performance. So with this project you can explore

97
00:12:12.500 --> 00:12:19.020
Nilesh Gupta: so like, how can you make neural networks past and like study, its impact on the networks performance.

98
00:12:19.130 --> 00:12:29.220
Nilesh Gupta: So now a few suggestions that you can now take is like you can start with an overview of what all different techniques can be used for inducing capacity in your network.

99
00:12:29.270 --> 00:12:44.640
Nilesh Gupta: And some of the techniques are pruning that you prune some of the weights that are not being used. Then, like there is parsity. Regularization like you can include regularization in your loss, which might promote sparsity

100
00:12:44.700 --> 00:12:54.220
Nilesh Gupta: and network connections, and then, after later, like you can tune the dead neurons from your network. And then there is something called quantization

101
00:12:54.440 --> 00:12:59.340
Nilesh Gupta: which is kind of compressing your neural network into

102
00:12:59.490 --> 00:13:01.860
Nilesh Gupta: with the

103
00:13:02.050 --> 00:13:09.950
Nilesh Gupta: a small at a smaller resolution. So these are some of the ideas that exist.

104
00:13:10.060 --> 00:13:14.600
Nilesh Gupta: There are multiple others, and you can implement some of these techniques

105
00:13:14.670 --> 00:13:17.710
Nilesh Gupta: compare their performance with traditional dense networks.

106
00:13:17.870 --> 00:13:29.660
Nilesh Gupta: and for this particular thing you can experiment on popular data sets, such as amnesty for 10, or if you want to go bigger, you can also, like, take a look at image net.

107
00:13:29.690 --> 00:13:39.160
Nilesh Gupta: These are all image classification data sets, but they are pretty well studied, and you might find a good amount of literature of studying space that works on these data sets.

108
00:13:39.280 --> 00:13:51.830
Nilesh Gupta: So yeah, it will be kind of easy to start with links, data sets. and me. as I said, like you can compare the performance of these past networks to traditional dense networks

109
00:13:51.850 --> 00:13:55.860
Nilesh Gupta: in terms of accuracy, memory, requirements, and inference time.

110
00:13:56.330 --> 00:14:08.150
Nilesh Gupta: You can also, like analyze this positive patterns of networks, and visualize the learned features, to gain insight into how it is affecting networks ability to learn

111
00:14:08.460 --> 00:14:20.830
Nilesh Gupta: you can Sometimes these is sparse connections that you get, or also like to, more interpretable than the dense connections. So you can examine your sparse neural network on that access as well.

112
00:14:21.310 --> 00:14:25.150
Nilesh Gupta: So these are some of the references that I have put.

113
00:14:25.250 --> 00:14:27.240
Nilesh Gupta: The first is

114
00:14:28.570 --> 00:14:30.010
Nilesh Gupta: tutorial.

115
00:14:30.280 --> 00:14:34.300
Nilesh Gupta: It's a pretty long tutorial. You don't have to go over all of it.

116
00:14:34.390 --> 00:14:37.280
Nilesh Gupta: But you can read like the first section.

117
00:14:39.710 --> 00:14:43.350
Nilesh Gupta: And yeah, maybe like, read this 1 point, one section.

118
00:14:43.560 --> 00:14:51.910
Nilesh Gupta: and whatever you feel in there, why is relevant here to gain insight into what techniques exist.

119
00:14:52.020 --> 00:14:59.660
Nilesh Gupta: and there's a I see my tutorial as well. which I believe, like has videos

120
00:14:59.810 --> 00:15:05.450
Nilesh Gupta: so like that will be more compressed version of nice overview of sparsity

121
00:15:05.620 --> 00:15:16.070
Nilesh Gupta: in neural networks. And then there is a pluning tutorial on python. So this also you can take a look if you What do we do this project?

122
00:15:16.610 --> 00:15:20.740
Nilesh Gupta: So any questions that anyone has on

123
00:15:20.750 --> 00:15:21.320
Yes.

124
00:15:21.810 --> 00:15:23.070
Nilesh Gupta: particular topic.

125
00:15:33.100 --> 00:15:33.920
Nilesh Gupta: Okay.

126
00:15:33.950 --> 00:15:37.450
Nilesh Gupta: so i'll go to the next topic

127
00:15:37.510 --> 00:15:44.790
Nilesh Gupta: that I had. We have in this document. The next one is exploring the kind of

128
00:15:45.240 --> 00:16:01.360
Nilesh Gupta: the training pipeline for long tailed classification. So what do I mean by long tail like long tail Data sets are pretty common in real world scenarios and a by long tail, I mean, like the majority of the data, belongs to a few classes.

129
00:16:02.050 --> 00:16:05.080
Nilesh Gupta: while the remaining classes have relatively few samples.

130
00:16:05.190 --> 00:16:20.480
Nilesh Gupta: So, for example, these data sets that I mentioned earlier this Msc. For 10 and image net. They are pretty well formulated. Data sets, like all classes, will have equal amount of training examples, an equal amount of test examples.

131
00:16:20.480 --> 00:16:26.290
Nilesh Gupta: But in many a real level scenario this is not the case when you go on collecting data. Like

132
00:16:26.440 --> 00:16:42.720
Nilesh Gupta: many times there, there'll be some classes which will have lot of data and some classes which will have very few amount of data, maybe only like one or 2 or 5 examples for that particular class. So in in when you, when you encounter these kind of scenarios.

133
00:16:42.880 --> 00:16:46.020
Nilesh Gupta: the imbalance in the label data, distribution.

134
00:16:46.050 --> 00:17:00.700
Nilesh Gupta: like poses, quite some significant challenges to the algorithms that we machine learning algorithms that you use like on the majority class. It will do pretty good, and it will try, like kind of overfit on that class, but

135
00:17:01.100 --> 00:17:15.420
Nilesh Gupta: on the minority classes it will it? It won't, be able to fit that well, and it will lead to poor performance. So with this project you can explore some techniques for addressing this class and balance problem.

136
00:17:15.460 --> 00:17:21.890
Nilesh Gupta: Some of these techniques are over sampling and oversampling of you when you are training like, instead of

137
00:17:22.140 --> 00:17:34.000
Nilesh Gupta: creating each sample as equal to the other sample, you can maybe like over sample. Few of this examples more, or you can do under sampling, which is the opposite of that.

138
00:17:34.050 --> 00:17:44.800
Nilesh Gupta: And then there is also. like you can use some synthetic data to kind of guide your training in the right direction.

139
00:17:44.960 --> 00:17:51.780
Nilesh Gupta: and the loss Formulation also plays some role in deciding

140
00:17:52.710 --> 00:17:58.080
Nilesh Gupta: this a. And in addressing this label imbalance problem.

141
00:17:58.100 --> 00:18:04.110
Nilesh Gupta: And there are others methods like you can think, ensemble of. You know things. You can.

142
00:18:04.140 --> 00:18:13.090
Nilesh Gupta: The treat the minority classes differently, and majority class differently, and try to take ensemble of those 2 methods at the test time.

143
00:18:13.360 --> 00:18:14.370
Nilesh Gupta: So

144
00:18:14.770 --> 00:18:27.350
Nilesh Gupta: some of the long tail classification data sets that you can use all these data. That's like. The first is a ux 4 K. Date, as it it's a text classification data set with 4,000 labels.

145
00:18:27.870 --> 00:18:34.050
Nilesh Gupta: and it it should be manageable to run experiments on this particular data set.

146
00:18:34.320 --> 00:18:44.340
Nilesh Gupta: The other data set is the image net Lt. Which stands for image at Long T, which is the same imaginary data, but it has been artificially made

147
00:18:44.420 --> 00:18:50.590
Nilesh Gupta: to behave like a long tail data set. So some classes will have more training point in. Some

148
00:18:50.950 --> 00:18:56.280
Nilesh Gupta: classes will have less training points. and if you want to go bigger, like, there is another

149
00:18:56.440 --> 00:19:04.780
Nilesh Gupta: recommendation data set which is Amazon title, 131 K it has around 130,000 labels.

150
00:19:05.090 --> 00:19:14.050
Nilesh Gupta: And and naturally, when you have these, these many labels like, there will be a a long tail Distribution of training signals that you will have

151
00:19:14.130 --> 00:19:15.280
Nilesh Gupta: but

152
00:19:15.650 --> 00:19:17.720
Nilesh Gupta: every class. So

153
00:19:19.640 --> 00:19:22.850
Nilesh Gupta: these are some of the resources.

154
00:19:23.230 --> 00:19:34.650
Nilesh Gupta: The first one is also a survey paper on deep, Long tail learning. This is specifically long tail learning in terms of deep learning.

155
00:19:34.990 --> 00:19:42.270
Nilesh Gupta: but I think it, it gets a nice overview of what all the ideas exist in this literature.

156
00:19:42.450 --> 00:19:44.310
Nilesh Gupta: There is some.

157
00:19:47.660 --> 00:19:51.860
Nilesh Gupta: This is oh. a popular paper, which I mean, which

158
00:19:52.030 --> 00:19:56.410
Nilesh Gupta: addresses this long tail recognition problem. And I think it was the first one

159
00:19:56.480 --> 00:19:59.180
to introduce this image, net Lt: data set.

160
00:19:59.550 --> 00:20:06.630
Nilesh Gupta: So here you can take a look at that as well. And then there is this Github page which

161
00:20:06.800 --> 00:20:13.170
Nilesh Gupta: less some of the recent papers that have come on long tail learning which you can

162
00:20:13.410 --> 00:20:14.250
Nilesh Gupta: take a look

163
00:20:14.870 --> 00:20:19.740
Nilesh Gupta: if you want to start doing this project on this topic.

164
00:20:21.300 --> 00:20:23.080
Nilesh Gupta: So any questions on this.

165
00:20:35.120 --> 00:20:35.780
Nilesh Gupta: Okay?

166
00:20:37.160 --> 00:20:42.130
Chitrank Gupta: So in at least, I just have a general

167
00:20:42.420 --> 00:20:47.900
Chitrank Gupta: like. I think most of us are not working on any of these

168
00:20:48.060 --> 00:21:04.740
Chitrank Gupta: like topics already on like it would be starting a fresh on these topics. So the so. So the thing is that I think there will be a lot to read. So I just wanted to like.

169
00:21:04.860 --> 00:21:08.770
Chitrank Gupta: ask, as a general advice, like how to find a

170
00:21:09.750 --> 00:21:17.760
Chitrank Gupta: I would say, like a good like how to narrow down to like one like a very specific research problem

171
00:21:18.080 --> 00:21:23.310
Chitrank Gupta: without, you know, going through a lot of reading material. if that's possible.

172
00:21:23.760 --> 00:21:44.290
Nilesh Gupta: I see so one thing that I wanted to mention in. No, I think in Professor in the G. It will clarify more on that, you know later. But the expectation of these projects is to kind of pick something and explore the things which already exist in this literature, and maybe

173
00:21:44.440 --> 00:21:49.350
Nilesh Gupta: run some experiments and show your like just to reproduce those results.

174
00:21:49.560 --> 00:22:04.130
Nilesh Gupta: You are kind of like. obviously like, if you do like, try to do research in this direction. that will be definitely like a is encouraged, and it's always good.

175
00:22:04.460 --> 00:22:11.330
Nilesh Gupta: But the expectation is not that, like you will be doing a thorough research in this direction. Because, like, if we are doing that.

176
00:22:11.360 --> 00:22:18.160
Nilesh Gupta: research like you will have to at some point like, read a lot about these things. and

177
00:22:18.350 --> 00:22:24.630
Nilesh Gupta: try to just have a good understanding of, like what the problem demands, and

178
00:22:24.800 --> 00:22:28.970
Nilesh Gupta: what all things people have tried, and what works and what not.

179
00:22:29.450 --> 00:22:39.610
Nilesh Gupta: So it's. It's just about like exposing yourself to some of the interesting problems that already exist, and what all things have been done.

180
00:22:40.210 --> 00:22:47.490
Nilesh Gupta: But in general, like usually like, If I want to read upon something like, I just

181
00:22:48.150 --> 00:22:49.150
Nilesh Gupta: todd

182
00:22:49.380 --> 00:23:00.110
Nilesh Gupta: googling and like. There are many times, if it's a popular topic. There are people who have organized the literature somewhere in a nice way.

183
00:23:00.380 --> 00:23:05.870
Nilesh Gupta: and maybe you can start from that, and there might be some blog post that you can read.

184
00:23:07.790 --> 00:23:11.040
Nilesh Gupta: Yeah. So that's my suggestion, I think.

185
00:23:14.130 --> 00:23:14.770
Nilesh Gupta: Yeah.

186
00:23:18.590 --> 00:23:22.180
Nilesh Gupta: Okay. So coming up to the next topic.

187
00:23:22.540 --> 00:23:25.150
Nilesh Gupta: So I know we haven't

188
00:23:25.450 --> 00:23:30.390
Nilesh Gupta: discussed sequence to sequence models in class, yet we will go over them.

189
00:23:30.490 --> 00:23:37.730
Nilesh Gupta: But I just wanted to give a very brief overview of what sequential sequence models are. And

190
00:23:37.760 --> 00:23:46.480
Nilesh Gupta: then in this project you can explore some of the applications of this modules. which can be pretty cool. So in general a sequence to sequence model

191
00:23:47.140 --> 00:23:52.080
Nilesh Gupta: takes a sequence as the input and tries to output another sequence as the output.

192
00:23:52.190 --> 00:23:57.330
Nilesh Gupta: For example, it can be translation that you take your English sentence.

193
00:23:57.410 --> 00:24:02.110
Nilesh Gupta: which can be like, given as a sequence as input and you can try to output

194
00:24:02.210 --> 00:24:09.680
Nilesh Gupta: like the translated French version of that sentence as output, and that will be another sequence. So they they are pretty

195
00:24:09.700 --> 00:24:13.290
Nilesh Gupta: generic in terms of what they can do. Like

196
00:24:13.440 --> 00:24:18.900
Nilesh Gupta: like they take a sequence as input and generate a sequence as output.

197
00:24:20.000 --> 00:24:26.520
Nilesh Gupta: And these there are a neural network architecture called transform, which is

198
00:24:26.710 --> 00:24:38.470
Nilesh Gupta: very popular, and one of the kind of the main underlying engines, behind everything that's been emerging from deep learning, which is exciting like

199
00:24:38.480 --> 00:24:41.660
Nilesh Gupta: If you look at Chat Gpt. Dali, everything like

200
00:24:41.750 --> 00:24:50.330
Nilesh Gupta: it uses some transformer in one way or another. So these transform our various sequence of sequence. Models are pretty powerful.

201
00:24:50.500 --> 00:24:53.780
Nilesh Gupta: and they have very cool applications.

202
00:24:54.120 --> 00:25:02.890
Nilesh Gupta: So with, if you want like, you can. With this project you can try to come up with some innovative way of using these powerful models.

203
00:25:02.980 --> 00:25:05.330
Nilesh Gupta: One example that

204
00:25:05.460 --> 00:25:06.230
Nilesh Gupta: so

205
00:25:06.720 --> 00:25:17.120
Nilesh Gupta: people have already done this, but like it's, it's still interesting that you can try to make a chess engine using a sequence. It's model, because if you look at chess like

206
00:25:17.190 --> 00:25:20.120
Nilesh Gupta: there's a history of sequence of events.

207
00:25:20.390 --> 00:25:24.160
Nilesh Gupta: And then, like based on that history, you want to predict the next move.

208
00:25:24.400 --> 00:25:37.640
Nilesh Gupta: So it's. It's very similar to how these sequences models are trained that they are taking a input as a sequence, sequential input. And they are predict trying to predict the next item in that sequence that will get.

209
00:25:37.980 --> 00:25:38.920
Nilesh Gupta: So

210
00:25:39.180 --> 00:25:49.220
Nilesh Gupta: that's so. One example that you can consider another example is so so music, generation, or song generation. You can also consider a sequence

211
00:25:49.470 --> 00:25:51.200
Nilesh Gupta: formulation.

212
00:25:51.470 --> 00:26:09.180
Nilesh Gupta: So if if you want to, you can try to look up these things and these things, and it will be like pretty interesting working with them. And you you won't have to go too much into detail or transformers, and like what are the exact things that make them work, because.

213
00:26:09.180 --> 00:26:14.020
Nilesh Gupta: like there are very neat libraries in pi touch and and every framework right now.

214
00:26:14.070 --> 00:26:21.180
Nilesh Gupta: which provide a very good high, level abstraction of these transformers, and you can start using right away.

215
00:26:21.260 --> 00:26:29.960
Nilesh Gupta: So some of the resources that I have listed is this: If you want to get started with how to what transformers are, and what they do.

216
00:26:30.030 --> 00:26:38.620
Nilesh Gupta: and what is the mechanism behind their architecture? You can look at this illustrated transformer blog, which is

217
00:26:39.180 --> 00:26:41.280
Nilesh Gupta: pretty nice. There is a

218
00:26:41.660 --> 00:26:44.790
Nilesh Gupta: encoder decoder blog on encoder decoder

219
00:26:44.800 --> 00:26:49.170
Nilesh Gupta: models, using transformer on the hugging face.

220
00:26:49.500 --> 00:26:58.720
Nilesh Gupta: And then this is like. if you don't know, andre like he's a very

221
00:26:59.120 --> 00:27:01.330
Nilesh Gupta: popular camel person.

222
00:27:01.380 --> 00:27:04.730
Nilesh Gupta: and he has a very nice video on this, like

223
00:27:04.870 --> 00:27:12.450
Nilesh Gupta: where he tries to build, Gpt. The underlying model behind Chat G from scratch and code.

224
00:27:12.550 --> 00:27:21.550
Nilesh Gupta: So you can take a look, and I think you'll be by the end of this video will be very comfortable working with transformers and in general sequence of sequence models.

225
00:27:21.710 --> 00:27:28.120
Nilesh Gupta: And there is some blog on how you can generate using using music, using sequence sequence for each.

226
00:27:28.250 --> 00:27:32.690
Nilesh Gupta: So you can take a look if you want to specifically do this.

227
00:27:33.570 --> 00:27:35.310
Nilesh Gupta: So yeah.

228
00:27:35.400 --> 00:27:37.070
Nilesh Gupta: any questions on this.

229
00:27:50.380 --> 00:27:52.570
Nilesh Gupta: on it. So

230
00:27:54.170 --> 00:27:57.810
Nilesh Gupta: the next topic is learning with multimodal input data.

231
00:27:58.440 --> 00:28:11.770
Nilesh Gupta: So what do we mean by multimodal. It's multi modal refers. Multimodal learning in general refers to the process of learning from multiple sources of data, like including visual auditory and textual inputs.

232
00:28:11.930 --> 00:28:12.900
Nilesh Gupta: So

233
00:28:13.070 --> 00:28:20.820
Nilesh Gupta: right now, like even the in homework to the problem that we have is, we just have a image of

234
00:28:20.920 --> 00:28:31.240
Nilesh Gupta: image, and like we are trying to predict the class of the object in the image. But in general, like humans, perceive multiple modalities of input

235
00:28:31.290 --> 00:28:43.930
Nilesh Gupta: like you have image. Maybe some sound to. You may have like some text for that image, too, and based on all of these information, you may maybe you want to do this classification.

236
00:28:44.510 --> 00:28:45.540
Nilesh Gupta: So

237
00:28:47.830 --> 00:28:54.720
Nilesh Gupta: you so like how to like. Train your machine learning model, which can incorporate all of

238
00:28:54.770 --> 00:29:01.280
Nilesh Gupta: different modalities of input that that is mainly referred to as multimodal learning.

239
00:29:01.350 --> 00:29:07.910
Nilesh Gupta: and the potential of multimodal learning lies in stability to extract more comprehensive and

240
00:29:07.950 --> 00:29:21.970
Nilesh Gupta: robust representations of ventured leading to improve performance. And in this project you can explore what are the best practices for learning with multimodal input data.

241
00:29:22.200 --> 00:29:30.760
Nilesh Gupta: and how to leverage different modalities, and how to combine information from different modalities into a single model

242
00:29:31.480 --> 00:29:37.420
Nilesh Gupta: which which might ultimately lead to enhancement in the performance of the model.

243
00:29:37.740 --> 00:29:38.710
Nilesh Gupta: So

244
00:29:38.910 --> 00:29:48.000
Nilesh Gupta: and this one potential benchmark or data set that you can consider. If you are doing this, they this caltech ucsd birds, 200 eighty-

245
00:29:48.300 --> 00:29:57.050
Nilesh Gupta: like it contains it it's a style classification data set where you want to classify images of birds into, I guess, their Pcs.

246
00:29:57.250 --> 00:30:03.230
Nilesh Gupta: But it has images of the birds along with their text descriptions.

247
00:30:03.420 --> 00:30:04.340
Nilesh Gupta: So

248
00:30:04.490 --> 00:30:11.100
Nilesh Gupta: so the challenge here is like how to kind of take both of these, and put into account the images

249
00:30:11.190 --> 00:30:24.260
Nilesh Gupta: of the birds that you're seeing, and the text that description that you're seeing both of them together to classify deploy. So here I have. So some of the links

250
00:30:24.380 --> 00:30:29.330
Nilesh Gupta: for the techniques, or like these are kind of compile links

251
00:30:29.520 --> 00:30:32.840
Nilesh Gupta: and subway papers on multimodal machine learning.

252
00:30:34.730 --> 00:30:39.430
Nilesh Gupta: Oh, yeah, for example, this is a nice.

253
00:30:39.690 --> 00:30:46.630
Nilesh Gupta: And then there is this Github page which compiles some of the recent

254
00:30:47.380 --> 00:30:54.930
Nilesh Gupta: papers that exist in this particular domain. Some of the service papers you want to go into detail.

255
00:30:56.150 --> 00:31:00.700
Nilesh Gupta: Yeah. So any questions on this one.

256
00:31:13.830 --> 00:31:16.170
Nilesh Gupta: All right. So

257
00:31:16.900 --> 00:31:25.710
Nilesh Gupta: going over to the next stopping. We have briefly discussed this in one of the post lecture discussions

258
00:31:25.920 --> 00:31:39.990
Nilesh Gupta: about double descent behavior. So in in this particular project you you are. You can just kind of try to review the existing literature on double descent.

259
00:31:40.180 --> 00:31:46.590
Nilesh Gupta: and, like empirically verify this behavior with it. It is happening in

260
00:31:46.960 --> 00:31:55.690
Nilesh Gupta: neural networks on the standard benchmarks are not so. What is double descent.

261
00:31:55.760 --> 00:32:01.060
Nilesh Gupta: so double descent is a recent phenomena, where the

262
00:32:01.170 --> 00:32:11.100
Nilesh Gupta: which I, in which, like the test error of a neural network first increases and then decreases as the number of parameters in the network increases.

263
00:32:11.360 --> 00:32:25.580
Nilesh Gupta: So basically the standard and machine learning the standard kind of folk knowledge is that at your optimal, if you have optimal number of hyper parameters parameters in your network.

264
00:32:25.830 --> 00:32:31.060
Nilesh Gupta: Till then, like your loss, will your validation loss will decrease.

265
00:32:31.270 --> 00:32:37.910
Nilesh Gupta: but like after that you will start over fitting, and it will will start increasing. But like

266
00:32:38.040 --> 00:32:53.300
Nilesh Gupta: what this phenomenon with this phenomena, and what's happening is like, it does increase after a certain point, and after that, when it starts decreasing again, so like as you go, bigger and bigger like you are actually getting a more generalized model.

267
00:32:53.570 --> 00:32:56.060
Nilesh Gupta: which is pretty surprising, and

268
00:32:56.570 --> 00:33:02.270
Nilesh Gupta: that's still. I don't think it's it's that well understood yet.

269
00:33:02.560 --> 00:33:07.950
Nilesh Gupta: But with this project. What you can do is you can try to empirically verify

270
00:33:08.010 --> 00:33:08.860
Nilesh Gupta: with that

271
00:33:08.930 --> 00:33:16.100
Nilesh Gupta: this phenomenon is happening or not, and you can study some theoretical foundations regarding this

272
00:33:16.240 --> 00:33:20.490
Nilesh Gupta: and implement and train neural networks of waiting sizes

273
00:33:20.560 --> 00:33:25.770
Nilesh Gupta: on standard classification data sets, such as C 5. And

274
00:33:26.300 --> 00:33:27.310
Nilesh Gupta: and

275
00:33:27.410 --> 00:33:32.610
Nilesh Gupta: we can then analyze the behavior of the tester as a function of number of parameters.

276
00:33:32.750 --> 00:33:43.790
Nilesh Gupta: And yeah, investigate this existence of the double decent phenomena. You can evaluate the impact of some of the regularization techniques

277
00:33:43.810 --> 00:33:53.040
Nilesh Gupta: that does drop out and be on this behavior and contrast it against the there is a in the existing literature. So

278
00:33:53.610 --> 00:33:55.650
Nilesh Gupta: I have. For

279
00:33:56.020 --> 00:34:03.010
Nilesh Gupta: same. There is this nice blog on open AI,

280
00:34:03.350 --> 00:34:08.060
Nilesh Gupta: which and discusses this double the same phenomena which is

281
00:34:08.159 --> 00:34:14.920
Nilesh Gupta: like. First, the loss decreases, then it increases, and after a certain point installed decrease working so

282
00:34:15.469 --> 00:34:20.560
Nilesh Gupta: you can, if you want to, can get started on this by reading this.

283
00:34:20.590 --> 00:34:24.210
Nilesh Gupta: and there are some other couple of documents.

284
00:34:24.370 --> 00:34:29.630
Nilesh Gupta: This is the paper I think vision discusses this phenomena in more detail.

285
00:34:30.650 --> 00:34:33.190
Nilesh Gupta: Yeah. And I think.

286
00:34:36.190 --> 00:34:41.159
Nilesh Gupta: Yeah, this is a blog. Oh. I'm: discussing this.

287
00:34:42.320 --> 00:34:43.630
Nilesh Gupta: So.

288
00:34:43.670 --> 00:34:45.560
Nilesh Gupta: yeah, any questions on this.

289
00:34:57.970 --> 00:35:01.910
Nilesh Gupta: Okay. So the next topic.

290
00:35:02.220 --> 00:35:06.510
Nilesh Gupta: it's slightly more specific, and

291
00:35:06.840 --> 00:35:22.940
Nilesh Gupta: it's specific, because I kind of work in this particular area is that in many real world scenarios you have a big output space, and by output space I mean just the number of classes in the data set that you have

292
00:35:23.060 --> 00:35:24.920
Nilesh Gupta: So right

293
00:35:24.940 --> 00:35:31.960
Nilesh Gupta: typically in most of the standard benchmark data rates, you have the order of 1,000 or 1,000 classes.

294
00:35:32.110 --> 00:35:43.800
Nilesh Gupta: But in many cases like. You can also have very large number of classes that maybe a 1 million, or even in a 1 billionif you are looking at the industry scale.

295
00:35:44.150 --> 00:35:50.190
Nilesh Gupta: So when you have these large amount of like a very big output, space.

296
00:35:50.410 --> 00:36:16.940
Nilesh Gupta: common approach is to do your classification efficiently is to arrange your labels in a 3 based hierarchy. So like, if you let's say you have a 1 million labels, you will arrange your labels into a previous hierarchy, where, at the first level, level. Maybe you have 4 things, and at the second level you have 64 things. At the third Level you have maybe

297
00:36:17.220 --> 00:36:22.840
Nilesh Gupta: some 1,000 things, and then like at the leaf level. You have all the 1 million lasers.

298
00:36:22.940 --> 00:36:29.950
Nilesh Gupta: So, and what you do is you try to search this label space, using

299
00:36:30.060 --> 00:36:48.210
Nilesh Gupta: some hierarchical search procedure, like one of the most popular thing is doing Beam, Search that you first try to come up with the best candidates in the First Level. Then you explore those candidates, and then you do the same thing on your second level, and then you go downwards in the hierarchy.

300
00:36:48.910 --> 00:36:50.020
Nilesh Gupta: So

301
00:36:50.080 --> 00:36:56.480
Nilesh Gupta: this the hierarchical search procedure relies heavily on how you are building this label tree

302
00:36:56.640 --> 00:37:04.940
Nilesh Gupta: like if your label tree is of a good quality, like your search procedure or the machine learning model that you will learn using this

303
00:37:05.340 --> 00:37:19.730
Nilesh Gupta: free structure will be better, and if it's not good like that, will your performance will be worse. So with this project, you can explore different ways of constructing this label tree and compare their performances and trade offs.

304
00:37:19.940 --> 00:37:28.220
Nilesh Gupta: So most of the of the existing methods kind of define their tree structure by doing a hierarchical clustering

305
00:37:28.800 --> 00:37:31.320
Nilesh Gupta: of data. So they will

306
00:37:31.420 --> 00:37:39.840
Nilesh Gupta: first cluster the data into 2 particular clusters. Then they will further divide those 2 clusters into

307
00:37:39.850 --> 00:37:51.340
Nilesh Gupta: each cluster into 2 sub clusters, and, like they will have a hierarchically do that until they have created like a free structure which is efficient enough for the application.

308
00:37:51.630 --> 00:38:10.930
Nilesh Gupta: So but one potential direction that you can try exploring with this project is, you can, instead of like doing this hard hierarchical gaming space partitioning of this output space. You can try a graph based partitioning, and by grab based, I just mean that

309
00:38:10.930 --> 00:38:25.440
Nilesh Gupta: you are still dividing that labels into a tree based hierarchy. But the partitioning you are doing, maybe like. instead of just using something like K-means you are using more involved techniques like metes

310
00:38:25.570 --> 00:38:34.040
Nilesh Gupta: as one of the craft partitioning algorithms that you can use. So yeah, and you can experiment

311
00:38:35.670 --> 00:38:47.540
Nilesh Gupta: on some of some of the extreme classification data sets, such as you like. 4 K. And Amazon, 6, 70, K. These are your standard classification data sets, but they have

312
00:38:47.650 --> 00:38:52.500
Nilesh Gupta: big output space for your Lex Doesn't have a very big output space, but it's still

313
00:38:52.550 --> 00:38:54.490
Nilesh Gupta: like

314
00:38:54.960 --> 00:39:00.500
Nilesh Gupta: large enough to do some prototypes. But Amazon 6, 70 K. Is the

315
00:39:00.680 --> 00:39:04.680
Nilesh Gupta: like. It has around half a 1 million labels. And

316
00:39:05.870 --> 00:39:10.130
Nilesh Gupta: yeah, it's one of the medium-sized data that we have

317
00:39:10.520 --> 00:39:13.900
Nilesh Gupta: for for for exploring these kind of problems.

318
00:39:14.500 --> 00:39:18.450
Nilesh Gupta: So there are some references here. The first one

319
00:39:18.630 --> 00:39:22.040
Nilesh Gupta: is the the cost paper. It's

320
00:39:22.400 --> 00:39:34.750
Nilesh Gupta: it. It is kind. You can consider it as a survey paper. It kind of discusses this whole hierarchical based next. Now i'm doing extreme classification and lot of detail.

321
00:39:35.120 --> 00:39:36.540
Nilesh Gupta: and

322
00:39:36.710 --> 00:39:42.660
Nilesh Gupta: it is a pretty simple word it, too. So I think you can go over it if you want to proceed.

323
00:39:42.760 --> 00:39:47.330
Nilesh Gupta: and then I've given some more references on this particular thing. Basically

324
00:39:47.400 --> 00:39:53.080
Nilesh Gupta: on my test, the graph partitioning algorithm you can take a look at this Github Page.

325
00:39:54.090 --> 00:40:01.190
Nilesh Gupta: So, yeah, any questions on this? So one

326
00:40:01.380 --> 00:40:13.060
Chitrank Gupta: like, have anyone tried like like so in the oh, so first question is that, how did you get this hierarchy? Did you like that? The metadata was already provided, or like

327
00:40:13.460 --> 00:40:18.020
Nilesh Gupta: so usually? What you do is for a label. You

328
00:40:19.350 --> 00:40:31.990
Nilesh Gupta: somehow try to be before even training. You somehow try to construct some label representation. What you can. Once a very simple label representation that you can get is you just take average of all the

329
00:40:32.270 --> 00:40:51.780
Nilesh Gupta: training points it has, and you represent it by its average. And once you have a representation, you can like do k-means clustering to divide it into 2 parts. And then you further do kings clustering to divide the the 2 parts that you have into another 2 sub parts. Okay.

330
00:40:51.950 --> 00:41:03.320
Nilesh Gupta: So that is one way of doing it. So in this particular you can explore whether, like what are the other ways and like whether you can get something, can do something better than that or not.

331
00:41:03.960 --> 00:41:10.270
Chitrank Gupta: and like, have anyone tried. So i'm also wondering that

332
00:41:10.590 --> 00:41:13.550
Chitrank Gupta: like does any label have

333
00:41:13.740 --> 00:41:19.260
Chitrank Gupta: like more than one Parents like like. It's not a strict hierarchical.

334
00:41:19.630 --> 00:41:31.100
Nilesh Gupta: So that's also been recently tried, and there so work called Elias, and which. like, instead of

335
00:41:31.730 --> 00:41:36.880
Nilesh Gupta: arranging this label space into yeah, into a

336
00:41:37.430 --> 00:41:41.770
Nilesh Gupta: 3 like it arranges the labels into kind of a directed, a cyclic graph.

337
00:41:41.800 --> 00:41:52.190
Nilesh Gupta: so they can label, can have multiple parents, and it can be released from the root node to the label, a leaf node by multiple parts.

338
00:41:52.450 --> 00:41:56.110
Nilesh Gupta: So there are like challenges related to that as well.

339
00:41:56.410 --> 00:42:02.840
Nilesh Gupta: But yeah, like it, it's this: there is like, not too much, but like some exploration that

340
00:42:02.920 --> 00:42:05.900
Chitrank Gupta: that

341
00:42:06.320 --> 00:42:20.540
Chitrank Gupta: th that if like, if you are using the user like, if you're using the X 3 presentations to do this clusting, then the the long tail problem will also hold. Because.

342
00:42:21.520 --> 00:42:24.360
Chitrank Gupta: yeah. is that true all?

343
00:42:25.070 --> 00:42:27.400
Nilesh Gupta: So what do you mean by long?

344
00:42:27.450 --> 00:42:30.430
Chitrank Gupta: I mean like

345
00:42:30.470 --> 00:42:39.880
Chitrank Gupta: so i'll like, I mean, if there are like 1 million labels. I don't think right so in opinion, if they like. For example, 1 million labels, then I don't think

346
00:42:39.970 --> 00:42:45.970
Chitrank Gupta: the like. What i'm thinking is that maybe only 100 or one of those labels will have like.

347
00:42:46.230 --> 00:42:55.280
Nilesh Gupta: like we'll take away. You chunk of the training day. I'm: on. Yeah, yeah, that that's true. So anytime, when you have a large output space, this long tail problem will

348
00:42:55.330 --> 00:42:56.250
Nilesh Gupta: exist.

349
00:42:56.450 --> 00:43:07.700
Nilesh Gupta: So if you just want to focus more on the long tail like, how how do we handle a data set with? Has this long tail in label distribution.

350
00:43:07.850 --> 00:43:10.410
Chitrank Gupta: so that I was also just wondering

351
00:43:10.420 --> 00:43:17.440
Chitrank Gupta: like is like this problem is should also like within this problem before that aggregated. If we are

352
00:43:17.530 --> 00:43:24.370
Chitrank Gupta: like using the use, user let's like using the X 3 presentations to do the clustering. Because

353
00:43:24.560 --> 00:43:32.080
Nilesh Gupta: yeah, so in a way, it's it's possible that when you are taking the average and representing the label by its average

354
00:43:32.190 --> 00:43:34.810
Nilesh Gupta: of the positive training points.

355
00:43:34.850 --> 00:43:37.860
Nilesh Gupta: then, if you have only one training point for a label.

356
00:43:37.880 --> 00:43:41.160
Nilesh Gupta: then you, the that you will get is yeah, worse.

357
00:43:42.050 --> 00:43:47.190
Nilesh Gupta: like I mean like it. It. It might not be that generalizable like

358
00:43:47.480 --> 00:43:51.910
Nilesh Gupta: that true representation for that label. like, if you only have one training point.

359
00:43:52.930 --> 00:43:53.790
Chitrank Gupta: Okay.

360
00:43:54.520 --> 00:43:58.200
Nilesh Gupta: Yeah. So that is definitely true. So like

361
00:43:58.570 --> 00:43:59.720
Nilesh Gupta: these.

362
00:44:03.530 --> 00:44:08.930
Chitrank Gupta: like, for example, for the Amazon data set like if you are using.

363
00:44:09.980 --> 00:44:28.380
Chitrank Gupta: So if you're using so, so, for example, you to calculate the the represent initial representation for some cosmetic, right? So we'll be like averaging the user re representations of those who bought that particular cost. But then i'm also wondering

364
00:44:28.580 --> 00:44:30.040
Chitrank Gupta: that

365
00:44:30.070 --> 00:44:40.080
Chitrank Gupta: I mean usually like the people who buy one cosmetic brand they do not buy from other cosmetic brands. So what i'm thinking is that like

366
00:44:40.900 --> 00:44:42.450
Chitrank Gupta: that.

367
00:44:44.060 --> 00:44:49.000
Chitrank Gupta: the thing is that I don't think that the

368
00:44:49.670 --> 00:44:51.860
Chitrank Gupta: the like, the you.

369
00:44:53.110 --> 00:44:57.190
Chitrank Gupta: the label representations will

370
00:44:58.050 --> 00:45:07.910
Chitrank Gupta: like, I mean the prop, the the the assumption that we are thinking that the the representations of similar labels will become similar will hold true.

371
00:45:07.990 --> 00:45:12.140
Chitrank Gupta: because usually I I mean like people, if by like.

372
00:45:12.160 --> 00:45:18.290
Chitrank Gupta: What I'm saying is that, for example, in some particular category like cosmetics, if they are buying one particular

373
00:45:20.210 --> 00:45:26.230
Chitrank Gupta: brand, and they do not usually by the

374
00:45:26.600 --> 00:45:29.790
Nilesh Gupta: you also have other information, like

375
00:45:29.870 --> 00:45:32.160
Nilesh Gupta: descriptions of

376
00:45:32.200 --> 00:45:36.330
Nilesh Gupta: your labels, we can potentially use that to

377
00:45:36.600 --> 00:45:38.750
Nilesh Gupta: mitigate that I like.

378
00:45:39.330 --> 00:45:44.810
Nilesh Gupta: because, like I, i'm sure like if there are descriptions given, then for different brands

379
00:45:44.890 --> 00:45:47.400
Nilesh Gupta: like there will be some overlap in the description.

380
00:45:47.550 --> 00:45:49.570
Chitrank Gupta: and that has also already been done.

381
00:45:50.690 --> 00:45:59.340
Nilesh Gupta: So it's. There are some like initial exploration, but, like there is no clear cut way of like, yeah, this is the best thing.

382
00:45:59.620 --> 00:46:03.180
Nilesh Gupta: So that's yeah. There is definitely like. if you

383
00:46:03.500 --> 00:46:05.810
Nilesh Gupta: let's go to our brain work here.

384
00:46:06.830 --> 00:46:09.750
Chitrank Gupta: Okay. yeah, that's it. Thanks.

385
00:46:10.490 --> 00:46:11.290
Nilesh Gupta: Okay.

386
00:46:13.080 --> 00:46:23.330
Nilesh Gupta: alright, so let me quickly look at the other 2 topics that we have. This one is fairly advanced and like

387
00:46:23.470 --> 00:46:27.590
Nilesh Gupta: it. because about the theoretical

388
00:46:28.010 --> 00:46:32.770
Nilesh Gupta: observation on the training dynamics of neural networks, and

389
00:46:33.520 --> 00:46:34.820
Nilesh Gupta: the

390
00:46:34.860 --> 00:46:37.260
Nilesh Gupta: it's it, the it's.

391
00:46:38.420 --> 00:46:43.980
Nilesh Gupta: The phenomenon is referred to as edge of a gradient descent, that edge of stability.

392
00:46:44.250 --> 00:46:46.920
Nilesh Gupta: So what does it say is that

393
00:46:47.180 --> 00:46:49.890
Nilesh Gupta: like when you are doing gradient descent or neural networks.

394
00:46:50.350 --> 00:46:52.940
Nilesh Gupta: typically

395
00:46:54.590 --> 00:46:56.100
Nilesh Gupta: it's it's

396
00:46:56.920 --> 00:47:00.590
Nilesh Gupta: here like. For now I

397
00:47:00.920 --> 00:47:07.410
Nilesh Gupta: won't define what exactly the sharpness is but like for a particular loss function.

398
00:47:09.260 --> 00:47:15.510
Nilesh Gupta: and the derivative that you are taking like you can define this thing quantity called sharpness.

399
00:47:15.930 --> 00:47:23.410
Nilesh Gupta: And it Basically, tells you like how sharp your lost landscape is at that particular point

400
00:47:23.660 --> 00:47:32.260
Nilesh Gupta: like how steep or how shallow the Lord's landscape is. It can like kind of give you hint towards this.

401
00:47:32.350 --> 00:47:36.420
Nilesh Gupta: So typically like the gradient descent training

402
00:47:36.690 --> 00:47:38.360
Nilesh Gupta: kind of

403
00:47:38.650 --> 00:47:46.150
Nilesh Gupta: converges. If the learning rate satisfies this inequality, and it kind of diverges when it satisfy

404
00:47:46.210 --> 00:47:48.480
Nilesh Gupta: this inequality.

405
00:47:48.700 --> 00:48:00.300
Nilesh Gupta: But what this result says is that doing these updates on a neural network? Typically, what have ends up happening is

406
00:48:00.520 --> 00:48:05.770
Nilesh Gupta: your sharpness is always oscillating here.

407
00:48:05.790 --> 00:48:16.800
Nilesh Gupta: this 2 by Ita threshold. Here it is the learning rate, so it's kind of always oscillating near the edge of stability, like if it if it was like above that

408
00:48:16.820 --> 00:48:26.050
Nilesh Gupta: particular region, then like it, would have been like the training, would have been diverse if it was below. Then, like a it's it's it's supposed to converge.

409
00:48:26.450 --> 00:48:27.420
Nilesh Gupta: So

410
00:48:28.180 --> 00:48:38.310
Nilesh Gupta: what this result shows is that this training neural network training dynamic follows this kind of pattern, where, like the sharpness of

411
00:48:38.590 --> 00:48:43.940
Nilesh Gupta: the it traces or oscillating near this edge of stability threshold.

412
00:48:44.230 --> 00:48:58.690
Nilesh Gupta: So with this project like we are like, I'm: yeah, we are not expecting that we will go into the details of everything here. But what you can do is try to just reproduce these findings empirically in some simple neural networks and on simpler benchmarks.

413
00:48:59.080 --> 00:49:06.450
Nilesh Gupta: So I have referenced some of the documents here like these are all papers which go into details.

414
00:49:06.780 --> 00:49:08.640
Nilesh Gupta: this particular result.

415
00:49:08.760 --> 00:49:10.110
Nilesh Gupta: But yeah.

416
00:49:10.290 --> 00:49:12.160
Nilesh Gupta: any questions on this.

417
00:49:21.230 --> 00:49:22.030
Nilesh Gupta: Okay.

418
00:49:22.420 --> 00:49:27.620
Nilesh Gupta: So okay, let me just quickly go over the last topic that we have.

419
00:49:27.670 --> 00:49:32.680
Nilesh Gupta: which is comparison of first order versus second order, optimization methods.

420
00:49:32.950 --> 00:49:37.300
Nilesh Gupta: So. as we all know, optimization is kind of the

421
00:49:37.530 --> 00:49:41.730
Nilesh Gupta: main learning component behind machine learning algorithms.

422
00:49:41.940 --> 00:49:46.930
Nilesh Gupta: And typically, you, as we have seen, like you in a shedley. What we are doing is

423
00:49:47.080 --> 00:49:50.220
Nilesh Gupta: we are making updates.

424
00:49:50.300 --> 00:49:58.940
Nilesh Gupta: We are taking the gradients, and we are making updates in the negative direction of the gradient. So we are just using the graded information to make our updates.

425
00:49:59.110 --> 00:50:03.660
Nilesh Gupta: But there are other family of optimization methods

426
00:50:03.720 --> 00:50:06.150
Nilesh Gupta: which are second order, optimization, methods

427
00:50:06.180 --> 00:50:12.360
Nilesh Gupta: which you that even the double derivative of the loss function with respect to the parameters.

428
00:50:12.580 --> 00:50:21.170
Nilesh Gupta: and they they use that information to make much more informed updates for your weights.

429
00:50:22.010 --> 00:50:31.660
Nilesh Gupta: So typically these double derivatives are kind of information stored in a matrix called Hessian matrix.

430
00:50:31.770 --> 00:50:43.920
Nilesh Gupta: and the updates will look something like this which is going to be wt plus one is wt minus some learning rate times this a ht inverse times the gradient.

431
00:50:44.020 --> 00:50:52.410
Nilesh Gupta: So this is different compared to the first order method at the by this Ht matrix, which is the hash matrix.

432
00:50:52.760 --> 00:51:00.000
Nilesh Gupta: So these particular methods come into the family of second order optimization methods

433
00:51:00.220 --> 00:51:10.660
Nilesh Gupta: you might have already like. I think some of you should might know already about the Newton method, which is particular.

434
00:51:10.760 --> 00:51:14.920
Nilesh Gupta: a second order, optimization, methods. But

435
00:51:15.020 --> 00:51:16.170
Nilesh Gupta: the

436
00:51:16.540 --> 00:51:24.870
Nilesh Gupta: downsides of these second order. Optimization methods are. They are very computationally expensive, especially for large models.

437
00:51:24.890 --> 00:51:42.640
Nilesh Gupta: And so, because of the simplicity and the low computational cost of standard first order, gradient methods. They are more, much more popular, and are of used much more extensively than second order, optimization, methods, and neural network training.

438
00:51:42.920 --> 00:51:56.430
Nilesh Gupta: So with this project, you can explore the differences, or between first order and second order, optimization methods on some very small neural networks, so that, like you, are indeed able to do

439
00:51:56.800 --> 00:51:58.630
Nilesh Gupta: second order optimization.

440
00:51:58.780 --> 00:52:02.130
Nilesh Gupta: and you can try to compare

441
00:52:02.170 --> 00:52:05.530
Nilesh Gupta: these 2 methods based on their convergence rate.

442
00:52:05.540 --> 00:52:09.920
Inderjit Dhillon: computational cost and the accuracy, the final accuracy, that ticket.

443
00:52:10.140 --> 00:52:23.660
Nilesh Gupta: and you can also explore the effect of hyper parameters, such as learning, date, path, size, and their robustness to these hyper parameters. When you are using these different class of methods.

444
00:52:24.220 --> 00:52:27.290
Nilesh Gupta: So I have given some links to

445
00:52:27.610 --> 00:52:37.660
Nilesh Gupta: some blog post and papers which go into details of this particular problem. Yeah.

446
00:52:38.370 --> 00:52:50.610
Chitrank Gupta: let me know if you have any questions on this one. Yeah, I have this one question that like is this: Haitian analytically computed, like as we do for first order, derivatives or like, is it like computationally.

447
00:52:51.330 --> 00:52:55.340
Chitrank Gupta: computer? So I have first order principles.

448
00:52:56.260 --> 00:53:04.050
Nilesh Gupta: So so it can be analytically computed like by even like bye Touch allows you to compute the session matrix.

449
00:53:04.110 --> 00:53:09.860
Nilesh Gupta: But that is very like computationally involved, since, like it's

450
00:53:09.940 --> 00:53:14.870
Nilesh Gupta: like. If you have n parameters, then you will have an N. Cross n matrix. So

451
00:53:17.320 --> 00:53:25.760
Chitrank Gupta: so it just memory. Are we right? Not like time. It's memory and computationally heavy, because, like for each element like you'll have to do that computation.

452
00:53:27.150 --> 00:53:29.540
Nilesh Gupta: And I think

453
00:53:29.600 --> 00:53:45.190
Nilesh Gupta: we're able to comment better on this. Yeah. So in in practical cases, you know, we actually this Ht is not a Hashian matrix, right? It's. That's why we we can write up. It says it's a precondition or matrix. And typically it is computed by

454
00:53:45.400 --> 00:53:50.760
Inderjit Dhillon: using the second moment it it basically of the gradients.

455
00:53:51.050 --> 00:53:55.760
Inderjit Dhillon: So it is of the form, you know, summation of Gg. Gtt. Transpose.

456
00:53:56.190 --> 00:54:08.140
Inderjit Dhillon: and then you again try to make it efficient. So if you look at these second order methods, they are generally, you know, some

457
00:54:08.160 --> 00:54:14.760
Inderjit Dhillon: we to condense this or sparsify this preconditional matrix.

458
00:54:21.160 --> 00:54:24.010
Inderjit Dhillon: And and then, for you know, like

459
00:54:24.170 --> 00:54:33.980
Inderjit Dhillon: So so thank you, Niche, for discussing these products. Do you have anything else to discuss? Or I think i'm done? If there are any discussions, and we can do

460
00:54:34.090 --> 00:54:34.710
Nilesh Gupta: Yeah.

461
00:54:34.790 --> 00:54:37.830
Inderjit Dhillon: yeah, Any other discussions on the last project?

462
00:54:38.360 --> 00:54:43.540
Chitrank Gupta: Did you have more questions or that? This was all. Thank you.

463
00:54:43.610 --> 00:54:44.790
Inderjit Dhillon: Okay, great.

464
00:54:45.120 --> 00:55:04.010
Inderjit Dhillon: So you know, some of these projects are actually, you know, closely related to like. For example, I think it was project to 8 and 6 that is closely related to kind of negligious research, so he can definitely guide you very closely. The last 2 projects are, you know, closely

465
00:55:04.090 --> 00:55:17.520
Inderjit Dhillon: related to the research of another student, and Psi and Nilesh will add his contact information in the project. Description idea, I don't think he's added that right now. But he

466
00:55:17.650 --> 00:55:21.070
Inderjit Dhillon: yeah, so so so as you kind of think about.

467
00:55:21.210 --> 00:55:33.190
Inderjit Dhillon: you know, deciding on the project, and also during the project, you know, you can discuss with the Middle East, for example, for Project 6, but also earlier ones, and

468
00:55:33.260 --> 00:55:38.300
Inderjit Dhillon: for 7 and 8. You can lean on side because he is interested in that.

469
00:55:39.680 --> 00:55:45.180
Inderjit Dhillon: Okay, any overall questions about what Middle East presented just now.

470
00:55:49.080 --> 00:55:57.920
Inderjit Dhillon: Okay. So just to kind of recap a little bit right? So now we are almost past the midpoint of the

471
00:55:58.180 --> 00:56:00.770
Inderjit Dhillon: course in terms of the number of lectures.

472
00:56:00.910 --> 00:56:05.810
Inderjit Dhillon: you know, and next week is spring break.

473
00:56:06.230 --> 00:56:15.220
Inderjit Dhillon: And now, in some sense, the course is entering into its second phase. So far it's been, you know, more

474
00:56:15.360 --> 00:56:18.990
Inderjit Dhillon: instruction was lecture based.

475
00:56:19.160 --> 00:56:21.560
Inderjit Dhillon: and you are doing a couple of homeworks.

476
00:56:21.740 --> 00:56:29.070
Inderjit Dhillon: But now it is actually going to transition to where you know you are going to start making having some presentations.

477
00:56:29.180 --> 00:56:32.300
Inderjit Dhillon: and you'll be involved in your project.

478
00:56:32.310 --> 00:56:42.650
Inderjit Dhillon: Okay, so let me just cut and paste over here. There's a zoom chat. So please take a look at the zoom chat.

479
00:56:42.840 --> 00:56:46.210
Inderjit Dhillon: and we, of course, have this in the course schedule.

480
00:56:48.330 --> 00:56:51.040
So this is the

481
00:56:51.500 --> 00:56:54.810
Inderjit Dhillon: the you know what is sort of coming up.

482
00:56:55.200 --> 00:57:07.880
Inderjit Dhillon: you know. We Your homework, too, is due this week end of this week. and after you come back we'll assign homework 3 on March twentieth. It'll be due on March 30 First

483
00:57:08.270 --> 00:57:12.400
Inderjit Dhillon: Home Book 4 will be the on April fifth.

484
00:57:12.770 --> 00:57:23.100
Inderjit Dhillon: Oh, sorry assigned on April fifth. You on April seventeenth, is just to give me an idea of what is coming ahead. Now Project, we want you to decide

485
00:57:25.390 --> 00:57:39.730
Inderjit Dhillon: by. So we've just given you project ideas. You can either choose from among these project ideas, or you can have your own project idea. I know that some of you are interested in kind of

486
00:57:39.790 --> 00:57:43.560
Inderjit Dhillon: research in your field of study.

487
00:57:43.780 --> 00:57:49.090
Inderjit Dhillon: But we do need there to be about 3 people per project.

488
00:57:49.420 --> 00:57:54.540
Inderjit Dhillon: Okay, so you'll have to do some convincing, or to if you want to.

489
00:57:54.590 --> 00:57:59.500
Inderjit Dhillon: other students to work on a project in your research area.

490
00:58:00.340 --> 00:58:09.190
Inderjit Dhillon: and I would like you to. So for for the project. They will be, you know, in some sense, 3 submissions.

491
00:58:09.660 --> 00:58:20.340
Inderjit Dhillon: The first one I should have mentioned it. The first one will actually be the week after spring break. That's just that. You tell us what your project is going to be on.

492
00:58:21.140 --> 00:58:26.540
Inderjit Dhillon: Did I? Did we decide on a particular date I think it was.

493
00:58:27.550 --> 00:58:34.800
Nilesh Gupta: I think it was March 24 right, like we just have to sign up for this 20s yeah, 20s 24.

494
00:58:34.820 --> 00:58:41.950
Inderjit Dhillon: There's no hard deadline. It basically just says you give us one sheet of paper

495
00:58:41.960 --> 00:58:45.560
Inderjit Dhillon: which gives the project what the project is.

496
00:58:45.660 --> 00:58:54.680
Inderjit Dhillon: My project title, a brief description and a brief plan of action. It can all fit in one page. And, of course, most importantly, you need to tell us

497
00:58:54.790 --> 00:59:08.510
Inderjit Dhillon: who is going to be doing this project. Okay. So every student must have their name on on a project. Okay, so that will be more April. Sorry. March 20s.

498
00:59:08.880 --> 00:59:11.650
Inderjit Dhillon: maybe. I's just that.

499
00:59:12.050 --> 00:59:15.420
Inderjit Dhillon: I'll write that in the chat. and then

500
00:59:15.950 --> 00:59:25.340
Inderjit Dhillon: I think it will be good for you to. And this is really mainly for your benefit, right? The project is going to be due on

501
00:59:25.570 --> 00:59:29.280
Inderjit Dhillon: around the April 20, eighth.

502
00:59:29.680 --> 00:59:34.170
Inderjit Dhillon: April, 20 eighth. The reason we have April 28 is it is the

503
00:59:34.270 --> 00:59:38.640
Inderjit Dhillon: the the classes end on April 20 Fourth. That's the last date of class.

504
00:59:40.640 --> 00:59:44.980
Inderjit Dhillon: Why, we did like to sign up by March 24. Okay.

505
00:59:45.000 --> 00:59:56.800
Inderjit Dhillon: but that's for the presentation. Sorry. So so the the project, the the final exam for this course is actually scheduled on April 20, eighth.

506
00:59:57.400 --> 01:00:00.420
Inderjit Dhillon: Now there's no final exam. So don't be alarmed.

507
01:00:00.690 --> 01:00:05.550
Inderjit Dhillon: right? But what we are saying is that you will. We will probably try and have

508
01:00:05.680 --> 01:00:11.860
Inderjit Dhillon: the project presentations. You know the talks by each of the groups

509
01:00:12.520 --> 01:00:23.520
Inderjit Dhillon: currently, tetratively scheduled for April 20, eighth. The slight bad news is that April 20, eighth, the time for the final and for these task presentations is

510
01:00:23.960 --> 01:00:28.080
Inderjit Dhillon: April 20, eighth, Friday, from 7 to 9. Pm.

511
01:00:28.690 --> 01:00:40.380
Inderjit Dhillon: So not the ideal time, I think, for either you or for us. So what we might try and do is try and move it up. But of course it only can be done. If there is No.

512
01:00:40.410 --> 01:00:45.030
Inderjit Dhillon: there is no conflict for any of you.

513
01:00:45.450 --> 01:00:51.520
Inderjit Dhillon: Okay, but think about April 20, eighth, as approximately the final submission date.

514
01:00:51.670 --> 01:01:07.820
Inderjit Dhillon: So, working backwards, I think what will be beneficial is that you have your first submission for the project. Be on April 10. Okay. And then the first submission. You can just describe what all you have done and the plans for later.

515
01:01:07.960 --> 01:01:11.890
Inderjit Dhillon: It's really for your benefit, so that you kind of

516
01:01:12.090 --> 01:01:24.450
Inderjit Dhillon: are on track, and we can give you some guidelines, or you know, guidance on how to proceed it. don't think of it as something where which you will be.

517
01:01:24.610 --> 01:01:31.080
Inderjit Dhillon: you know, graded on it's really meant to be helpful to you right? So the project really lasts from.

518
01:01:31.130 --> 01:01:33.740
Inderjit Dhillon: You know the time that you decide on the project.

519
01:01:33.820 --> 01:01:37.380
Inderjit Dhillon: March 20s goes for just over a month.

520
01:01:37.490 --> 01:01:40.080
Inderjit Dhillon: and is due on April 28.

521
01:01:41.660 --> 01:01:46.370
Inderjit Dhillon: So any questions on that, and i'll then get to the last part which is the past presentations.

522
01:01:48.940 --> 01:01:50.840
Inderjit Dhillon: Any questions about the

523
01:01:51.480 --> 01:01:56.800
Inderjit Dhillon: projects now that we have given you project ideas. I think you should just start talking right away

524
01:01:56.970 --> 01:02:05.610
Inderjit Dhillon: to you know, to figure out what projects you want and who your partners are going to be.

525
01:02:11.550 --> 01:02:17.750
Inderjit Dhillon: I know that, you know, when we, for example, these classes meet physically, it's easier to maybe connect with

526
01:02:18.700 --> 01:02:28.480
Inderjit Dhillon: different people. So let me know if there's any in the way that you know, we can facilitate meetings between you people we can have like some

527
01:02:28.860 --> 01:02:36.340
Inderjit Dhillon: zoom sessions and just arrange for people to be on, so that they can discuss it. Oh.

528
01:02:37.130 --> 01:02:45.110
Inderjit Dhillon: but you know, of course you can use piazza and email, and so on to connect with other students

529
01:02:50.180 --> 01:02:51.950
Inderjit Dhillon: any more any questions.

530
01:02:53.050 --> 01:03:04.830
Inderjit Dhillon: If you don't have any questions, I'll just assume that you are all on board with this plan, and you also have a plan on what projects you will choose and how you will select your

531
01:03:07.030 --> 01:03:10.440
Inderjit Dhillon: project. Me project partners.

532
01:03:15.190 --> 01:03:16.060
Inderjit Dhillon: Okay.

533
01:03:16.910 --> 01:03:20.960
And then, like, I said, You know the last part of the class will be

534
01:03:21.080 --> 01:03:23.810
Inderjit Dhillon: actually presentations by you.

535
01:03:24.550 --> 01:03:28.660
Inderjit Dhillon: These are not necessarily related to the project, but can be.

536
01:03:28.880 --> 01:03:34.510
Inderjit Dhillon: but these are just presentations of technical material that

537
01:03:35.470 --> 01:03:38.040
Inderjit Dhillon: will be made in groups of 2.

538
01:03:38.600 --> 01:03:47.170
Inderjit Dhillon: We'll send more details soon. We would like you to start signing up for these project presentations by March 20 fourth.

539
01:03:47.420 --> 01:03:51.800
Inderjit Dhillon: It'll be just a short presentation about 10 or 12min

540
01:03:52.050 --> 01:03:57.230
Inderjit Dhillon: by groups of 2, and it could be something like, you know.

541
01:03:57.610 --> 01:04:09.360
Inderjit Dhillon: We will have different material, you know. It will be, you know, layer normalization in neural networks, or you know, some concrete topic, or it could be something like, you know.

542
01:04:09.720 --> 01:04:12.060
Inderjit Dhillon: or does chat, Gp: work at a high level.

543
01:04:12.140 --> 01:04:18.550
Inderjit Dhillon: Right? So these are presentations that you will make, and we will deserve the last 4 days

544
01:04:18.720 --> 01:04:23.390
Inderjit Dhillon: of the class for these presentations, so we'll plan to cover about.

545
01:04:23.610 --> 01:04:27.190
Inderjit Dhillon: The plan is to cover what 5 of these presentations

546
01:04:27.380 --> 01:04:29.990
Inderjit Dhillon: in each

547
01:04:30.210 --> 01:04:33.370
Inderjit Dhillon: you know one and a half lecture. one and a half hour like.

548
01:04:34.880 --> 01:04:36.890
Inderjit Dhillon: and we'll send out details to you.

549
01:04:37.560 --> 01:04:43.160
Inderjit Dhillon: So that's the kind of plan for now that we are, you know, just past the halfway

550
01:04:43.310 --> 01:04:57.260
Inderjit Dhillon: mark of this course. This is just a plan of action for the rest of the rest of the course. you know. I'll still be giving some lectures. But you know, I really want you to concentrate now on

551
01:04:57.400 --> 01:05:01.140
Inderjit Dhillon: the projects as well as these presentations.

552
01:05:03.970 --> 01:05:05.450
Inderjit Dhillon: Any questions.

553
01:05:15.240 --> 01:05:19.900
Inderjit Dhillon: Okay, it looks like everybody is on board with this plan.

554
01:05:20.200 --> 01:05:28.770
Inderjit Dhillon: But of course i'm kidding. I'm sure you'll have questions later. You're welcome to ask. You know the ta or me more questions.

555
01:05:28.790 --> 01:05:42.710
Inderjit Dhillon: I don't really have any more thing to discuss. I just wanted to spend an entire lecture talking about class projects, and really to emphasize the importance that you know that you really need to

556
01:05:42.730 --> 01:05:51.280
Inderjit Dhillon: look at these project ideas, or you might have already a project idea. In either case you need to form a team of about 3

557
01:05:51.420 --> 01:06:03.140
Inderjit Dhillon: and decide on your project, and if you are, you know, decided on your project. You're welcome to start working on it right. I mean, we just need to. Okay, the project right if you're coming.

558
01:06:03.780 --> 01:06:16.130
Inderjit Dhillon: Obviously, if it is one of the product ideas we have given that's fine. And if you. If it is a project coming from your own research or some other area, it

559
01:06:16.170 --> 01:06:20.970
Inderjit Dhillon: you kind of do need to get. Write it up and get kind of approval from us.

560
01:06:22.650 --> 01:06:27.630
Inderjit Dhillon: I don't know if Nilesh mentioned. I think it is fine. If 2 teams

561
01:06:29.030 --> 01:06:42.170
Inderjit Dhillon: do you know the same project? Of course the work might be different, and we will try and make sure that if 2 teams select the same project from one, the project ideas

562
01:06:42.190 --> 01:06:44.580
Inderjit Dhillon: that you are doing slightly different work.

563
01:06:44.860 --> 01:06:52.520
Inderjit Dhillon: Obviously, that's not the same for the presentations. Every team will be presenting on a different topic.

564
01:06:54.680 --> 01:07:00.450
Inderjit Dhillon: Okay, so one last call any questions. Otherwise we can actually conclude today.

565
01:07:07.420 --> 01:07:15.070
Inderjit Dhillon: Okay, sounds good. So we'll meet next time. I think I will actually go a little bit more into

566
01:07:15.550 --> 01:07:27.710
Inderjit Dhillon: the nonlinearly separable. Svm: because that naturally leads to these things called techniques called kernel methods. So I think i'll do that in class on Wednesday.

567
01:07:27.900 --> 01:07:32.650
Inderjit Dhillon: And so I look forward to seeing you them. Okay, Bye, bye.

568
01:07:35.360 --> 01:07:37.130
Ashray Desai: Okay, thanks.

569
01:07:37.280 --> 01:07:38.480
Inderjit Dhillon: Okay, Bye.

WEBVTT

1
00:01:03.990 --> 00:01:06.000
Inderjit Dhillon: Very good afternoon.

2
00:01:07.600 --> 00:01:09.180
Inderjit Dhillon: Welcome everybody.

3
00:01:12.560 --> 00:01:14.820
Inderjit Dhillon: Any questions before I get started.

4
00:01:26.090 --> 00:01:28.090
Inderjit Dhillon: Okay, If not, then.

5
00:01:29.310 --> 00:01:31.730
Inderjit Dhillon: and start sharing my screen.

6
00:01:32.870 --> 00:01:33.540
Okay.

7
00:01:59.040 --> 00:02:00.650
Inderjit Dhillon: Okay.

8
00:02:01.710 --> 00:02:15.510
Inderjit Dhillon: So remember just a few details. Spring break is next week, so hopefully, all of you will have a great spring break. There is a homework queue to which is due on Friday.

9
00:02:16.090 --> 00:02:19.560
Inderjit Dhillon: Let me know if you have any questions.

10
00:02:21.980 --> 00:02:29.630
Inderjit Dhillon: Okay, if not, let's get started. So last time well last lecture that I gave.

11
00:02:29.810 --> 00:02:46.520
Inderjit Dhillon: you know last time. On Monday we discussed about class projects so hopefully that is going well in terms of choosing what project that you are going to do. I have seen a lot of activity on P. Outside, and looks like people are

12
00:02:46.670 --> 00:02:58.330
Inderjit Dhillon: forming teams. But if you do need kind of any assistance in getting matched up and choosing a project i'll be, you know we'll be happy to have help as much as we can.

13
00:02:58.810 --> 00:03:06.530
Inderjit Dhillon: But before that, in the lecture, before we had talked about support vector machines. We had talked before about

14
00:03:08.070 --> 00:03:20.880
Inderjit Dhillon: about perceptron. Then we had this maximum margin viewpoint, and then we talked about support vector machines. and we had did this derivation. Looking at the primal problem.

15
00:03:20.910 --> 00:03:40.420
Inderjit Dhillon: we derive the dual problem, but we saw that that was also applicable only to the case of linearly separable support vector machines. And really that's not the form that support vector, machines are or were used. And just to kind of give you an idea. They were probably one of the best

16
00:03:41.160 --> 00:03:50.850
Inderjit Dhillon: classification techniques before sort of the onset of of deep learning. But it actually has. You might also ask me

17
00:03:51.010 --> 00:04:03.730
Inderjit Dhillon: that, hey? You know, when you do some other classification techniques that we've talked about like logistic regression. We have this viewpoint about loss and regularization.

18
00:04:03.810 --> 00:04:06.670
Inderjit Dhillon: Well, what is the analogy over here. Okay.

19
00:04:06.760 --> 00:04:20.410
Inderjit Dhillon: So today we kind of wrap up talking about support vector machines. And i'll also introduce a a concept of kernel methods through talking about kernel support vector machines.

20
00:04:20.649 --> 00:04:35.980
Inderjit Dhillon: and that's an important concept, maybe not as important as it was 10 or 15 years ago. But still I think it's something that everybody who does machine learning should be aware of. So that's kind of the purpose of this lecture. We'll talk about

21
00:04:36.030 --> 00:04:40.910
Inderjit Dhillon: nonlinearly separable support vector machines and talk also about

22
00:04:41.000 --> 00:04:45.720
Inderjit Dhillon: kernel methods in particular kernel support vector machines. Okay.

23
00:04:45.920 --> 00:04:52.940
Inderjit Dhillon: So we are today talking about non linearly

24
00:04:54.460 --> 00:04:55.830
Inderjit Dhillon: approval.

25
00:04:56.970 --> 00:05:00.890
Inderjit Dhillon: Svm's. Okay, I'm: just to remind you

26
00:05:01.620 --> 00:05:05.960
Inderjit Dhillon: what spms are. These are support vector

27
00:05:06.490 --> 00:05:10.890
Inderjit Dhillon: machines. I kind of explained why they are called support vector machines.

28
00:05:10.950 --> 00:05:15.840
Inderjit Dhillon: and we can go over it again. Okay. And today we'll also talk about

29
00:05:16.200 --> 00:05:17.170
Inderjit Dhillon: Colonel.

30
00:05:17.280 --> 00:05:18.770
Inderjit Dhillon: Nothing else.

31
00:05:19.580 --> 00:05:21.520
Inderjit Dhillon: Okay, so that's what we will.

32
00:05:22.670 --> 00:05:25.850
Inderjit Dhillon: I will teach about today. So let's recall

33
00:05:27.490 --> 00:05:29.530
Inderjit Dhillon: okay, what we did so far

34
00:05:29.830 --> 00:05:32.400
Inderjit Dhillon: is linearly

35
00:05:33.860 --> 00:05:35.090
Inderjit Dhillon: the preval

36
00:05:40.430 --> 00:05:47.730
Inderjit Dhillon: That means your class can be classes. You know we are looking at binary classification problems

37
00:05:47.960 --> 00:05:50.740
Inderjit Dhillon: that your classification

38
00:05:52.220 --> 00:05:56.170
Inderjit Dhillon: problem is a binary classification problem. Okay.

39
00:05:56.270 --> 00:05:59.470
Inderjit Dhillon: So the Svm.

40
00:06:03.010 --> 00:06:13.820
Inderjit Dhillon: Okay. So the Svm. Primer. Let me write both the primal as well as dual. Okay, so let me write the so here is the kind of the picture

41
00:06:14.090 --> 00:06:17.860
Inderjit Dhillon: we have one plus denoted by pluses.

42
00:06:19.280 --> 00:06:21.510
Inderjit Dhillon: The other class denoted by

43
00:06:22.580 --> 00:06:24.840
Inderjit Dhillon: today. I'll just say minuses.

44
00:06:25.970 --> 00:06:28.770
Inderjit Dhillon: Are you circled in the past. Okay.

45
00:06:28.900 --> 00:06:31.590
and what we are trying to find is

46
00:06:32.290 --> 00:06:39.960
Inderjit Dhillon: separating hyperplane and one that has the maximum margin. And if you recall.

47
00:06:41.500 --> 00:06:45.070
Inderjit Dhillon: you know, these are the points on the boundary.

48
00:06:46.290 --> 00:06:48.210
Inderjit Dhillon: and then let

49
00:06:49.960 --> 00:06:51.600
Inderjit Dhillon: these points

50
00:06:52.950 --> 00:06:56.280
Inderjit Dhillon: also be on the boundary.

51
00:06:57.030 --> 00:06:57.760
Inderjit Dhillon: Okay.

52
00:06:57.800 --> 00:07:00.040
Inderjit Dhillon: The margin

53
00:07:01.530 --> 00:07:03.210
Inderjit Dhillon: is given by

54
00:07:04.260 --> 00:07:09.720
Inderjit Dhillon: one divided by the normal to you as we saw

55
00:07:09.750 --> 00:07:16.530
Inderjit Dhillon: before. Okay. So the Svm. Primal. So this is just a recap.

56
00:07:18.080 --> 00:07:19.690
Inderjit Dhillon: S. 3, I'm. Primal

57
00:07:21.230 --> 00:07:36.230
Inderjit Dhillon: is minimize over W. Of all the 2 norm of W Square. subject to these inequality constraints, which is

58
00:07:36.460 --> 00:07:46.680
Inderjit Dhillon: one minus y. I remember that the yi's are plus ones or minus one plus one for class, one minus one for class, 2

59
00:07:47.290 --> 00:07:52.520
Inderjit Dhillon: W. Transpose X. I plus W. Naught

60
00:07:53.620 --> 00:07:57.980
Inderjit Dhillon: is less than equal to 0. We are writing it in the standard form where

61
00:07:58.060 --> 00:08:01.900
Inderjit Dhillon: I have minimize, if not subject to

62
00:08:02.400 --> 00:08:09.300
Inderjit Dhillon: F. One or multiple F of W. Is less than equal to 0. So this is from I

63
00:08:09.530 --> 00:08:12.280
Inderjit Dhillon: equals 1 2 through.

64
00:08:12.350 --> 00:08:12.990
And

65
00:08:14.990 --> 00:08:17.650
Inderjit Dhillon: so this is the primal.

66
00:08:17.910 --> 00:08:19.800
Inderjit Dhillon: So Svm: Primal.

67
00:08:23.500 --> 00:08:34.770
Inderjit Dhillon: Okay. And then what we did last time was derive the dual we went through, if you remember, and we'll actually go through it again, just to give you some more exercise.

68
00:08:35.280 --> 00:08:49.260
Inderjit Dhillon: We'll, you know, form the lagrangian, and form the dual function, and then talk about maximizing the tool. But the dual in this case was maximize over Alpha, and remember the Alphas

69
00:08:49.400 --> 00:08:59.500
Inderjit Dhillon: correspond to the lagrange multipliers, for each of the inequality constraints, which means that there is a Lagrange multiplier that corresponds to each of the training points

70
00:08:59.740 --> 00:09:02.400
Inderjit Dhillon: because each of the inequality constraint

71
00:09:02.530 --> 00:09:05.890
Inderjit Dhillon: talks about the margin of the training.

72
00:09:06.880 --> 00:09:10.860
Inderjit Dhillon: So the my, the dual is

73
00:09:12.110 --> 00:09:17.350
Inderjit Dhillon: maximize summation of Alpha I I equals one through m

74
00:09:17.920 --> 00:09:20.300
Inderjit Dhillon: minus one half.

75
00:09:21.440 --> 00:09:31.550
Inderjit Dhillon: and then there's a dual sound. I equals one through end. J equals one through M. Alpha I

76
00:09:32.840 --> 00:09:35.440
Inderjit Dhillon: Y. I what J.

77
00:09:36.520 --> 00:09:42.470
Inderjit Dhillon: X. I transpose X to. Okay, so it can be expressed in terms of the inner products

78
00:09:42.700 --> 00:09:53.700
Inderjit Dhillon: between the training points. And that's something that we'll see is helpful or important when we come to, especially to kernel methods. and the

79
00:09:54.050 --> 00:09:57.870
Inderjit Dhillon: constraints are that the dual variables are feasible.

80
00:09:58.520 --> 00:10:08.870
Inderjit Dhillon: which means that each of the Lagrange multipliers is greater than equal to 0, because they correspond to linear inequality, or less than equal to constraints.

81
00:10:10.390 --> 00:10:15.500
Inderjit Dhillon: And then I have a balancing constraint. Summation of alpha I

82
00:10:15.710 --> 00:10:19.880
Inderjit Dhillon: y. I equals 0. I equals one through.

83
00:10:20.920 --> 00:10:23.680
Inderjit Dhillon: So that is what we kind of did.

84
00:10:27.890 --> 00:10:30.650
Inderjit Dhillon: and the last lecture on Svm's.

85
00:10:32.080 --> 00:10:38.650
Inderjit Dhillon: And if you remember well, if you remember, W. Has this phone, the Optimal W.

86
00:10:38.700 --> 00:10:40.670
Inderjit Dhillon: Has the form that it is

87
00:10:40.770 --> 00:10:44.880
Inderjit Dhillon: summation of Alpha I. Y. I x i.

88
00:10:44.980 --> 00:10:46.000
Inderjit Dhillon: Where

89
00:10:47.570 --> 00:10:58.590
Inderjit Dhillon: alpha I is the solution of the dual variable. So that's the connection between the primal, optimal and the dual optimal.

90
00:10:59.110 --> 00:10:59.840
Inderjit Dhillon: Okay.

91
00:11:00.170 --> 00:11:07.140
Inderjit Dhillon: Now, there's a thing that we learned about last time or talked about last time it was complementary slackness.

92
00:11:07.420 --> 00:11:10.170
Inderjit Dhillon: which means that that if

93
00:11:10.360 --> 00:11:15.310
Inderjit Dhillon: the inequality constraint over here, if this inequality constraint

94
00:11:15.590 --> 00:11:24.110
Inderjit Dhillon: is a strict inequality, that means this does not have an equality that means that the corresponding dual variable

95
00:11:24.160 --> 00:11:26.760
Inderjit Dhillon: must be actually equal to 0.

96
00:11:27.720 --> 00:11:31.430
Inderjit Dhillon: Okay. So so

97
00:11:31.490 --> 00:11:35.190
Inderjit Dhillon: where what are the points for which

98
00:11:35.260 --> 00:11:39.520
Inderjit Dhillon: this is equality constraint? So let me put it in a different color.

99
00:11:40.180 --> 00:11:42.980
Inderjit Dhillon: Okay. So these are the points

100
00:11:44.480 --> 00:11:46.200
Inderjit Dhillon: which are on the boundary.

101
00:11:47.690 --> 00:11:49.890
Inderjit Dhillon: and they are the points where

102
00:11:52.400 --> 00:11:54.000
Inderjit Dhillon: they actually lie

103
00:11:55.170 --> 00:11:57.450
Inderjit Dhillon: on that kind of

104
00:11:58.100 --> 00:11:59.490
Inderjit Dhillon: boundary.

105
00:11:59.930 --> 00:12:04.240
Inderjit Dhillon: and these are exactly the points where the

106
00:12:04.530 --> 00:12:10.540
Inderjit Dhillon: alpha eyes are greater than 0, because these are the points where

107
00:12:10.750 --> 00:12:13.240
Inderjit Dhillon: this is met with equality.

108
00:12:13.590 --> 00:12:16.260
Inderjit Dhillon: And so this is

109
00:12:16.700 --> 00:12:21.070
Inderjit Dhillon: Alpha I is generally greater than 0, whereas

110
00:12:21.190 --> 00:12:26.820
Inderjit Dhillon: all these points. which are which are less than 0.

111
00:12:28.370 --> 00:12:30.300
Inderjit Dhillon: All these points

112
00:12:33.370 --> 00:12:36.140
Inderjit Dhillon: they have. Alpha I equal to 0.

113
00:12:39.680 --> 00:12:43.550
Inderjit Dhillon: Okay, so what so? And remember, this is there, right?

114
00:12:46.970 --> 00:12:50.390
Inderjit Dhillon: So these points on the boundary over here.

115
00:12:51.460 --> 00:12:55.700
Inderjit Dhillon: these points on the boundary, they support this hyperplane.

116
00:12:56.820 --> 00:13:10.140
Inderjit Dhillon: Okay, as a result of which this is called support vector machines, and your W. Is a linear combination of the training points multiplied by the class label

117
00:13:10.240 --> 00:13:22.700
Inderjit Dhillon: plus one or minus one. and only the support vector machines actually count. Support. Vectors count towards that. If the alpha is greater than Z is equal to 0. It doesn't count.

118
00:13:22.890 --> 00:13:30.620
Inderjit Dhillon: And that's exactly these points over here, the points inside. Okay. So by kind of forming this dual.

119
00:13:30.990 --> 00:13:37.050
Inderjit Dhillon: we are able to kind of get much more intuition into what the support vector machine is doing.

120
00:13:37.630 --> 00:13:38.290
Inderjit Dhillon: Okay.

121
00:13:38.490 --> 00:13:49.660
Inderjit Dhillon: But of course, data and this is only works. If the data is linearly separable, we Haven't actually extended it to data which is not linearly separable.

122
00:13:49.970 --> 00:14:02.640
Inderjit Dhillon: And of course, that happens many times right. You might get data which is, you know, not as clean as what we had drawn before. So you know. Yes, you have this pluses.

123
00:14:03.330 --> 00:14:16.130
Inderjit Dhillon: and you have these minuses which are separated. But there could be some minuses here. and there could be some pluses over here. That's what you know. Most kind of real data

124
00:14:16.220 --> 00:14:17.020
Inderjit Dhillon: well

125
00:14:17.550 --> 00:14:20.440
Inderjit Dhillon: look like. So what do we do over here?

126
00:14:21.630 --> 00:14:27.020
Inderjit Dhillon: Well, if you think about this particular constraint that we had in this particular constraint.

127
00:14:27.450 --> 00:14:37.470
Inderjit Dhillon: this is the one that's saying that, hey? You know I better ensure that all my training points are on one side of the hyperplane.

128
00:14:37.890 --> 00:14:40.180
Inderjit Dhillon: But what if they are not

129
00:14:40.540 --> 00:14:52.530
Inderjit Dhillon: so, the way to do it will be to kind of relax this by adding what's called a slack variable. So the nonlinearly separable. Svm.

130
00:14:52.980 --> 00:14:54.610
Inderjit Dhillon: Okay. So the non

131
00:14:55.750 --> 00:14:57.170
Inderjit Dhillon: linearly

132
00:14:58.900 --> 00:15:00.220
Inderjit Dhillon: that I will.

133
00:15:02.100 --> 00:15:03.230
Inderjit Dhillon: Svm.

134
00:15:07.970 --> 00:15:09.920
Inderjit Dhillon: We'll say that the primal

135
00:15:10.680 --> 00:15:15.620
Inderjit Dhillon: let's write out the primal, same objective function minimize.

136
00:15:16.230 --> 00:15:17.190
Inderjit Dhillon: Oh.

137
00:15:18.360 --> 00:15:25.180
Inderjit Dhillon: W. 2 square. And remember, here the minimum was over. W. And I actually W: not.

138
00:15:26.320 --> 00:15:36.530
Inderjit Dhillon: Okay. So what are we minimizing over? We'll come to that. And then. just like before I have this constraint, which is

139
00:15:37.050 --> 00:15:43.050
Inderjit Dhillon: y I times W. Transpose X. I plus w naught

140
00:15:43.810 --> 00:15:47.720
Inderjit Dhillon: is greater than equal to one. This is the constraint that I had before.

141
00:15:48.350 --> 00:15:52.680
Inderjit Dhillon: This says that the points should be on either side of the hyperplane.

142
00:15:53.060 --> 00:15:57.870
Inderjit Dhillon: But I can relax this by saying, okay, this is greater than equal to one minus. I

143
00:15:58.480 --> 00:15:59.200
sorry

144
00:16:01.600 --> 00:16:04.600
Inderjit Dhillon: one minus. Sorry.

145
00:16:05.470 --> 00:16:16.440
Inderjit Dhillon: Okay, so that's the difference. And we'll say, of course, that psi is are greater than equal to 0. This is, I equals 1, 2 through N.

146
00:16:16.810 --> 00:16:23.360
Inderjit Dhillon: But somehow I want to limit that the size is not too big or not too many.

147
00:16:23.760 --> 00:16:32.320
Inderjit Dhillon: So what I can then say is, I'll add a constraint, that summation of size I equals one through M

148
00:16:32.820 --> 00:16:34.930
Inderjit Dhillon: is less than equal to some cost.

149
00:16:38.860 --> 00:16:42.220
Inderjit Dhillon: Okay, so that's one kind of

150
00:16:42.340 --> 00:16:51.470
Inderjit Dhillon: way of writing the primal. Usually the way it is done is you actually take this. This is a constraint.

151
00:16:52.050 --> 00:16:56.380
Inderjit Dhillon: Okay. But this constant, this is a hyper parameter.

152
00:16:56.610 --> 00:16:59.910
Inderjit Dhillon: right? So instead of writing it like that

153
00:16:59.930 --> 00:17:05.760
Inderjit Dhillon: most of the time. You actually incorporate it into the constraint. Okay.

154
00:17:05.859 --> 00:17:10.319
Inderjit Dhillon: And now this minimization is over. W. W. Not

155
00:17:10.450 --> 00:17:12.730
Inderjit Dhillon: and also slides.

156
00:17:13.740 --> 00:17:22.599
Inderjit Dhillon: Okay, so what do I mean by incorporating it into the into the objective? Sorry. That's what I meant. Incorporated into the objective instead of as a constraint.

157
00:17:23.450 --> 00:17:31.510
Inderjit Dhillon: So the Svm Primal for the nonlinearly separable Svm. Is written as

158
00:17:33.220 --> 00:17:34.260
Inderjit Dhillon: of

159
00:17:35.030 --> 00:17:39.830
Inderjit Dhillon: W. 2 square. Okay, plus.

160
00:17:39.980 --> 00:17:50.820
Inderjit Dhillon: i'm actually going to think about. You know, Gamma, as like the Lagrange multiplier. and I'm going to move it into the constraint into the objective. Sorry.

161
00:17:53.320 --> 00:17:54.150
Inderjit Dhillon: Okay.

162
00:17:54.460 --> 00:18:03.660
Inderjit Dhillon: And then I have the constraints. And now now I'm going to write it as the inequality constraint less than equal to so one minus. I

163
00:18:03.980 --> 00:18:08.170
Inderjit Dhillon: to get it into like more of a standard form that we've dealt with so far

164
00:18:09.170 --> 00:18:14.780
Inderjit Dhillon: W. Transpose. Xi. Plus w naught is less than equal to 0.

165
00:18:15.040 --> 00:18:18.070
Inderjit Dhillon: I equals 1, 2 through M.

166
00:18:18.280 --> 00:18:27.180
Inderjit Dhillon: And now I have. I is greater than, or equal to 0, and I can rewrite that as

167
00:18:31.040 --> 00:18:38.680
Inderjit Dhillon: just because I want to have something which is less than equal to 0, I can write it as minus I I is less than equal to 0.

168
00:18:39.020 --> 00:18:40.790
Inderjit Dhillon: I equals one.

169
00:18:41.450 --> 00:18:48.630
Inderjit Dhillon: 2 through N: okay. So I I is greater than 0 is the same as minus. I I is less than equal to 0.

170
00:18:48.650 --> 00:18:58.710
Inderjit Dhillon: Okay. So I have this as the first constraint. and this as the second constraint. And this is the primal that we will work with.

171
00:19:03.680 --> 00:19:19.210
Inderjit Dhillon: Okay. So the saying that you know we are allowed to make kind of mistakes on the training data, right? We don't have to get it exactly, because maybe there is no linearly linear separator that separates class one from class 2.

172
00:19:19.590 --> 00:19:28.370
Inderjit Dhillon: Yeah. So now, if I, if you remember, we kind of went through some machinery to to.

173
00:19:28.450 --> 00:19:32.460
Inderjit Dhillon: you know, derive this dual from this primal.

174
00:19:32.740 --> 00:19:36.560
Inderjit Dhillon: So i'm actually just going to repeat it, but much more quickly.

175
00:19:36.580 --> 00:19:46.510
Inderjit Dhillon: Okay. So i'm going to start basically writing out a bunch of equations. So kind of be patient. Okay. So the lagrangian.

176
00:19:47.850 --> 00:19:55.530
Inderjit Dhillon: But you can, you know, follow. Follow, Follow with me. The lagrangian is the function of the primal variables

177
00:19:55.600 --> 00:20:01.590
Inderjit Dhillon: slide. and then I will have dual variables. Okay. So now I have 2 kinds of constraints

178
00:20:02.070 --> 00:20:07.820
Inderjit Dhillon: denoted by one and 2 above here, one here, 2 here.

179
00:20:07.940 --> 00:20:12.420
Inderjit Dhillon: So for this i'll use lagrange multipliers, alpha.

180
00:20:12.590 --> 00:20:24.110
Inderjit Dhillon: and for these i'll use lagrange multipliers me. So that means that my primal, variable or sorry dual variables or alpha. and you know Okay. So now.

181
00:20:24.280 --> 00:20:28.100
Inderjit Dhillon: what is my lagrangian? Well, let me write the objective.

182
00:20:29.120 --> 00:20:34.310
Inderjit Dhillon: That's gamma summation by I I one through M.

183
00:20:34.600 --> 00:20:39.020
Inderjit Dhillon: And then I have my like dual variables, right?

184
00:20:39.110 --> 00:20:41.190
Inderjit Dhillon: So alpha, I

185
00:20:41.460 --> 00:20:43.850
Inderjit Dhillon: I equals one through N.

186
00:20:44.130 --> 00:20:49.330
Inderjit Dhillon: And I basically copy this over here.

187
00:20:50.480 --> 00:20:54.650
Inderjit Dhillon: one minus I I minus y. I

188
00:20:55.160 --> 00:20:58.350
Inderjit Dhillon: W. Transpose X: I plus w not

189
00:21:01.320 --> 00:21:02.080
Inderjit Dhillon: yeah.

190
00:21:02.110 --> 00:21:07.910
Inderjit Dhillon: And then for the other set of. So for equation 2

191
00:21:07.950 --> 00:21:09.130
Inderjit Dhillon: I have

192
00:21:09.240 --> 00:21:13.200
Inderjit Dhillon: mu I as the corresponding lagrange multiplier.

193
00:21:13.520 --> 00:21:16.510
Inderjit Dhillon: So I have minus Mu: I

194
00:21:17.020 --> 00:21:18.100
Inderjit Dhillon: Oh, yeah.

195
00:21:19.250 --> 00:21:20.030
Inderjit Dhillon: okay.

196
00:21:21.120 --> 00:21:30.000
Inderjit Dhillon: So again, to kind of go through with the machinery. I need to take gradients of the lagrangian with respect to

197
00:21:30.430 --> 00:21:32.160
Inderjit Dhillon: the primal variables.

198
00:21:33.740 --> 00:21:36.780
Inderjit Dhillon: and there are these primal variables

199
00:21:38.850 --> 00:21:42.000
Inderjit Dhillon: W. W. Not, and psi.

200
00:21:42.400 --> 00:21:45.860
Inderjit Dhillon: So if I set this equal to 0,

201
00:21:47.320 --> 00:21:54.240
Inderjit Dhillon: I get that W. I. I'll again go quickly. Similarly, as before, W is equal to

202
00:21:54.300 --> 00:21:57.580
Inderjit Dhillon: alpha I Y. I X. I

203
00:21:57.610 --> 00:22:09.970
Inderjit Dhillon: I equals one through end. and just to make sure you understand how I got it. Well, the gradient of this is W. And then, if I look, there's no W here. There's only a W here.

204
00:22:10.210 --> 00:22:15.060
Inderjit Dhillon: and if I look at the gradient, it's alpha I y I x, i.

205
00:22:15.460 --> 00:22:22.880
Inderjit Dhillon: and it'll be with a minus sign, so I can equate it to 0. Move it to the other side. and that's how I get this.

206
00:22:25.240 --> 00:22:35.510
Inderjit Dhillon: Similarly, when I do gradient of with respect to W. Not equals 0. I get the we might remember the balancing constraint summation of Alpha I

207
00:22:35.800 --> 00:22:37.770
Inderjit Dhillon: y equals 0.

208
00:22:39.310 --> 00:22:41.820
Inderjit Dhillon: Okay. And again.

209
00:22:42.690 --> 00:22:47.270
Inderjit Dhillon: this is not W. Not here. No. W. Not here. W. Naught is here

210
00:22:47.530 --> 00:22:59.000
Inderjit Dhillon: the question of W. Naught is y minus y I alpha, I Okay, so that's why, when you take the gradient and it to 0, this is what you get. You get this balancing constraint.

211
00:22:59.120 --> 00:23:00.960
Inderjit Dhillon: Okay, off the dual variables.

212
00:23:01.150 --> 00:23:07.660
Inderjit Dhillon: When I said, the reason I say balancing, is this: this: what this means is that

213
00:23:09.610 --> 00:23:18.860
Inderjit Dhillon: since the Yi is a plus one or minus one, so the summation of Alpha I is for class one or the same as the summation of Alpha I 4 plus 2.

214
00:23:19.960 --> 00:23:20.620
Inderjit Dhillon: Okay.

215
00:23:21.310 --> 00:23:32.420
Inderjit Dhillon: And now I put the gradient with respect to size to be 0. Okay, and what do I get over here? Let's see. This is new. So let's look at this in more detail.

216
00:23:32.650 --> 00:23:36.690
Inderjit Dhillon: Most I over here? Well, there's Gamma that will come.

217
00:23:37.310 --> 00:23:40.670
Inderjit Dhillon: And then there's Alpha eyes and new lines.

218
00:23:41.750 --> 00:23:46.120
Inderjit Dhillon: Yeah. So if I do that, I will see that this is

219
00:23:46.350 --> 00:23:49.760
Inderjit Dhillon: that I have gamma minus

220
00:23:50.840 --> 00:23:55.640
Inderjit Dhillon: alpha I minus mu, I equals 0,

221
00:23:56.230 --> 00:23:58.330
Inderjit Dhillon: and this is for all

222
00:23:58.490 --> 00:24:05.480
Inderjit Dhillon: I equals one through n alpha I, which means that alpha I is gamma minus me.

223
00:24:08.350 --> 00:24:10.510
Inderjit Dhillon: So this is what is different than before.

224
00:24:12.860 --> 00:24:18.280
Inderjit Dhillon: And now again. to get the dual function I substitute

225
00:24:20.110 --> 00:24:23.320
Inderjit Dhillon: back into the Lagrangian.

226
00:24:29.160 --> 00:24:32.800
Inderjit Dhillon: and I get. If I substitute back into the Lagrangian, well

227
00:24:33.260 --> 00:24:45.820
Inderjit Dhillon: remember that this is the lagrangian I have to substitute W over here W over here. and then collect all these terms that, say I,

228
00:24:46.330 --> 00:24:48.980
Inderjit Dhillon: and see what happens. Okay.

229
00:24:50.310 --> 00:24:59.540
Inderjit Dhillon: So substituting back into the lagrangian, I get half of W. Square. But W is. or norm of W. That is

230
00:24:59.790 --> 00:25:07.220
Inderjit Dhillon: Alpha I Y. I X. I norm Square. I equals one through m

231
00:25:08.850 --> 00:25:16.690
Inderjit Dhillon: plus. I have the second term. which is this one. Okay.

232
00:25:17.110 --> 00:25:22.800
Inderjit Dhillon: but let me just collect all the terms, all the things which I have to say. I

233
00:25:22.990 --> 00:25:26.020
Inderjit Dhillon: there's minus Alpha, I say I here

234
00:25:26.160 --> 00:25:29.430
Inderjit Dhillon: and there's a minus Mu, I so I here right

235
00:25:29.550 --> 00:25:31.830
Inderjit Dhillon: so I can write this as

236
00:25:32.290 --> 00:25:36.880
Inderjit Dhillon: summation of I equals one through, and

237
00:25:37.840 --> 00:25:45.870
Inderjit Dhillon: why I gamma minus alpha I minus. Okay, so remember this came from.

238
00:25:48.040 --> 00:25:51.960
Inderjit Dhillon: Let's see. Oh. gamma

239
00:25:52.940 --> 00:25:55.880
Inderjit Dhillon: minus Alpha, I minus me. I

240
00:25:56.220 --> 00:25:59.950
Inderjit Dhillon: Okay. And then I get

241
00:26:01.220 --> 00:26:02.340
Inderjit Dhillon: plus

242
00:26:04.630 --> 00:26:06.290
Inderjit Dhillon: this term over here.

243
00:26:08.280 --> 00:26:14.760
Inderjit Dhillon: We've already done. Si, I term. So it's basically Alpha I one and Alpha I times this.

244
00:26:15.120 --> 00:26:19.940
Inderjit Dhillon: and we've already taken care of this. So i'll separate them out.

245
00:26:19.990 --> 00:26:23.930
Inderjit Dhillon: Or or maybe let's see. Yeah, let's separate them out.

246
00:26:24.120 --> 00:26:36.660
Inderjit Dhillon: Alpha I I equals one through m. and then the other term is minus. I equals one through n alpha I y I

247
00:26:37.270 --> 00:26:41.140
Inderjit Dhillon: x. I transpose times

248
00:26:42.080 --> 00:26:55.520
Inderjit Dhillon: w right, and remember that w is summation of Alpha. Let me just write it as alpha K. Y. K Xk: because I've already used I:

249
00:26:56.470 --> 00:26:59.980
Inderjit Dhillon: Okay. So just to make sure

250
00:27:00.030 --> 00:27:11.360
Inderjit Dhillon: that you are on the same page. Basically this is W. Transpose X: I. So Xi. Transpose W. Is the same as W. Transpose. Xi: so this is Xi. Transpose times. W:

251
00:27:12.310 --> 00:27:14.720
Inderjit Dhillon: Okay, and that W. Remember, came from here.

252
00:27:15.800 --> 00:27:16.570
Inderjit Dhillon: Okay.

253
00:27:17.970 --> 00:27:21.690
Inderjit Dhillon: Now, let's simplify.

254
00:27:24.840 --> 00:27:26.570
Inderjit Dhillon: Okay. So

255
00:27:26.650 --> 00:27:31.760
Inderjit Dhillon: if you look at these 2 terms this term and this term

256
00:27:32.480 --> 00:27:39.470
Inderjit Dhillon: well, this is actually exactly the same as this, except there is a one half over here and there, the minus one over here.

257
00:27:40.850 --> 00:27:48.300
Inderjit Dhillon: So when I combine these 2 terms when I combine this term with this term.

258
00:27:48.410 --> 00:27:50.100
Inderjit Dhillon: I essentially get

259
00:27:51.930 --> 00:27:54.800
Inderjit Dhillon: minus one over 2 off

260
00:27:55.720 --> 00:27:58.150
Inderjit Dhillon: Alpha I Y. I

261
00:27:58.180 --> 00:28:01.430
Inderjit Dhillon: excellent. I cool one

262
00:28:01.440 --> 00:28:04.980
chitrank: professor software in adoption. Where did the W. Not?

263
00:28:07.480 --> 00:28:09.710
Where did the W. Not term go.

264
00:28:09.780 --> 00:28:13.120
Inderjit Dhillon: Yeah. So remember that it's as we did before.

265
00:28:18.470 --> 00:28:32.810
Inderjit Dhillon: Let me see. Did I miss the W. Not term? Yes, I did miss it right. But remember that that w not term is I mean, I I think there's the reason why I kind of may have left it out, but

266
00:28:33.050 --> 00:28:37.720
Inderjit Dhillon: but you're right to be correct. I want to have

267
00:28:38.460 --> 00:28:41.490
Inderjit Dhillon: Alpha I Why, I

268
00:28:43.750 --> 00:28:44.560
Inderjit Dhillon: correct.

269
00:28:46.050 --> 00:28:46.980
chitrank: Yeah.

270
00:28:47.250 --> 00:28:51.520
Inderjit Dhillon: right? So I have. What is it minus

271
00:28:53.410 --> 00:28:59.690
Inderjit Dhillon: minus?

272
00:29:00.270 --> 00:29:01.230
Inderjit Dhillon: Thank you.

273
00:29:02.840 --> 00:29:06.790
Inderjit Dhillon: but not a big deal right? Because

274
00:29:07.850 --> 00:29:10.540
Inderjit Dhillon: this is actually equal to 0, right?

275
00:29:12.480 --> 00:29:18.830
Inderjit Dhillon: Because I thought it. Yeah, W: Not this constant and some missions. Okay, because it comes from here.

276
00:29:22.510 --> 00:29:23.310
Inderjit Dhillon: Okay?

277
00:29:24.300 --> 00:29:26.970
Inderjit Dhillon: And similarly you'll see that

278
00:29:26.980 --> 00:29:28.860
Inderjit Dhillon: because of this.

279
00:29:32.540 --> 00:29:38.400
Inderjit Dhillon: this is over here. and this term is also equal to 0.

280
00:29:43.130 --> 00:29:47.330
Inderjit Dhillon: So all the things in the red disappear. And so what is left is

281
00:29:48.530 --> 00:29:53.940
Inderjit Dhillon: this term plus summation of Alpha I. I equals one through it.

282
00:29:58.110 --> 00:30:01.920
Inderjit Dhillon: So that becomes kind of the dual objective.

283
00:30:02.790 --> 00:30:03.880
Inderjit Dhillon: Is that clear.

284
00:30:07.610 --> 00:30:11.360
Inderjit Dhillon: Okay. So the dual problem becomes the following.

285
00:30:15.280 --> 00:30:21.220
Inderjit Dhillon: it's actually very similar to what we did before. All right, we'll. We'll see that there is one twist to it

286
00:30:22.640 --> 00:30:27.430
Inderjit Dhillon: with. So the dual problem is maximize, what are my dual variables.

287
00:30:27.770 --> 00:30:36.470
Inderjit Dhillon: you know, Alpha and Mu. So it is summation of Alpha. I I equals one through and

288
00:30:36.970 --> 00:30:40.140
Inderjit Dhillon: minus one half of

289
00:30:40.980 --> 00:30:43.450
Inderjit Dhillon: this this store.

290
00:30:44.150 --> 00:30:51.090
Inderjit Dhillon: Okay, and I'm just gonna rewrite that in terms of so that the inner products become clear.

291
00:30:51.480 --> 00:30:53.370
Inderjit Dhillon: And it is so

292
00:30:54.720 --> 00:30:59.940
Inderjit Dhillon: let me write it in terms of J. Alpha I alpha J.

293
00:31:01.340 --> 00:31:07.680
Inderjit Dhillon: There are 2 summation signs. I equals one through M. J. Equals one to one

294
00:31:08.120 --> 00:31:18.180
Inderjit Dhillon: Y I Yj. Xi transpose x 2, and then the constraints are that the dual variables are feasible.

295
00:31:18.520 --> 00:31:22.090
Inderjit Dhillon: which means that alpha I is greater than or equal to 0

296
00:31:22.830 --> 00:31:26.320
Inderjit Dhillon: and Mu I is greater than, or equal to 0.

297
00:31:26.450 --> 00:31:32.500
Inderjit Dhillon: And there is this balancing constraint which is Alpha I y I equal to.

298
00:31:40.220 --> 00:31:43.780
Inderjit Dhillon: and notice that the new term only appears over here.

299
00:31:45.820 --> 00:31:50.380
Inderjit Dhillon: Okay, it doesn't appear anywhere else. But if I look at the mute term

300
00:31:52.550 --> 00:31:56.530
Inderjit Dhillon: right, Alpha I is actually gamma minus mu I

301
00:31:58.480 --> 00:32:10.890
Inderjit Dhillon: so alpha I is equal to gamma minus me. I mu I is greater than or equal to 0. So I can actually just this this, and this implies.

302
00:32:11.040 --> 00:32:14.800
Inderjit Dhillon: then Alpha I is less than equal to. Yeah.

303
00:32:24.730 --> 00:32:37.820
Inderjit Dhillon: So earlier we just had Alpha I greater than equal. greater than or equal to 0. But now I also have an upper bound on the dual variables, which means that Alpha is less than equal to 0,

304
00:32:38.340 --> 00:32:49.610
Inderjit Dhillon: and they essentially arise from the Mu. I is greater than or equal to 0, which is the Lagrange multiplier for those like variables.

305
00:32:50.140 --> 00:32:55.030
Inderjit Dhillon: Okay, As a result, I can simplify this, and I can write my Svm. Dual

306
00:32:56.460 --> 00:32:59.040
Inderjit Dhillon: as maximize

307
00:32:59.490 --> 00:33:05.760
Inderjit Dhillon: over out for because i'm going to eliminate because we've eliminated the mu eyes.

308
00:33:06.440 --> 00:33:11.600
Inderjit Dhillon: I equals one through n alpha I minus one half

309
00:33:12.070 --> 00:33:14.960
Inderjit Dhillon: summation. I equals one through N.

310
00:33:16.040 --> 00:33:18.250
Inderjit Dhillon: J. Equals one through m

311
00:33:19.270 --> 00:33:24.120
Inderjit Dhillon: alpha I alpha j Y. I

312
00:33:24.590 --> 00:33:28.840
Inderjit Dhillon: xi transpose x day such that

313
00:33:30.320 --> 00:33:39.950
Inderjit Dhillon: I can write in the dual feasibility now as following that 0 is less than or equal to Alpha I is less than equal to gamma. So Alpha is, or sandwiched

314
00:33:40.240 --> 00:33:43.410
Inderjit Dhillon: between 0 and Gamma.

315
00:33:43.880 --> 00:33:53.760
Inderjit Dhillon: This is, for I equals one to through n. and then I have that balancing constraint that Alpha I y I equals one through and

316
00:33:54.070 --> 00:33:55.720
Inderjit Dhillon: is equal to 0.

317
00:33:57.290 --> 00:33:59.270
Inderjit Dhillon: So this is my

318
00:34:00.780 --> 00:34:02.100
Inderjit Dhillon: Svm. To.

319
00:34:05.640 --> 00:34:16.040
Inderjit Dhillon: And if you compare it with the non ability, the linearly separable, it's actually kind of remarkable right that it almost looks exactly the same

320
00:34:16.730 --> 00:34:21.210
Inderjit Dhillon: as this. except that this is changed

321
00:34:21.460 --> 00:34:24.860
Inderjit Dhillon: to not only be greater than equal to 0,

322
00:34:25.719 --> 00:34:29.159
Inderjit Dhillon: but to be less than equal to Gamma.

323
00:34:31.070 --> 00:34:34.330
Inderjit Dhillon: Okay. So that turns out to be the only difference

324
00:34:34.360 --> 00:34:37.760
Inderjit Dhillon: in terms of equations. But of course, in terms of

325
00:34:37.800 --> 00:34:46.880
Inderjit Dhillon: what the Svm. Can do. It's a big big difference, right in terms. Now, we can basically handle any binary classification data set.

326
00:34:47.550 --> 00:34:52.020
Inderjit Dhillon: So now let's again look at what this means right in the over here.

327
00:34:52.449 --> 00:34:56.770
Inderjit Dhillon: when it was linearly separable. We had given this nice interpretation

328
00:34:57.110 --> 00:34:58.060
Inderjit Dhillon: that

329
00:34:58.200 --> 00:34:59.520
Inderjit Dhillon: oh, sorry.

330
00:35:03.680 --> 00:35:10.510
Inderjit Dhillon: this nice interpretation that you know the support vector machine support vectors are just the ones which actually are on the boundary.

331
00:35:11.880 --> 00:35:13.960
Inderjit Dhillon: Okay, but what about now?

332
00:35:17.200 --> 00:35:24.910
Inderjit Dhillon: And if you remember, we had looked at this thing called complementary slackness. Right? So let's look at the complementary slightness. Conditions.

333
00:35:26.460 --> 00:35:30.840
chitrank: Professor, I have confusion here. Why, you.

334
00:35:30.910 --> 00:35:33.120
chitrank: gamma included like

335
00:35:33.240 --> 00:35:37.810
chitrank: Gamma, is also a a dual variable right?

336
00:35:37.970 --> 00:35:39.760
Inderjit Dhillon: No, it wasn't the primal

337
00:35:41.810 --> 00:35:44.820
Inderjit Dhillon: right, if you remember what we had done is we had

338
00:35:45.420 --> 00:35:52.030
Inderjit Dhillon: basically taking this over here, and instead of this constant, we are. Basically

339
00:35:52.210 --> 00:35:53.660
Inderjit Dhillon: Do this over here?

340
00:35:54.700 --> 00:35:56.610
Inderjit Dhillon: Okay. Got it?

341
00:35:58.210 --> 00:36:02.670
Inderjit Dhillon: Okay. So the complementary slightest conditions are that

342
00:36:03.150 --> 00:36:05.370
Inderjit Dhillon: I have Alpha I.

343
00:36:06.280 --> 00:36:11.230
Inderjit Dhillon: So so again, remember the complimentary like this: conditions. Basically, look you look at the primal.

344
00:36:12.550 --> 00:36:14.930
Inderjit Dhillon: So this is the primal over here.

345
00:36:16.640 --> 00:36:30.410
Inderjit Dhillon: And i'm basically going to say that the product of the dual Lagrange multiplier times. This must be equal to 0 product of the Lagrange multiplier times. This must be equal to 0. What will happen is that Alpha I times this must be equal to 0.

346
00:36:30.720 --> 00:36:37.590
Inderjit Dhillon: And Mu i. Times. P. I must be equal to 0. So let me write that down, and we'll see what what that implies.

347
00:36:38.710 --> 00:36:44.660
Inderjit Dhillon: Alpha I times one minus I minus y. I

348
00:36:45.790 --> 00:36:49.110
Inderjit Dhillon: w transpose xi plus w Naught

349
00:36:50.230 --> 00:36:52.990
Inderjit Dhillon: must be equal to 0 at the optimal.

350
00:36:54.070 --> 00:36:58.360
Inderjit Dhillon: Oh, yeah, Remember, we are only talking about at the optimal. Okay. So that's

351
00:36:59.190 --> 00:37:01.830
Inderjit Dhillon: well. Let me call it.

352
00:37:02.330 --> 00:37:05.300
Inderjit Dhillon: I don't think I need it. But i'm just calling it to question one.

353
00:37:05.600 --> 00:37:09.700
Inderjit Dhillon: And then I have mu. I I equals 0.

354
00:37:10.150 --> 00:37:13.060
Inderjit Dhillon: I equals one to

355
00:37:15.230 --> 00:37:22.330
Inderjit Dhillon: Okay. So now let's again kind of draw, so that we get some geometric intuition. Right? So let's see.

356
00:37:22.460 --> 00:37:24.300
Inderjit Dhillon: I have pluses

357
00:37:28.950 --> 00:37:31.740
Inderjit Dhillon: over here, and then I have a few pluses here.

358
00:37:33.850 --> 00:37:39.380
Inderjit Dhillon: and then I have minuses. This is the second class. I have them here.

359
00:37:40.610 --> 00:37:46.340
Inderjit Dhillon: and I have some minuses here. Okay. this is kind of the

360
00:37:47.670 --> 00:37:49.900
Inderjit Dhillon: maximum margin hyperplane.

361
00:37:51.330 --> 00:37:53.640
Inderjit Dhillon: Okay? And

362
00:37:59.270 --> 00:38:02.960
Inderjit Dhillon: This is the kind of the boundary.

363
00:38:05.320 --> 00:38:05.980
Inderjit Dhillon: Okay.

364
00:38:06.390 --> 00:38:09.270
Inderjit Dhillon: as before, this is

365
00:38:10.460 --> 00:38:14.110
Inderjit Dhillon: one divided by norm of W.

366
00:38:15.640 --> 00:38:16.330
Inderjit Dhillon: Okay.

367
00:38:20.730 --> 00:38:27.310
Inderjit Dhillon: maybe i'll use a different color. Let's have fun with colors. So I have pluses. So these ones

368
00:38:28.490 --> 00:38:31.000
Inderjit Dhillon: or green they're kind of.

369
00:38:32.430 --> 00:38:34.470
Inderjit Dhillon: you know, correctly classified.

370
00:38:35.800 --> 00:38:37.850
Inderjit Dhillon: which means that

371
00:38:38.920 --> 00:38:41.620
Inderjit Dhillon: this is

372
00:38:43.410 --> 00:38:48.160
Inderjit Dhillon: not met with the quality and the size of actually not equal to 0.

373
00:38:50.910 --> 00:39:00.560
Inderjit Dhillon: Okay. So this means that these points have all the green points. Oh. Alpha, I equal to 0

374
00:39:09.380 --> 00:39:10.810
Inderjit Dhillon: As before

375
00:39:12.340 --> 00:39:14.200
Inderjit Dhillon: we have

376
00:39:15.310 --> 00:39:19.440
Inderjit Dhillon: points on the boundary, let's label them as pink.

377
00:39:27.880 --> 00:39:32.040
Inderjit Dhillon: Okay. And these are Alpha I,

378
00:39:33.320 --> 00:39:36.830
Inderjit Dhillon: or greater than 0.

379
00:39:39.690 --> 00:39:44.590
Inderjit Dhillon: Okay, because there the inequality in inequality and has an inequality.

380
00:39:44.850 --> 00:39:46.620
Inderjit Dhillon: With this I is equal to 0.

381
00:39:48.690 --> 00:39:53.550
Inderjit Dhillon: Okay. But what about these points? Now? The ones which are

382
00:39:54.230 --> 00:40:00.190
Inderjit Dhillon: kind of misclassified. So this is a minus. This is this classified. This is a minus. This is misclassified

383
00:40:00.520 --> 00:40:06.560
Inderjit Dhillon: over here. This is the plus that's misqualified over here. The I is are going to be greater than 0,

384
00:40:08.480 --> 00:40:12.670
Inderjit Dhillon: which means that the Mu I's are going to be equal to 0.

385
00:40:13.700 --> 00:40:16.670
Inderjit Dhillon: The Mu I equal to 0 means

386
00:40:17.370 --> 00:40:19.970
Inderjit Dhillon: that Alpha I will be actually gamma.

387
00:40:24.870 --> 00:40:29.960
Inderjit Dhillon: Okay. this means that these are kind of the points in

388
00:40:30.350 --> 00:40:37.160
Inderjit Dhillon: in red. These are misclassified. So let's see pluses over here

389
00:40:38.920 --> 00:40:40.970
Inderjit Dhillon: and minus is over here.

390
00:40:42.280 --> 00:40:44.220
Inderjit Dhillon: These points have been

391
00:40:46.510 --> 00:40:48.150
Inderjit Dhillon: misclassified.

392
00:40:51.680 --> 00:40:58.860
Inderjit Dhillon: which means that the side eyes. or greater than greater than 0,

393
00:41:01.270 --> 00:41:02.870
Inderjit Dhillon: greater than 0,

394
00:41:03.950 --> 00:41:12.520
Inderjit Dhillon: which means that the Mu, i's are equal to 0. Which means that Alpha I, which is gamma minus me. I

395
00:41:13.560 --> 00:41:15.380
Inderjit Dhillon: right. We have.

396
00:41:16.190 --> 00:41:18.230
Inderjit Dhillon: I

397
00:41:19.810 --> 00:41:21.450
Inderjit Dhillon: is equal to G.

398
00:41:26.540 --> 00:41:29.010
Inderjit Dhillon: Okay, and remember that.

399
00:41:34.880 --> 00:41:40.830
Inderjit Dhillon: Okay, Remember that, Alpha I it's all sorry. W:

400
00:41:41.820 --> 00:41:47.630
Inderjit Dhillon: The optimal W. Is summation of I equals one through N.

401
00:41:48.260 --> 00:41:51.400
Inderjit Dhillon: Alpha I. Y. I. Xi.

402
00:41:54.370 --> 00:42:06.010
Inderjit Dhillon: Okay. So now, what this means is that the alpha eyes are not. or non-zero. for all the points on the boundary and all the misclassified points.

403
00:42:06.260 --> 00:42:15.740
Inderjit Dhillon: But the misclassified point can't have too big a gamma to bigger, coefficient. It is actually restricted to be less than equal to the

404
00:42:22.020 --> 00:42:24.840
Inderjit Dhillon: Okay. So let me kind of

405
00:42:25.900 --> 00:42:32.770
Inderjit Dhillon: write this out, and this is kind of the final form of the Yes, we am primal.

406
00:42:35.650 --> 00:42:40.150
Inderjit Dhillon: It is minimize one half

407
00:42:40.510 --> 00:42:48.690
Inderjit Dhillon: w square plus gamma summation outside. I I equals one through, and

408
00:42:49.240 --> 00:42:53.990
Inderjit Dhillon: that's that. Phi. I is greater than or equal to 0.

409
00:42:54.310 --> 00:43:03.770
Inderjit Dhillon: I quote 1, 2 through n and one minus. I I i'm sorry one minus I

410
00:43:06.250 --> 00:43:14.800
Inderjit Dhillon: minus y. I w transpose x I plus w naught is less than equal to the

411
00:43:18.140 --> 00:43:20.090
Inderjit Dhillon: and then there's V and dual

412
00:43:24.420 --> 00:43:26.930
Inderjit Dhillon: is you maximize

413
00:43:27.950 --> 00:43:34.700
Inderjit Dhillon: over a summation of Alpha I I equals one through m, you know, maximizing over the Alphas

414
00:43:35.230 --> 00:43:39.990
Inderjit Dhillon: minus one half. I have a some double sum, I

415
00:43:40.380 --> 00:43:46.620
Inderjit Dhillon: one through one Alpha I Alpha J.

416
00:43:48.390 --> 00:43:50.610
Inderjit Dhillon: Y. I.

417
00:43:54.710 --> 00:43:58.990
Inderjit Dhillon: So the inner products come into the play here, and the constraints are

418
00:43:59.140 --> 00:44:02.170
Inderjit Dhillon: the 0 is less than or equal to Alpha I

419
00:44:03.930 --> 00:44:07.220
Inderjit Dhillon: i'll fly a sandwich between 0 and gamma.

420
00:44:08.310 --> 00:44:10.470
Inderjit Dhillon: I is one through N.

421
00:44:11.490 --> 00:44:14.190
Inderjit Dhillon: Such that alpha Iy I.

422
00:44:18.580 --> 00:44:21.240
Inderjit Dhillon: Okay. So this is this: we have final.

423
00:44:25.510 --> 00:44:27.400
Inderjit Dhillon: And this is the

424
00:44:39.610 --> 00:44:41.740
Inderjit Dhillon: okay, any questions so far.

425
00:44:41.960 --> 00:44:49.320
Inderjit Dhillon: that kind of concludes the nonlinear. Dsm: I'll actually write it in a little bit more digestible form in a minute.

426
00:44:52.770 --> 00:45:02.060
Inderjit Dhillon: Okay. So you can see that you know it's a little bit messy. I've had to carry it on all these coefficients around all the indices around like Alpha I Alpha J.

427
00:45:02.350 --> 00:45:14.990
Inderjit Dhillon: So many times linear algebra can actually help in simplifying, or you know, at least in writing it out. Okay, so what I can do is I can define just like we did before my training data matrix

428
00:45:15.300 --> 00:45:18.330
Inderjit Dhillon: as X, one X, 2

429
00:45:18.610 --> 00:45:24.970
Inderjit Dhillon: through X and and I can define the diagonal label matrix

430
00:45:26.220 --> 00:45:28.840
Inderjit Dhillon: as diagonal, which has

431
00:45:29.870 --> 00:45:35.420
Inderjit Dhillon: either plus one or minus one on it, depending upon which class the

432
00:45:37.330 --> 00:45:44.010
Inderjit Dhillon: Xi belongs to. Okay. And if you think about the matrix X, transpose X:

433
00:45:44.750 --> 00:45:47.060
Inderjit Dhillon: Okay. And the I get element.

434
00:45:47.530 --> 00:45:59.980
Inderjit Dhillon: This is exactly equal to Xi. Transpose X 2. Okay, and many times this matrix of in our products is also called the Round Matrix.

435
00:46:01.910 --> 00:46:07.520
Inderjit Dhillon: Okay, I'll just make one more notation, right? I'll represent all one specter

436
00:46:07.890 --> 00:46:09.110
Inderjit Dhillon: with E.

437
00:46:09.950 --> 00:46:14.490
Inderjit Dhillon: So e is the vector where all the components are plus ones.

438
00:46:15.280 --> 00:46:19.800
Inderjit Dhillon: Okay? And then i'll have e transpose to be equal to the row, vector

439
00:46:20.960 --> 00:46:22.320
Inderjit Dhillon: which is all ones.

440
00:46:24.260 --> 00:46:30.650
Inderjit Dhillon: Okay, With that it turns out that I can write the dual in a much more compact form, and i'll do that.

441
00:46:31.600 --> 00:46:34.690
Inderjit Dhillon: I can write it as maximum or alpha

442
00:46:35.770 --> 00:46:41.920
Inderjit Dhillon: summation of Alpha I. I can write as inner product of E and E with

443
00:46:42.150 --> 00:46:43.040
Inderjit Dhillon: awful.

444
00:46:44.690 --> 00:46:48.480
Inderjit Dhillon: And then this complicated thing, this double sum

445
00:46:48.940 --> 00:47:01.400
Inderjit Dhillon: I can actually invite as minus one half right. That's the same as one half over here. But this complicated thing I can actually write more compactly as

446
00:47:01.680 --> 00:47:10.310
Inderjit Dhillon: a quadratic form alpha transpose y transpose X. Transpose X. Why in time, cell phone.

447
00:47:12.040 --> 00:47:18.580
Inderjit Dhillon: and then what I have is that this vector of Alphas is between

448
00:47:20.720 --> 00:47:22.490
Inderjit Dhillon: 0 and no.

449
00:47:22.970 --> 00:47:29.040
Inderjit Dhillon: So here this is the 0. Vector this is the all identity, all one's. Vector

450
00:47:29.060 --> 00:47:34.210
Inderjit Dhillon: and then this balancing constraint. Also I can write as Alpha transpose Y equals

451
00:47:35.320 --> 00:47:38.000
Inderjit Dhillon: so much kind of easier to write this out.

452
00:47:39.870 --> 00:47:43.260
Inderjit Dhillon: Okay. And sometimes you say that this matrix is

453
00:47:44.750 --> 00:47:53.150
Inderjit Dhillon: Q. And then it basically becomes, You know this will become e transpose alpha minus

454
00:47:53.730 --> 00:47:57.690
Inderjit Dhillon: one half alpha transpose because it's constant for the data set.

455
00:47:59.760 --> 00:48:01.730
Inderjit Dhillon: Okay, this will become this.

456
00:48:07.060 --> 00:48:09.510
Inderjit Dhillon: But this is the do.

457
00:48:10.830 --> 00:48:13.930
Inderjit Dhillon: You can write the objective either in this way.

458
00:48:14.710 --> 00:48:17.130
Inderjit Dhillon: all this even more compact from

459
00:48:20.030 --> 00:48:26.220
Inderjit Dhillon: Okay. So one of the reasons why I have done this. There are 2 reasons. Okay, is to

460
00:48:29.430 --> 00:48:33.340
Inderjit Dhillon: let's talk about the the

461
00:48:35.700 --> 00:48:46.460
Inderjit Dhillon: the and and let's think about. You know what it means that all this can be expressed just in terms. You know all that matters over. Here are the inner products between X's and

462
00:48:46.880 --> 00:48:55.680
Inderjit Dhillon: between the access themselves. Okay. So let's talk a little bit about thermal methods.

463
00:49:02.930 --> 00:49:13.850
Inderjit Dhillon: One of the data sets that you've looked at, and it's actually in your homework is this: concentric circles data set right where I have one last, which is.

464
00:49:14.630 --> 00:49:17.790
Inderjit Dhillon: belongs to one contrary concentric circle.

465
00:49:18.530 --> 00:49:20.300
Inderjit Dhillon: And then there's another

466
00:49:21.580 --> 00:49:28.750
Inderjit Dhillon: plus that belongs to like the surrounding concentric cycle, the bigger concentric cycle.

467
00:49:30.560 --> 00:49:42.800
Inderjit Dhillon: This is like a synthetic data set, but the idea behind this is well, clearly. it is null linearly separable. The classes are not linearly simple.

468
00:49:43.870 --> 00:49:47.090
Inderjit Dhillon: Okay? And you know.

469
00:49:47.280 --> 00:49:51.260
Inderjit Dhillon: The question is, what is a good separating surface? Well, it's clear

470
00:49:51.650 --> 00:49:55.660
Inderjit Dhillon: that this would be

471
00:49:56.860 --> 00:50:01.770
Inderjit Dhillon: good service, a good decision surface that separate the circles

472
00:50:01.800 --> 00:50:03.520
Inderjit Dhillon: from the pluses.

473
00:50:05.440 --> 00:50:08.420
Inderjit Dhillon: Okay? And what is the equation of this?

474
00:50:10.270 --> 00:50:15.450
Inderjit Dhillon: Well, the equation of this is, you know. Suppose I'm just in 2 dimensions, right? So

475
00:50:15.480 --> 00:50:19.000
Inderjit Dhillon: my data points are, you know, X

476
00:50:19.510 --> 00:50:22.640
Inderjit Dhillon: is x, one x 2.

477
00:50:23.010 --> 00:50:30.010
Inderjit Dhillon: Those are my data points right in general. Of course, X will belong to Rd right here, these 2.

478
00:50:30.290 --> 00:50:34.560
Inderjit Dhillon: So this surface is just the equation of a circle

479
00:50:34.880 --> 00:50:39.420
Inderjit Dhillon: with the appropriate radius. Right. X. One's, 2, x, one square.

480
00:50:39.690 --> 00:50:42.570
Inderjit Dhillon: plus x, 2 square equals all.

481
00:50:50.280 --> 00:50:52.280
Inderjit Dhillon: But this is not linear

482
00:50:55.040 --> 00:51:00.240
Inderjit Dhillon: in X it is a nonlinear surface. What

483
00:51:02.230 --> 00:51:11.390
Inderjit Dhillon: but what one can do? And this is kind of the idea behind the first idea behind kernel methods is that I can take my pre or input space.

484
00:51:12.640 --> 00:51:14.870
Inderjit Dhillon: My inputs are given in this form.

485
00:51:15.560 --> 00:51:27.070
Inderjit Dhillon: but I can transform them into what's called a feature space by saying that this is V effects. And let's do one particular transformation right, which is.

486
00:51:27.720 --> 00:51:33.030
Inderjit Dhillon: I would like to construct more features than the input space by taking

487
00:51:33.240 --> 00:51:39.200
Inderjit Dhillon: either retaining the features as they are, or taking crosses of features. So let me see what I mean.

488
00:51:39.600 --> 00:51:41.050
Inderjit Dhillon: I have one.

489
00:51:41.630 --> 00:51:47.620
Inderjit Dhillon: I can have square root, 2 x, one square root, 2 x, 2 the square root kind of don't.

490
00:51:47.720 --> 00:51:53.220
Inderjit Dhillon: What are you too much about that part? And then I can have X, one square x, 2 square.

491
00:51:53.880 --> 00:51:57.540
Inderjit Dhillon: and then I have a cross feature, square root, 2 x, one x, 2

492
00:52:02.220 --> 00:52:04.960
Inderjit Dhillon: to note that, you know. Initially.

493
00:52:05.530 --> 00:52:07.810
Inderjit Dhillon: my input space was 2 dimensional.

494
00:52:08.790 --> 00:52:13.900
Inderjit Dhillon: and now it is 1 2 3 4 5 6 dimensional.

495
00:52:14.840 --> 00:52:28.600
Inderjit Dhillon: Okay. I kind of have x, one and X 2. Still as variables our features. But now, of course, I have a constant one, and then I have x, one square x, 2 square

496
00:52:29.170 --> 00:52:31.880
Inderjit Dhillon: and square root, 2 X, One, x, 2,

497
00:52:33.180 --> 00:52:36.630
Inderjit Dhillon: and then the crucial observation is that even though

498
00:52:37.830 --> 00:52:40.360
Inderjit Dhillon: if I come back to this particular example.

499
00:52:41.970 --> 00:52:46.770
Inderjit Dhillon: this surface is this decision surface as an orientation in X,

500
00:52:47.460 --> 00:52:48.610
Inderjit Dhillon: but

501
00:52:50.050 --> 00:52:52.430
Inderjit Dhillon: it is linear

502
00:52:54.260 --> 00:52:56.020
Inderjit Dhillon: and few effects.

503
00:53:08.240 --> 00:53:09.050
Inderjit Dhillon: Okay.

504
00:53:10.360 --> 00:53:23.940
Inderjit Dhillon: So this is what this is suggesting is that, hey? You know, I have these linear Svm: so remember, this was still a linear Svm: right: what I have discussed over here. This is a linear Svm: I'm: taking the inner product over here.

505
00:53:24.980 --> 00:53:25.870
Inderjit Dhillon: Okay.

506
00:53:26.420 --> 00:53:29.080
Inderjit Dhillon: if I take this data set

507
00:53:29.850 --> 00:53:40.980
Inderjit Dhillon: and give this to the linear Svm. There is really no home, because all it will do is try to find a separating hyper plan a linear surface.

508
00:53:41.150 --> 00:53:45.150
Inderjit Dhillon: But there is no separating hyperplane kind of available.

509
00:53:47.850 --> 00:53:53.230
Inderjit Dhillon: Okay, so sorry. Maggie asked. Where did I get it from? I mean. I just

510
00:53:54.130 --> 00:53:57.900
Inderjit Dhillon: pull it out of the hat right now, right so so we'll see where it comes from.

511
00:53:57.930 --> 00:54:02.780
Inderjit Dhillon: But the question is, you know, where did this to come from?

512
00:54:05.340 --> 00:54:12.280
Inderjit Dhillon: Where did this turn come from? I I basically have said that suppose you had this as the input, space

513
00:54:12.630 --> 00:54:18.190
Inderjit Dhillon: i'm going to construct a feature space from this. Okay, so this is the feature space.

514
00:54:22.680 --> 00:54:24.260
Inderjit Dhillon: So if you give me

515
00:54:26.260 --> 00:54:31.400
Inderjit Dhillon: and X one X 2, I could determine the domestically construct this.

516
00:54:33.100 --> 00:54:46.160
Inderjit Dhillon: Okay? And what I'm saying is, suppose that I constructed this feature space. then my separating surface over here, which is a circle. is linear an X one in in V of X.

517
00:54:48.840 --> 00:54:52.440
Inderjit Dhillon: So what that means is it gives us a recipe. Right?

518
00:54:53.070 --> 00:55:00.190
Inderjit Dhillon: Okay. And now how to construct this new feature space? That's a quick question. But to solve this particular problem

519
00:55:00.780 --> 00:55:04.590
Inderjit Dhillon: that you have okay to solve this particular problem.

520
00:55:07.360 --> 00:55:09.650
Inderjit Dhillon: you can construct this feature space.

521
00:55:10.770 --> 00:55:14.670
Inderjit Dhillon: and then you can solve this problem. That's the problem.

522
00:55:17.530 --> 00:55:19.550
Inderjit Dhillon: because that will give you a linear

523
00:55:20.190 --> 00:55:23.150
Inderjit Dhillon: decision. Surface in the feature space

524
00:55:23.640 --> 00:55:28.530
Inderjit Dhillon: and in the feature Space X. One square is a feature X 2 square is a feature.

525
00:55:28.850 --> 00:55:31.880
Inderjit Dhillon: So it's just linear in fear of it.

526
00:55:37.060 --> 00:55:43.910
Inderjit Dhillon: Okay. So that's one recipe. Okay, however, computationally, you'll note

527
00:55:44.390 --> 00:55:49.890
Inderjit Dhillon: that this is a much bigger feature space, right? This was 2 dimensional.

528
00:55:50.360 --> 00:55:52.900
Inderjit Dhillon: I went to 6 dimensions here.

529
00:55:54.720 --> 00:55:59.020
Inderjit Dhillon: and then you might ask, Why do I stop here right if my

530
00:55:59.100 --> 00:56:00.960
Inderjit Dhillon: X was an Rd.

531
00:56:02.570 --> 00:56:05.860
Inderjit Dhillon: Well, let's just think about, you know. If I just take.

532
00:56:06.280 --> 00:56:12.600
Inderjit Dhillon: you know, crosses x one x 2. If x was Rd. And I took every pairwise

533
00:56:14.670 --> 00:56:16.950
Inderjit Dhillon: input

534
00:56:17.190 --> 00:56:23.640
Inderjit Dhillon: pair wise core. If I look at all the coordinates and input space and took pairwise combinations.

535
00:56:24.160 --> 00:56:25.960
Inderjit Dhillon: If this was Rd.

536
00:56:26.680 --> 00:56:31.680
Inderjit Dhillon: then this would be about D Square right? Slightly more than one. It will be

537
00:56:32.060 --> 00:56:35.160
Inderjit Dhillon: d square, plus d plus one.

538
00:56:37.230 --> 00:56:41.220
Inderjit Dhillon: So what that means is that the amount of competition would actually increase up.

539
00:56:44.170 --> 00:56:53.310
Inderjit Dhillon: And that's where kind of this really neat idea about kernel methods and kernel fix comes

540
00:56:53.500 --> 00:56:57.920
Inderjit Dhillon: which will be a way to solve the kernel is Vm: problem.

541
00:56:59.280 --> 00:57:02.270
Inderjit Dhillon: Okay. without increasing that dimensionality.

542
00:57:07.250 --> 00:57:14.520
Inderjit Dhillon: Okay, so let me kind of illustrate it. Suppose I have X and and that's it. So suppose I have x prime

543
00:57:14.980 --> 00:57:24.430
Inderjit Dhillon: another point, and I say that this is X, one prime x, 2 prime. Then if I take Fe of X prime using the

544
00:57:24.860 --> 00:57:28.110
Inderjit Dhillon: this recipe above I basically get one

545
00:57:29.980 --> 00:57:34.990
Inderjit Dhillon: square root, 2 x, one prime square root, 2 x, 2 prime.

546
00:57:35.060 --> 00:57:35.780
Inderjit Dhillon: Sorry

547
00:57:36.900 --> 00:57:46.410
Inderjit Dhillon: X, 2 prime x, one prime square X, 2 prime square.

548
00:57:47.820 --> 00:57:51.640
Inderjit Dhillon: and then I have square root, 2, X, one prime x, 2.

549
00:57:58.750 --> 00:58:04.070
Inderjit Dhillon: The neat thing is okay, and this turns out to be more generally applicable

550
00:58:04.480 --> 00:58:07.860
Inderjit Dhillon: is that if I take, let me take this function.

551
00:58:14.080 --> 00:58:17.690
Inderjit Dhillon: Let me look at this inner product right between

552
00:58:18.490 --> 00:58:19.610
Inderjit Dhillon: the X

553
00:58:20.160 --> 00:58:22.130
Inderjit Dhillon: transpose with.

554
00:58:22.880 --> 00:58:24.050
Inderjit Dhillon: we explained.

555
00:58:25.280 --> 00:58:29.030
Inderjit Dhillon: This is an inner product between these 2 6 dimensional vectors.

556
00:58:29.450 --> 00:58:32.710
Inderjit Dhillon: Right? That's the same as one

557
00:58:33.340 --> 00:58:37.080
Inderjit Dhillon: square root, 2 x, one square root, 2 x, 2

558
00:58:37.480 --> 00:58:40.450
Inderjit Dhillon: x, one square x, 2 square

559
00:58:40.490 --> 00:58:43.010
Inderjit Dhillon: square root, 2 x, one x, 2,

560
00:58:44.480 --> 00:58:56.220
Inderjit Dhillon: and then I have p. Of X. Prime, which is one square root, 2 x, one prime square root, 2 x, 2 prime x, one prime square x, 2 prime square

561
00:58:57.920 --> 00:59:01.170
Inderjit Dhillon: square root, 2 x, one prime X. 2 plan.

562
00:59:03.420 --> 00:59:05.410
Inderjit Dhillon: Let me just take this inner product.

563
00:59:05.580 --> 00:59:10.090
Inderjit Dhillon: Okay, and if I multiply it through i'll see that it is one

564
00:59:10.750 --> 00:59:11.510
Inderjit Dhillon: right.

565
00:59:11.880 --> 00:59:17.650
Inderjit Dhillon: It is one times one plus this times, this plus this times this, and so on.

566
00:59:18.320 --> 00:59:22.930
Inderjit Dhillon: Well get one plus 2, x, one x, one prime.

567
00:59:24.850 --> 00:59:31.480
Inderjit Dhillon: plus 2 x, 2 x, 2 prime plus x, one square x.

568
00:59:32.650 --> 00:59:36.850
Inderjit Dhillon: Sorry x one square. I have to be careful. Too many

569
00:59:36.880 --> 00:59:41.730
Inderjit Dhillon: X. One prime square plus x, 2 square

570
00:59:43.080 --> 00:59:47.580
Inderjit Dhillon: x, 2 prime square, plus 2

571
00:59:48.320 --> 00:59:49.960
Inderjit Dhillon: x, one

572
00:59:51.740 --> 00:59:53.250
Inderjit Dhillon: x, 2

573
00:59:53.490 --> 00:59:55.950
Inderjit Dhillon: x, one prime x, 2 prime.

574
00:59:57.360 --> 01:00:06.820
Inderjit Dhillon: and what you can do is you can verify that this is the same as one plus x, one x, one prime plus x, 2, x, 2 prime

575
01:00:07.820 --> 01:00:08.580
Inderjit Dhillon: well.

576
01:00:09.980 --> 01:00:17.650
Inderjit Dhillon: which one can write as one plus x transpose x prime well.

577
01:00:20.450 --> 01:00:26.830
Inderjit Dhillon: and that I can represent as this what's called this kernel function between X and

578
01:00:27.040 --> 01:00:27.910
Inderjit Dhillon: Excellent.

579
01:00:29.160 --> 01:00:32.900
Inderjit Dhillon: and this is sometimes called the Polynomial

580
01:00:36.970 --> 01:00:38.100
Inderjit Dhillon: Don't know.

581
01:00:39.750 --> 01:00:41.650
Inderjit Dhillon: Oh, they great to

582
01:00:45.740 --> 01:00:48.960
Inderjit Dhillon: Okay. So let's just try to understand what we've done right.

583
01:00:51.120 --> 01:00:55.990
Inderjit Dhillon: We have said that in our products, in this feature space

584
01:00:56.410 --> 01:01:03.170
Inderjit Dhillon: in this particular example can actually be represented by a function. Okay.

585
01:01:04.500 --> 01:01:11.670
Inderjit Dhillon: where it's actually not essential to form these 6 dimensional vectors, because I can just do it by doing this competition.

586
01:01:13.950 --> 01:01:17.460
Inderjit Dhillon: This competition is in 2 dimensions, not in 6 dimensions.

587
01:01:19.480 --> 01:01:25.750
Inderjit Dhillon: and the neat thing about the support vector machine is. and just by changing.

588
01:01:27.310 --> 01:01:28.640
Inderjit Dhillon: let me.

589
01:01:29.120 --> 01:01:33.140
Inderjit Dhillon: if I just change this part.

590
01:01:37.160 --> 01:01:44.920
Inderjit Dhillon: Actually, let me just keep this to, instead of an straightforward inner product. I replace it by K. Of X. I. X.

591
01:01:50.970 --> 01:01:53.650
Inderjit Dhillon: Then I will be actually be able to solve this problem

592
01:01:57.200 --> 01:01:59.990
Inderjit Dhillon: where K. Is equal to this.

593
01:02:01.880 --> 01:02:12.520
Inderjit Dhillon: So that's actually another reason why the dual problem is so important. So the dual problem only expresses the competition in the form of the inner products between the

594
01:02:12.570 --> 01:02:17.680
Inderjit Dhillon: input vectors. And so what this is happening is that it is like.

595
01:02:18.560 --> 01:02:23.310
Inderjit Dhillon: and it is exactly the same as the of X. I:

596
01:02:27.270 --> 01:02:31.090
Inderjit Dhillon: Okay. So this is sometimes known as the

597
01:02:34.050 --> 01:02:35.250
Inderjit Dhillon: Colonel

598
01:02:37.290 --> 01:02:38.180
Inderjit Dhillon: Click.

599
01:02:40.660 --> 01:02:41.500
Inderjit Dhillon: Okay.

600
01:02:41.640 --> 01:02:45.750
Inderjit Dhillon: which is that K. Of X. X. Prime

601
01:02:46.070 --> 01:02:52.330
Inderjit Dhillon: can be written at is the same as G. Of X. Transpose V. Of X. One.

602
01:02:53.540 --> 01:02:59.500
Inderjit Dhillon: What that means is, if you have an algorithm where you can just work by inner products.

603
01:03:00.200 --> 01:03:02.500
Inderjit Dhillon: then I can always colonelize that out.

604
01:03:03.640 --> 01:03:08.490
Inderjit Dhillon: Yeah. And then there are these examples of kernel functions.

605
01:03:16.560 --> 01:03:18.880
Inderjit Dhillon: Yeah. So there is a let's say.

606
01:03:20.120 --> 01:03:29.240
Inderjit Dhillon: you know, extending this second order, Polynomial. There is a D. It degree polynomial.

607
01:03:33.380 --> 01:03:38.120
Inderjit Dhillon: which is that K. Of X. X. Prime is equal to

608
01:03:38.390 --> 01:03:42.620
Inderjit Dhillon: this one plus x transpose X prime to the power. D:

609
01:03:43.770 --> 01:03:48.720
Inderjit Dhillon: Okay? And then another one, which is probably the most important one is

610
01:03:49.330 --> 01:03:54.790
Inderjit Dhillon: radio basis or those in Colonel.

611
01:03:59.490 --> 01:04:03.110
Inderjit Dhillon: Okay. And this is K. Of X. X. Prime

612
01:04:03.950 --> 01:04:08.990
Inderjit Dhillon: is equal to E to the power minus x minus x prime

613
01:04:09.070 --> 01:04:12.510
Inderjit Dhillon: square. divided by a with parameter, c.

614
01:04:15.580 --> 01:04:28.110
Inderjit Dhillon: The one thing that you should notice is that the you know the most important point about the kernel trick is that this allows you to work in the original dimensional space.

615
01:04:30.030 --> 01:04:33.360
Inderjit Dhillon: Okay, so maybe I should not have written.

616
01:04:34.210 --> 01:04:39.120
Inderjit Dhillon: I've overloaded D over here. Let me actually change it, so I put

617
01:04:40.260 --> 01:04:41.990
Inderjit Dhillon: would be here.

618
01:04:44.770 --> 01:04:47.890
Inderjit Dhillon: Sorry I put t here and here.

619
01:04:48.330 --> 01:04:53.660
Inderjit Dhillon: but Di, I was also using for dimensionality. So i'll just write it as mth degree polynomial.

620
01:04:57.480 --> 01:05:08.070
Inderjit Dhillon: So remember for Ms. Degree polynomial for 2 degree polynomial. I had transformed my input space of dimension D into a feature space or dimension D square

621
01:05:08.600 --> 01:05:15.810
Inderjit Dhillon: for an M. To degree polynomial, I would have to essentially do it. It's an inner product between.

622
01:05:16.100 --> 01:05:19.220
Inderjit Dhillon: let's say, D to the power M.

623
01:05:20.790 --> 01:05:26.970
Inderjit Dhillon: The family is large. It's very, very big, and, in fact. you can show that

624
01:05:27.170 --> 01:05:33.260
Inderjit Dhillon: over here. This corresponds to a map, to an infinite dimensional space. This

625
01:05:35.440 --> 01:05:42.390
Inderjit Dhillon: so conceptually you can think that you're mapping to an infinite dimensional space. And then you're looking at a linear

626
01:05:42.510 --> 01:05:46.890
Inderjit Dhillon: surface in the input, in infinite dimensional space.

627
01:05:47.570 --> 01:05:51.300
Inderjit Dhillon: right? And then there is a you know there's a theorem that says that

628
01:05:51.310 --> 01:06:00.940
Inderjit Dhillon: you know, in a higher dimensional space you can always separate. There always exists a linear separator. You can just think about it.

629
01:06:01.810 --> 01:06:03.720
Inderjit Dhillon: Yeah. So Basically.

630
01:06:03.860 --> 01:06:07.900
Inderjit Dhillon: linear separator in a higher dimensional feature space

631
01:06:08.090 --> 01:06:11.530
Inderjit Dhillon: can be achieved by using the kernel trick.

632
01:06:12.330 --> 01:06:23.270
Inderjit Dhillon: So I've given this kind of 2 dimensional sorry. The 2 concentric circle data set another classical data set is, you know what sometimes all the X or data cent.

633
01:06:24.330 --> 01:06:27.080
Inderjit Dhillon: Okay, let's see X or data.

634
01:06:29.140 --> 01:06:38.040
Inderjit Dhillon: So if I think about X, one and x 2 like sort is an operation where, you know, if both quantities are 0, you get a 0.

635
01:06:39.580 --> 01:06:42.880
Inderjit Dhillon: If one of the quantities is

636
01:06:44.520 --> 01:06:47.110
let's see x or so, one

637
01:06:47.140 --> 01:06:51.760
Inderjit Dhillon: 0, Xr. 0, one Xr, one is also 0,

638
01:06:52.980 --> 01:06:56.870
Inderjit Dhillon: 0, x r. One is one one like 4.

639
01:06:57.140 --> 01:07:00.420
Inderjit Dhillon: 0 is one. So which means that if I have

640
01:07:01.020 --> 01:07:08.550
Inderjit Dhillon: data set, which is of this form that I have class, One consists of these pluses

641
01:07:10.320 --> 01:07:13.350
Inderjit Dhillon: last 2 consists of this pluses.

642
01:07:14.910 --> 01:07:16.730
Inderjit Dhillon: I'm sorry these circles

643
01:07:18.710 --> 01:07:29.570
Inderjit Dhillon: Clearly you can see that there is no there is no linear separator that separates the the classes, but

644
01:07:29.760 --> 01:07:37.990
Inderjit Dhillon: you can actually use the kernel trick to find a good classifier, because there's no reason why you can't find a good classifier.

645
01:07:38.930 --> 01:07:43.730
Inderjit Dhillon: And then, in terms of like, software there are these libraries called lib. Linear

646
01:07:45.480 --> 01:07:51.680
Inderjit Dhillon: which for linear Svm's. And then

647
01:07:53.680 --> 01:07:54.330
Inderjit Dhillon: okay.

648
01:07:56.510 --> 01:08:08.280
Inderjit Dhillon: The second important point I wanted to make is the following: which is that if I now go back to the primal over here, and I look at it right.

649
01:08:08.400 --> 01:08:17.010
Inderjit Dhillon: One of the things that we've said to you is that you know you can think of both regression, problems and classification problems as

650
01:08:17.350 --> 01:08:20.640
Inderjit Dhillon: loss, function, and regularization.

651
01:08:23.450 --> 01:08:29.830
Inderjit Dhillon: Now, if I think about what this is. remember, I said, this is misclassified.

652
01:08:31.310 --> 01:08:37.279
Inderjit Dhillon: Of course I I is not equal to one always right. Otherwise it would count the number of misclassifications.

653
01:08:40.020 --> 01:08:43.529
Inderjit Dhillon: Okay. So you can actually think of this

654
01:08:44.340 --> 01:08:46.120
Inderjit Dhillon: as

655
01:08:48.640 --> 01:08:53.069
Inderjit Dhillon: we don't change colors again. So you can think of this as

656
01:08:53.140 --> 01:08:58.160
Inderjit Dhillon: regularization. and you can look at this.

657
01:08:59.279 --> 01:09:01.870
Inderjit Dhillon: and you can look at this as

658
01:09:02.939 --> 01:09:06.540
Inderjit Dhillon: or loss function, and is sometimes called as hinge loss.

659
01:09:10.689 --> 01:09:13.200
Inderjit Dhillon: And this is regularization.

660
01:09:17.960 --> 01:09:23.090
Inderjit Dhillon: Okay, let's let's actually look at this right? Why am I calling it the inch loss? Okay.

661
01:09:23.670 --> 01:09:27.120
Inderjit Dhillon: So kind of before that.

662
01:09:29.020 --> 01:09:32.819
Inderjit Dhillon: Remember that again. Regression.

663
01:09:36.430 --> 01:09:38.029
Inderjit Dhillon: loss, function.

664
01:09:40.670 --> 01:09:42.819
Inderjit Dhillon: lost regularization.

665
01:09:47.060 --> 01:09:52.550
Inderjit Dhillon: So remember that ridge regularization, so just reminding you, was squared.

666
01:09:53.910 --> 01:09:56.340
Inderjit Dhillon: and 2 regularization

667
01:09:57.040 --> 01:10:02.020
Inderjit Dhillon: lasso was equivalent to at one regularization

668
01:10:02.790 --> 01:10:05.150
Inderjit Dhillon: in classification. Also.

669
01:10:06.530 --> 01:10:12.350
Inderjit Dhillon: you've seen that classification right? I can write as

670
01:10:14.190 --> 01:10:20.990
Inderjit Dhillon: you've seen, with logistic regression that I can write as loss function plus regularization

671
01:10:22.610 --> 01:10:30.340
Inderjit Dhillon: in particular for logistic logistic loss. You know, L. Logistic

672
01:10:31.470 --> 01:10:38.000
Inderjit Dhillon: is log of one plus e to the power minus y y hat.

673
01:10:38.840 --> 01:10:45.870
Inderjit Dhillon: Okay, that y hat is kind of the let's say the predicted label. Why is the original or given label?

674
01:10:46.370 --> 01:10:48.490
Inderjit Dhillon: Okay, the Svm's

675
01:10:50.100 --> 01:10:53.690
Inderjit Dhillon: correspond to what's called a hinge loss.

676
01:10:54.980 --> 01:11:01.270
Inderjit Dhillon: Yeah, and then we'll see why it's the hinge lost because it is Max of

677
01:11:05.570 --> 01:11:06.610
Inderjit Dhillon: is

678
01:11:06.680 --> 01:11:09.340
Inderjit Dhillon: Max of 0

679
01:11:10.180 --> 01:11:13.140
Inderjit Dhillon: and one minus Y,

680
01:11:14.610 --> 01:11:24.980
Inderjit Dhillon: and it's really a STEM from the side eyes. Okay, remember that as I, If. Xi is correctly classified. so I I go to 0.

681
01:11:29.390 --> 01:11:30.310
Inderjit Dhillon: Okay.

682
01:11:31.830 --> 01:11:37.540
Inderjit Dhillon: If Xi is correctly.

683
01:11:42.190 --> 01:11:46.260
Inderjit Dhillon: then I I equals 0. So that means that the loss is 0.

684
01:11:47.720 --> 01:11:49.100
Inderjit Dhillon: But if not.

685
01:11:51.140 --> 01:11:54.710
Inderjit Dhillon: then I have I I as the loss

686
01:11:55.310 --> 01:11:59.450
Inderjit Dhillon: and say, I is just

687
01:12:00.270 --> 01:12:02.320
Inderjit Dhillon: one minus y.

688
01:12:02.670 --> 01:12:05.340
Inderjit Dhillon: so I can plot on

689
01:12:05.620 --> 01:12:11.910
Inderjit Dhillon: the x-axis if I plot yy hat. so remember this is

690
01:12:14.230 --> 01:12:23.320
plus one minus one. Why, I had can take any kind of value right. Remember, the y's are either plus one or minus one, but the y head can actually take any. I have a real value.

691
01:12:23.780 --> 01:12:26.520
Inderjit Dhillon: The y-y hat can range from

692
01:12:28.640 --> 01:12:30.280
Inderjit Dhillon: over all the real numbers

693
01:12:30.820 --> 01:12:36.120
Inderjit Dhillon: the actual loss. If I think about the number of missed classification as the loss.

694
01:12:36.340 --> 01:12:39.560
Inderjit Dhillon: then this is what you want

695
01:12:41.240 --> 01:12:45.990
Inderjit Dhillon: that when why I had agree. and there is no loss.

696
01:12:47.310 --> 01:12:55.420
Inderjit Dhillon: So when yy had our when the signs agree, there is no loss right. But when

697
01:12:56.150 --> 01:13:03.450
Inderjit Dhillon: the science. Don't agree. That means why I had is minus one. Then there is a unit loss.

698
01:13:06.320 --> 01:13:11.360
Inderjit Dhillon: Okay. And now, if I want to plot over here the hinge loss.

699
01:13:12.190 --> 01:13:22.060
Inderjit Dhillon: You can see that the hinge loss is going to be. Let's see. Let me give a color to hinge loss, which is.

700
01:13:22.180 --> 01:13:25.670
Inderjit Dhillon: let's see what should we choose? Pink.

701
01:13:26.700 --> 01:13:35.200
Inderjit Dhillon: So this is hinge loss. The hinge loss is going to be 0 here. and as

702
01:13:37.370 --> 01:13:38.760
Inderjit Dhillon: so I I

703
01:13:43.390 --> 01:13:45.100
Inderjit Dhillon: it's going to be like this.

704
01:13:48.380 --> 01:13:51.850
Inderjit Dhillon: and logistic loss is.

705
01:13:52.830 --> 01:13:56.490
Inderjit Dhillon: Let's put it by, let's say, Orange.

706
01:13:56.980 --> 01:13:58.790
Inderjit Dhillon: This is logistic loss.

707
01:13:59.510 --> 01:14:03.050
Inderjit Dhillon: and that's going to be kind of an exponential. Sorry my

708
01:14:03.220 --> 01:14:05.110
Inderjit Dhillon: figure is not great, but

709
01:14:07.070 --> 01:14:08.980
Inderjit Dhillon: it's something like this.

710
01:14:10.490 --> 01:14:12.880
Inderjit Dhillon: Okay? And this is at 0.

711
01:14:15.240 --> 01:14:17.100
Inderjit Dhillon: I mean, I should just

712
01:14:17.170 --> 01:14:18.800
Inderjit Dhillon: it is all this.

713
01:14:37.120 --> 01:14:38.850
Inderjit Dhillon: and that's also the

714
01:14:40.950 --> 01:14:42.200
Inderjit Dhillon: Exactly.

715
01:14:44.600 --> 01:14:45.470
Inderjit Dhillon: Okay.

716
01:14:47.270 --> 01:14:54.990
Inderjit Dhillon: So so that's another reason to to kind of expose this to you. That again, like with Svm's: Also.

717
01:14:55.160 --> 01:14:57.610
Inderjit Dhillon: I can basically write it as

718
01:15:02.240 --> 01:15:05.700
Inderjit Dhillon: loss function plus regularization.

719
01:15:06.010 --> 01:15:10.630
Inderjit Dhillon: and the last function is hinge loss.

720
01:15:12.660 --> 01:15:21.070
Inderjit Dhillon: Well, says logistic loss for logistic regression, this can kind of create some problems because this is a non differentiable loss function.

721
01:15:22.780 --> 01:15:28.950
Inderjit Dhillon: Okay. But those them to get reflected in the

722
01:15:29.810 --> 01:15:35.220
Inderjit Dhillon: Okay, I think i'm just kind of minute over time. But

723
01:15:36.110 --> 01:15:38.690
Inderjit Dhillon: that concludes

724
01:15:39.290 --> 01:15:41.890
Inderjit Dhillon: our discussion about

725
01:15:41.930 --> 01:15:48.710
Inderjit Dhillon: support vector machines, and I actually I don't think i'm going to talk about kernel methods also, but kernel methods. You can

726
01:15:49.590 --> 01:16:03.250
Inderjit Dhillon: essentially try to kernelize any method that uses a linear surface. As long as the competition can be expressed in terms of inner products, you can replace that in a product with the kernel function, and you can work with that.

727
01:16:04.080 --> 01:16:05.740
Inderjit Dhillon: So

728
01:16:06.810 --> 01:16:18.780
Inderjit Dhillon: with that, I think for a time being we will start stop talking about supervised learning for the time being, and once we get back from Spring break

729
01:16:19.040 --> 01:16:27.800
Inderjit Dhillon: we'll talk a little bit about unsupervised learning where there may not be any. Why, i's associated with the data. You're just given a set of excise.

730
01:16:27.860 --> 01:16:39.960
Inderjit Dhillon: What can you gleam by looking at unsupervised techniques like clustering data, clustering graphical spring techniques like principal components? Analysis?

731
01:16:40.230 --> 01:16:48.720
Inderjit Dhillon: Okay. And then finally, we'll get on to again some deep learning architectures. We'll talk about the transformers and then, of course, we'll move on to

732
01:16:50.450 --> 01:17:01.410
Inderjit Dhillon: the present class presentations, and of course you will be doing the class products. Okay, so thank you. Have a great spring break.

733
01:17:01.550 --> 01:17:12.680
Inderjit Dhillon: and hopefully you'll be well rested after spring break to tackle the rest of the class. Okay. So Oh, I I guess I forgot to ask any questions.

734
01:17:12.730 --> 01:17:18.350
chitrank: Oh, yeah, so so it's. It is also regarding the last lecture as well.

735
01:17:18.600 --> 01:17:23.000
chitrank: Can you like? Explain again, how do we obtain the complementary slackness conditions.

736
01:17:24.330 --> 01:17:27.440
Inderjit Dhillon: complementary slightness conditions? Okay.

737
01:17:28.690 --> 01:17:30.140
Inderjit Dhillon: let me.

738
01:17:30.650 --> 01:17:33.040
Let's see. Is it in here?

739
01:17:37.950 --> 01:17:42.440
Inderjit Dhillon: Yeah. So if you recall right, I did kind of go through these.

740
01:17:45.890 --> 01:17:48.910
Inderjit Dhillon: And I said, these are part of the Kkt conditions.

741
01:17:53.550 --> 01:17:56.700
Inderjit Dhillon: So i'm not going to go into detail.

742
01:17:56.870 --> 01:18:01.050
Inderjit Dhillon: Let me see, so I can give you a reference which is, Look at

743
01:18:01.790 --> 01:18:04.420
Inderjit Dhillon: Boyd and

744
01:18:05.480 --> 01:18:06.620
Inderjit Dhillon: Vanderbilt.

745
01:18:08.730 --> 01:18:11.930
Inderjit Dhillon: This is a textbook, and I believe it's available online.

746
01:18:13.340 --> 01:18:27.150
chitrank: And and why do we call it like, Why do we? Why, why have you given a name like this like? What's the intuition behind this name. Compliment.

747
01:18:27.520 --> 01:18:29.160
Inderjit Dhillon: Right? You say that

748
01:18:29.370 --> 01:18:33.830
Inderjit Dhillon: the inequality has slack if it's actually a strict inequality.

749
01:18:34.420 --> 01:18:35.090
Hmm.

750
01:18:35.170 --> 01:18:47.640
Inderjit Dhillon: Which means that there is some room to wiggle around right. You can change your variable so that you know it become the slide becomes 0. There is some slag over here. so that's why

751
01:18:48.090 --> 01:19:00.880
Inderjit Dhillon: you know. and then lambda eyes are the dual variables. and there is slack if those lambda is a greater than 0 and not equal to 0.

752
01:19:01.750 --> 01:19:06.420
Inderjit Dhillon: So there is a complementarity between the primal and the dual.

753
01:19:06.840 --> 01:19:09.700
Inderjit Dhillon: It's actually talking about the slack

754
01:19:10.270 --> 01:19:18.370
Inderjit Dhillon: in each part. I did not name it right, but i'm giving you my answer as to why it is called complementary slides.

755
01:19:19.560 --> 01:19:25.460
And and these conditions are for what? Exactly like what will happen if these conditions are true.

756
01:19:25.730 --> 01:19:31.900
Inderjit Dhillon: It basically provides, like I said over here it provides the certificate of optimality

757
01:19:32.540 --> 01:19:34.520
Inderjit Dhillon: when this problem is convex

758
01:19:36.180 --> 01:19:38.950
Inderjit Dhillon: and these are these Kkt conditions.

759
01:19:39.000 --> 01:19:40.950
Inderjit Dhillon: Karish Kun Tucker conditions.

760
01:19:42.000 --> 01:19:48.360
Inderjit Dhillon: These are 3 optimizers, I guess, who were in the 1930 S. Who formulated these conditions.

761
01:19:50.570 --> 01:19:52.610
chitrank: Okay, I got it. Thank you.

762
01:19:52.690 --> 01:20:02.390
Inderjit Dhillon: Okay, I mean, yeah, I mean, this is something that I quickly went over. What I really wanted to convey was the kind of consequence of this. What happens in support with provisions.

763
01:20:05.310 --> 01:20:07.380
Inderjit Dhillon: Okay? Oh, and so

764
01:20:07.490 --> 01:20:09.120
chitrank: I also forward so

765
01:20:09.380 --> 01:20:14.930
chitrank: like, Why, why are we like assuming that these conditions are true? Like?

766
01:20:15.540 --> 01:20:20.200
Inderjit Dhillon: No, in this case they actually called so when we have

767
01:20:22.400 --> 01:20:29.680
Inderjit Dhillon: when we have again, I didn't go one, as I said that this is, you know, when the problem is convex right? So this is a

768
01:20:31.890 --> 01:20:38.410
chitrank: This is a oh, yeah, yeah, yeah, this is a quadratic objective with linear inequality

769
01:20:38.440 --> 01:20:39.070
marketing.

770
01:20:39.210 --> 01:20:44.520
chitrank: It should. But then why is like the

771
01:20:44.730 --> 01:20:48.370
chitrank: the the problem that we like. The modified spm

772
01:20:48.650 --> 01:20:51.510
primal that we discussed is convex.

773
01:20:51.700 --> 01:20:57.280
Inderjit Dhillon: It's also convex right. I mean, if you go back. and if you look at my lecture today.

774
01:21:00.520 --> 01:21:03.110
Inderjit Dhillon: Right? Let's see what is the plan? No problem.

775
01:21:05.040 --> 01:21:07.640
Inderjit Dhillon: This is also convex right?

776
01:21:08.060 --> 01:21:10.410
Inderjit Dhillon: This is linear this is called

777
01:21:11.940 --> 01:21:17.730
Inderjit Dhillon: inequality constraints. The dual is also complex. Well, con case.

778
01:21:19.300 --> 01:21:19.870
Okay.

779
01:21:22.180 --> 01:21:24.560
chitrank: So these are also complex problems.

780
01:21:25.940 --> 01:21:31.490
Inderjit Dhillon: and in some sense not not that difficult right? They're not general context. They're quadratic optimization problems.

781
01:21:31.890 --> 01:21:36.970
Inderjit Dhillon: quadratic objective linear constraints.

782
01:21:38.760 --> 01:21:39.860
chitrank: Thank you.

783
01:21:39.870 --> 01:21:41.890
Inderjit Dhillon: Any more questions.

784
01:21:42.540 --> 01:21:43.120
Yeah.

785
01:21:44.570 --> 01:21:48.700
Inderjit Dhillon: Okay. Any more questions from anybody else.

786
01:21:50.870 --> 01:21:54.790
Inderjit Dhillon: Okay. Otherwise, again, have a great spring break.

787
01:21:55.250 --> 01:22:03.930
Inderjit Dhillon: Feel free to unmute yourself and wish all your other classmates so happy. Spring break. Of course you have to do your homework, and we'll meet after a spring break.

788
01:22:07.610 --> 01:22:10.220
Margaret Von Ebers: Okay, Bye, bye.

789
01:22:10.480 --> 01:22:11.400
Christopher Lawson: Thank you.

790
01:22:21.280 --> 01:22:22.540
Nilesh Gupta: Bye, Everyone.

WEBVTT

1
00:00:17.880 --> 00:00:20.180
Inderjit Dhillon: Good afternoon.


2
00:00:21.860 --> 00:00:24.000
Inderjit Dhillon: Just give me a minute.


3
00:00:33.720 --> 00:00:38.600
Inderjit Dhillon: Okay, Great welcome back, everybody.


4
00:00:39.040 --> 00:00:41.670
Inderjit Dhillon: I see people are trickling in, so


5
00:00:42.030 --> 00:00:44.570
Inderjit Dhillon: we'll start soon.


6
00:00:45.500 --> 00:00:55.410
Inderjit Dhillon: I hope you all had a good spring break, and you are ready to now go on to the second half of this class. So


7
00:00:56.450 --> 00:01:00.630
Inderjit Dhillon: there's just one announcement. I believe homework 3 is out.


8
00:01:00.910 --> 00:01:15.010
Inderjit Dhillon: So homework 3 has been released. So please take a look at it. It has 2 problems. One is they're actually things that you've looked at before. But now, looking at it with the with the different lengths.


9
00:01:15.260 --> 00:01:29.730
Inderjit Dhillon: One is the, you know, concentric circle problem and asking you to do it, using Svm's, and the second is the problem you did in the first homework with. you know, matrix factorization that you did, but this time doing it with


10
00:01:29.770 --> 00:01:41.090
Inderjit Dhillon: deep learning. So please kind of, you know. Try and get started on it soon. And then the other reminder is that the second part


11
00:01:41.130 --> 00:01:50.800
Inderjit Dhillon: of the classes, you know, heavily dominated by You know your the project that you're going to be doing so. Remember that on Wednesday


12
00:01:50.950 --> 00:01:57.100
Inderjit Dhillon: you need to get to us what project you are doing. So it's just a one or 2 loop page


13
00:01:57.210 --> 00:02:14.910
Inderjit Dhillon: right up that you need to submit a project. Name people who are doing it a couple of sentences, a couple of paragraphs about You know your project, and then how you are going to approach it. So overall we are looking for just a short right up


14
00:02:14.940 --> 00:02:16.830
Inderjit Dhillon: one to 2.


15
00:02:18.480 --> 00:02:20.240
Inderjit Dhillon: So any questions on that.


16
00:02:32.410 --> 00:02:35.210
Inderjit Dhillon: Okay. So if not, then let me


17
00:02:36.230 --> 00:02:42.540
Inderjit Dhillon: share my screen, and I will start lecturing. So


18
00:02:42.620 --> 00:02:44.140
Inderjit Dhillon: that's not good.


19
00:02:50.280 --> 00:02:55.230
Inderjit Dhillon: Malaysia is out today, so this can be 1min. Huh?


20
00:03:19.780 --> 00:03:30.530
Inderjit Dhillon: Oh, yeah, just give me 1min. I'm just having some. I think I got locked out of everything. So I just need to make sure. I'm logged into zoom with my current. Correct


21
00:03:35.230 --> 00:03:37.870
Inderjit Dhillon: you T: because i'm getting an error.


22
00:05:27.020 --> 00:05:29.360
Inderjit Dhillon: Okay, I'm: almost there.


23
00:06:06.130 --> 00:06:07.650
Inderjit Dhillon: Okay.


24
00:06:07.660 --> 00:06:11.480
Inderjit Dhillon: Sorry about the slight technical delay.


25
00:06:11.720 --> 00:06:17.260
Inderjit Dhillon: Okay, so so far in the class, we've studied, mostly supervised learning.


26
00:06:19.550 --> 00:06:20.760
Inderjit Dhillon: So today


27
00:06:20.830 --> 00:06:25.520
Inderjit Dhillon: we'll talk a little bit about unsupervised 9. Okay. So


28
00:06:29.340 --> 00:06:31.060
Inderjit Dhillon: unsupervised.


29
00:06:37.490 --> 00:06:44.140
Inderjit Dhillon: Yeah. So remember the supervised learning setting where we typically have


30
00:06:46.820 --> 00:06:51.380
Inderjit Dhillon: the supervised learning setting is the following: yeah. supervised


31
00:06:52.720 --> 00:06:55.380
Inderjit Dhillon: learning setting


32
00:06:57.330 --> 00:06:59.680
Inderjit Dhillon: you typically have X. I.


33
00:07:00.380 --> 00:07:04.900
Inderjit Dhillon: Y. I. Typically X. I belongs to Rd.


34
00:07:05.360 --> 00:07:16.930
Inderjit Dhillon: And then why I is some label. But it is very crucial that you have your Yi. So if you look type regression problems or you look at classification problem.


35
00:07:17.170 --> 00:07:23.160
Inderjit Dhillon: then there is a training set that is given in the unsupervised


36
00:07:25.890 --> 00:07:27.330
Inderjit Dhillon: learning setting


37
00:07:30.770 --> 00:07:38.690
Inderjit Dhillon: the supervision information is just not there, so all you have is Xi. Belongs to already.


38
00:07:38.880 --> 00:07:45.210
Inderjit Dhillon: Why, i's all the labels they come in. This supervised setting. But in the


39
00:07:45.570 --> 00:07:51.710
Inderjit Dhillon: you don't have anything. So it's more a question of just trying to understand a particular data set


40
00:07:53.850 --> 00:08:01.020
Inderjit Dhillon: which is comprised of excise. So excise could be a collection of email that was somehow embedded in the d dimensional, vector


41
00:08:01.090 --> 00:08:05.370
Inderjit Dhillon: or they could just come from some other way


42
00:08:05.400 --> 00:08:14.370
Inderjit Dhillon: of data collection. But so so now we are in this scenario. We are just given the excise. And so one


43
00:08:14.690 --> 00:08:22.610
Inderjit Dhillon: very useful technique in unsupervised learning is the problem of clustering.


44
00:08:23.170 --> 00:08:30.470
Inderjit Dhillon: clustering means that I will arrange my data into groups or clusters.


45
00:08:30.690 --> 00:08:41.820
Inderjit Dhillon: So suppose you know, I have data which is again, i'm. you know, thinking of it as 2 dimensional. So think of each point as in Rd.


46
00:08:42.210 --> 00:08:48.860
Inderjit Dhillon: And notice that i'm not putting any, you know. Earlier I would put crosses and circles.


47
00:08:48.950 --> 00:08:53.260
Inderjit Dhillon: But right now, since there are no labels, i'm just putting crosses right.


48
00:08:54.160 --> 00:09:05.810
Inderjit Dhillon: So now i'm giving this X. I belong to already right, and you want to try and say, okay, similar objects should go into or similar data points. Xi. Should go into the same cluster.


49
00:09:05.820 --> 00:09:10.260
Inderjit Dhillon: and this similar point should go into the similar clusters.


50
00:09:10.590 --> 00:09:20.570
Inderjit Dhillon: So how would you go about kind of doing this task? This is the clustering task. So that's what we are going to talk about today.


51
00:09:21.060 --> 00:09:27.540
Inderjit Dhillon: So one possibility would be that you go to each point and you say, okay.


52
00:09:27.630 --> 00:09:35.910
Inderjit Dhillon: how can I? You know this point looks close to this point. This looks close to this. This looks close to this. These are close to each other.


53
00:09:36.080 --> 00:09:39.530
Inderjit Dhillon: These are close to each other. These are close to each other.


54
00:09:39.910 --> 00:09:42.750
Inderjit Dhillon: Similarly this similarly this.


55
00:09:42.890 --> 00:09:52.510
Inderjit Dhillon: and then I say, oh, these points are actually close to each other. These 2 groups are close to each other. Similarly over here similarly over here.


56
00:09:52.850 --> 00:10:01.910
Inderjit Dhillon: And similarly, this is one cluster, and this is another cluster right? And if you recall, when I had to kind of drawn these points like this.


57
00:10:02.140 --> 00:10:15.480
Inderjit Dhillon: And if I had asked you well, how many clusters are there in this data set? You know. A reasonable answer would be that there are 2 clusters. And what I just described to you is a very simple


58
00:10:16.130 --> 00:10:17.300
Inderjit Dhillon: algorithm


59
00:10:17.340 --> 00:10:23.930
Inderjit Dhillon: for doing this clustering, which is called, and this is what we'll talk a little bit about first, which is called


60
00:10:24.360 --> 00:10:25.990
Inderjit Dhillon: hierarchical


61
00:10:29.140 --> 00:10:30.790
Inderjit Dhillon: agglomerative


62
00:10:34.890 --> 00:10:35.820
Inderjit Dhillon: plus one.


63
00:10:40.690 --> 00:10:47.460
Inderjit Dhillon: Yeah. So that's the first kind of technique that we will study. And you can now say, okay, when I


64
00:10:47.630 --> 00:10:53.180
Inderjit Dhillon: to this 2 points, and I merge them. Suppose I write them as leaves


65
00:10:53.620 --> 00:10:56.100
Inderjit Dhillon: in a tree. Then


66
00:10:56.420 --> 00:11:03.450
Inderjit Dhillon: I can kind of say that. Oh, well, I actually merge those 2 points. Then. Similarly, I merge these 2 points.


67
00:11:04.280 --> 00:11:06.090
Inderjit Dhillon: Merge these 2,


68
00:11:06.930 --> 00:11:10.350
Inderjit Dhillon: merge these 2 right? So these would represent


69
00:11:10.600 --> 00:11:15.120
Inderjit Dhillon: all the points over here. and then, similarly for


70
00:11:17.840 --> 00:11:30.370
Inderjit Dhillon: the 8 points on the right. I could think about building a tree like this. Then the fact that I merge these would point to that. I merge this.


71
00:11:30.720 --> 00:11:41.730
Inderjit Dhillon: What's this large, these 2? And when I kind of merge all large. you know these


72
00:11:41.760 --> 00:11:44.780
Inderjit Dhillon: 2 into this one cluster


73
00:11:44.950 --> 00:11:48.090
Inderjit Dhillon: that you can think of as doing this.


74
00:11:49.440 --> 00:11:55.040
Inderjit Dhillon: Okay, so I can think of the clustering process as building


75
00:11:56.460 --> 00:11:59.170
Inderjit Dhillon: this hierarchical tree.


76
00:12:05.530 --> 00:12:15.740
Inderjit Dhillon: Again, the question is, how would you go about doing it? Okay. So let's look at an algorithm and let's write down the algorithm So what you are given is


77
00:12:16.480 --> 00:12:18.600
Inderjit Dhillon: you are given endpoints.


78
00:12:20.080 --> 00:12:28.210
Inderjit Dhillon: So remember, all we have is excise belongs to already. We do not have any supervision information, so we do not actually have any


79
00:12:28.260 --> 00:12:29.430
Inderjit Dhillon: labels.


80
00:12:30.720 --> 00:12:35.780
Inderjit Dhillon: And now I want to build this. I do this hierarchical, agglomerative trust.


81
00:12:35.960 --> 00:12:48.670
Inderjit Dhillon: So what I can do is I can say. And then suppose the the point is to actually get. See clusters here. I build this tree all the way to the top. But maybe you only wanted 2 clusters this and this.


82
00:12:48.970 --> 00:12:53.550
Inderjit Dhillon: Okay. So let's suppose that C is given, and as an input


83
00:12:53.760 --> 00:13:04.370
Inderjit Dhillon: or K is given as an input that means that you want K. Clusters. So let me have a temporary variable, called c. Which captures the total number of customs.


84
00:13:04.500 --> 00:13:12.640
Inderjit Dhillon: So in the beginning I can think that oh, every point is in its own cluster. So in the beginning you can say that C equals n.


85
00:13:13.020 --> 00:13:17.370
Inderjit Dhillon: and you can say that each cluster, c. I


86
00:13:17.840 --> 00:13:23.600
Inderjit Dhillon: is just each individual point by itself, right? So every point is its own cluster.


87
00:13:23.730 --> 00:13:29.960
Inderjit Dhillon: and you have total and clusters which are all the training data points given to you.


88
00:13:30.310 --> 00:13:30.970
Inderjit Dhillon: Okay?


89
00:13:31.020 --> 00:13:37.850
Inderjit Dhillon: And then at every step in the algorithm you execute this while loop, which says that


90
00:13:37.990 --> 00:13:41.680
Inderjit Dhillon: as long as the total number of clusters is greater than equal to K.


91
00:13:41.690 --> 00:13:46.630
Inderjit Dhillon: What you did was Well, when I merge these 2 points over here, right?


92
00:13:46.760 --> 00:13:49.430
Inderjit Dhillon: And what I was trying to do is say, okay.


93
00:13:49.630 --> 00:13:58.380
Inderjit Dhillon: i'll find the nearest point to the cluster, and for to, or to an existing cluster, and i'll merge them together, or I can find the nearest


94
00:13:58.620 --> 00:14:02.040
Inderjit Dhillon: pair of clusters, and I want to merge them together right?


95
00:14:02.100 --> 00:14:03.150
Inderjit Dhillon: So


96
00:14:03.900 --> 00:14:12.380
Inderjit Dhillon: I write. Find the nearest and actually not that I haven't actually defined what nearest is


97
00:14:12.950 --> 00:14:17.230
Inderjit Dhillon: right, but let's write it down nearest pair of clusters.


98
00:14:18.640 --> 00:14:23.100
Inderjit Dhillon: And this is, you know. Just let me clarify that this is the distinct clusters.


99
00:14:30.990 --> 00:14:40.670
Inderjit Dhillon: c. I. And suppose that the nearest pair of clusters is C. I and C 2. Okay. You first find out from among all possible pairs.


100
00:14:40.910 --> 00:14:43.860
Inderjit Dhillon: and then you merge


101
00:14:44.730 --> 00:14:51.520
Inderjit Dhillon: clusters, merge clusters. c. I and C. J.


102
00:14:52.280 --> 00:14:56.900
Inderjit Dhillon: And suppose you call the merge cluster. All


103
00:14:57.370 --> 00:14:59.600
Inderjit Dhillon: the merge cluster


104
00:15:03.540 --> 00:15:04.860
Inderjit Dhillon: as C. I.


105
00:15:05.510 --> 00:15:07.290
Inderjit Dhillon: Okay, and then


106
00:15:07.850 --> 00:15:09.020
Inderjit Dhillon: delete


107
00:15:12.690 --> 00:15:13.680
Inderjit Dhillon: Cj.


108
00:15:14.740 --> 00:15:24.440
Inderjit Dhillon: Okay. And then you decrement the amount. The number of clusters is reduced by one. and you keep on doing this Still.


109
00:15:24.940 --> 00:15:28.960
Inderjit Dhillon: the number of clusters is greater than equal to. Okay.


110
00:15:30.340 --> 00:15:33.300
Inderjit Dhillon: So this is one kind of algorithm


111
00:15:33.710 --> 00:15:36.900
Inderjit Dhillon: to do hierarchical


112
00:15:37.250 --> 00:15:44.150
Inderjit Dhillon: right now. It's not kind of I haven't given you all the details, because I've just said nearest pair. Right?


113
00:15:46.790 --> 00:15:52.490
Inderjit Dhillon: So what do I need to do. I need to find the nearest pair of distinct clusters


114
00:15:52.920 --> 00:15:55.370
Inderjit Dhillon: in the bigger name. each


115
00:15:58.240 --> 00:16:01.100
Inderjit Dhillon: each point ends up, being in its own cluster.


116
00:16:02.190 --> 00:16:13.740
Inderjit Dhillon: But as you start merging the clusters. then each cluster is actually a set of points. So what does it mean to me to find the nearest pair


117
00:16:14.150 --> 00:16:17.620
Inderjit Dhillon: off 2 sets of points.


118
00:16:18.810 --> 00:16:24.310
Inderjit Dhillon: Okay, so it's not just 2 points, but 2 sets of points.


119
00:16:25.580 --> 00:16:37.890
Inderjit Dhillon: So now you can actually have various measures of nearer nearness. So let's look at some of the ones that are typically used. So you have measures of nearness


120
00:16:40.860 --> 00:16:42.350
Inderjit Dhillon: or distance.


121
00:16:47.220 --> 00:16:55.140
Inderjit Dhillon: Okay, so one is, you let's say, D of c. I. C. J.


122
00:16:55.900 --> 00:17:06.960
Inderjit Dhillon: Right. Remember, C. I and C. They both have some points. What you can to take is you can look at all the points so in Ci and Cj.


123
00:17:07.130 --> 00:17:12.940
Inderjit Dhillon: And you can define the distance between the clusters as the 2 points


124
00:17:12.980 --> 00:17:20.520
Inderjit Dhillon: in cluster, the the the pairs, the the pair of points in cluster. One and 2 were actually the nearest to each other.


125
00:17:20.760 --> 00:17:21.540
Inderjit Dhillon: Okay.


126
00:17:21.849 --> 00:17:27.740
Inderjit Dhillon: So suppose I. So let me write that what I mean. So it take men off


127
00:17:28.240 --> 00:17:33.050
Inderjit Dhillon: X minus X. Prime, where X belongs to C. I.


128
00:17:33.510 --> 00:17:35.800
Inderjit Dhillon: And X. Prime belongs to


129
00:17:37.380 --> 00:17:38.400
Inderjit Dhillon: Cj.


130
00:17:42.790 --> 00:17:55.920
Inderjit Dhillon: And sometimes this is called the D men. because what i'm taking is i'm looking at all pairwise. I'm looking at all distances between points in cluster, one and points in cluster 2.


131
00:17:57.230 --> 00:17:58.690
Inderjit Dhillon: I end up, taking the minimum.


132
00:17:59.060 --> 00:18:04.620
Inderjit Dhillon: There's no real reason to only take the minimum. I could actually have taken the maximum right.


133
00:18:05.000 --> 00:18:08.070
Inderjit Dhillon: So I could define the Max


134
00:18:09.380 --> 00:18:12.370
Inderjit Dhillon: C. I. C. J. To be Max


135
00:18:13.170 --> 00:18:21.690
Inderjit Dhillon: for all pairwise distances between points and Cluster I and points in cluster.


136
00:18:22.680 --> 00:18:23.430
Inderjit Dhillon: Okay.


137
00:18:24.560 --> 00:18:25.240
Inderjit Dhillon: okay.


138
00:18:26.330 --> 00:18:30.550
Inderjit Dhillon: this is in literature. This is called the single link. Algorithm


139
00:18:32.590 --> 00:18:35.800
Inderjit Dhillon: and this is called the Complete Link. Algorithm


140
00:18:39.490 --> 00:18:45.520
Inderjit Dhillon: Okay. And there are reasons why this is done. So. So suppose I have.


141
00:18:45.750 --> 00:18:51.100
Inderjit Dhillon: Let's say, you know, let me just say that I have a set of points like this.


142
00:18:55.200 --> 00:19:10.060
Inderjit Dhillon: What this will do is it will construct. Let's say iglomerate these 2. Let's say theglomerates, these 2. Let's say it agglomerates these 2 agglomerates these 2.


143
00:19:10.690 --> 00:19:16.730
Inderjit Dhillon: Right, then the distance between these and this cluster, and this cluster is just a distance. This distance.


144
00:19:17.430 --> 00:19:20.580
Inderjit Dhillon: so it will agglomerate these.


145
00:19:21.030 --> 00:19:25.520
Inderjit Dhillon: The distance between this and this is this: it will acclimate these.


146
00:19:26.000 --> 00:19:34.770
Inderjit Dhillon: and then the distance between this cluster and this cluster. Is this so? It will enough forglomerate this. So it constructs these kind of long chains.


147
00:19:38.360 --> 00:19:46.280
Inderjit Dhillon: Okay, and that's why. Sometimes it's just called single link. because the the the


148
00:19:46.710 --> 00:20:00.480
Inderjit Dhillon: the distance between the cluster just is measured by you know the presence of a point in one cluster with the presence of another point, and another cluster, such that they are nearest to each other. All the other points could actually have been very far away.


149
00:20:00.670 --> 00:20:09.090
Inderjit Dhillon: right and so and similarly complete link. Actually it looks at all the pairwise distances, and then chooses the Max.


150
00:20:09.510 --> 00:20:15.900
Inderjit Dhillon: Then there are other ways to measure distances. So, for example, you could take the


151
00:20:15.960 --> 00:20:23.790
Inderjit Dhillon: what's called mean, c. I. Cj. And this is you take M. I. Minus Mg.


152
00:20:24.240 --> 00:20:30.190
Inderjit Dhillon: Okay, where am I? Is the mean


153
00:20:31.370 --> 00:20:33.780
Inderjit Dhillon: off. So


154
00:20:34.810 --> 00:20:41.570
Inderjit Dhillon: C. I and M is the mean of cluster. Cj.


155
00:20:41.830 --> 00:20:45.150
Inderjit Dhillon: So I basically take the centroid of each cluster


156
00:20:45.570 --> 00:20:48.400
Inderjit Dhillon: of cluster I yesterday.


157
00:20:48.630 --> 00:20:53.380
Inderjit Dhillon: And then I say that i'm just going to take the different, the the distance


158
00:20:53.490 --> 00:21:09.290
Inderjit Dhillon: between these cluster centroids. So that is one way of defining the distance. So, as you can see, there are many ways of defining the disc distances, and so you might say, Well, instead of taking men, Max, or mean.


159
00:21:09.410 --> 00:21:18.730
Inderjit Dhillon: can I even take the average? And certainly you can take the average right so you can take C. I. C. J. You can take


160
00:21:20.640 --> 00:21:33.840
Inderjit Dhillon: all the pair wise distances. Suppose this is a notation that we'll be using through class today that Ni is the number of points in class I just to I and and J is the number of points in cluster. J.


161
00:21:34.490 --> 00:21:37.390
Inderjit Dhillon: Currently, then, I can essentially take


162
00:21:37.660 --> 00:21:42.430
Inderjit Dhillon: X minus X Prime X belongs to C. I.


163
00:21:42.650 --> 00:21:46.570
Inderjit Dhillon: X. Prime belongs to. So I can basically take


164
00:21:46.640 --> 00:21:49.050
Inderjit Dhillon: the average


165
00:21:51.020 --> 00:21:54.270
Inderjit Dhillon: of all the distances. Okay.


166
00:21:55.150 --> 00:22:01.310
Inderjit Dhillon: So all these kind of are valid valid algorithms to use in


167
00:22:01.400 --> 00:22:04.110
Inderjit Dhillon: an article agglomerative clustering.


168
00:22:05.410 --> 00:22:18.110
Inderjit Dhillon: But they all have very distinct characteristics. Right? So Sometimes you might, you know, want a particular characteristic in your in your algorithm, and then you may want to choose the distance function


169
00:22:18.170 --> 00:22:24.680
Inderjit Dhillon: to be appropriate. I can kind of give you an example from. You know my


170
00:22:24.960 --> 00:22:38.170
Inderjit Dhillon: on kind of work that I've done so I you know. I think, about 15 years ago I was working with somebody in industry who was trying to


171
00:22:38.420 --> 00:22:43.260
Inderjit Dhillon: work in the automotive industry to try and make


172
00:22:43.510 --> 00:22:53.750
Inderjit Dhillon: expert systems in car dealerships and what they were trying to do is they were trying to, you know, when they record cases of


173
00:22:54.070 --> 00:23:12.770
Inderjit Dhillon: you know, especially Trou, some cases mechanical, you know, when when they need to repair cars, they would create a case for each car based upon the diagnosis of each car, and what the various steps were to fix the car and possible resolution at the end.


174
00:23:13.130 --> 00:23:26.990
Inderjit Dhillon: and they wanted to build an expert system so that the mechanic did not mechanics in trouble. Some cases did not actually have to call a central location, let's say, for Toyota or for Honda, or for you know, the particular dealership.


175
00:23:27.540 --> 00:23:34.890
Inderjit Dhillon: And in those cases they actually did want to kind of cluster these cases. right, each case being a repair case.


176
00:23:35.100 --> 00:23:38.730
Inderjit Dhillon: and one of the things that they identified was


177
00:23:38.750 --> 00:23:42.120
Inderjit Dhillon: that they actually wanted to all the cases to be very coherent.


178
00:23:42.890 --> 00:23:47.770
Inderjit Dhillon: right, the cases to be very similar to each other right. In that case


179
00:23:47.940 --> 00:23:54.190
Inderjit Dhillon: the correct distance measure ended up. You know that ended up working better, was D. Max.


180
00:23:54.470 --> 00:23:58.340
Inderjit Dhillon: Because what D Max does is it actually says that


181
00:23:58.610 --> 00:24:11.900
Inderjit Dhillon: the maximum distance between the point between the 2 cases is actually a small, because I am going to take the you know this is the distance between the clusters, but finally


182
00:24:12.010 --> 00:24:22.490
Inderjit Dhillon: I do find the nearest pair of the stream clusters. So so really you can't. But you know you can't a priori define. You know what a better


183
00:24:24.000 --> 00:24:31.980
Inderjit Dhillon: distance, measure, or distance function is clustering is often kind of a very


184
00:24:32.370 --> 00:24:38.040
Inderjit Dhillon: subjective phenomena. Right? So you know you, you.


185
00:24:38.500 --> 00:24:50.960
Inderjit Dhillon: You can have different granularities of clusters, and it often depends upon the particular job more and more that I look at, clustering many times, clustering ends up kind of being a


186
00:24:51.010 --> 00:25:01.350
Inderjit Dhillon: means to an end and not an end in itself right? It's not like people are just looking at the clusters. But, for example, in the mechanic case, people were trying to get to a resolution quickly


187
00:25:01.370 --> 00:25:06.990
Inderjit Dhillon: to the problem, and clustering was a means towards this particular end.


188
00:25:08.350 --> 00:25:12.500
Inderjit Dhillon: Now. if I look back at this algorithm


189
00:25:14.500 --> 00:25:20.580
Inderjit Dhillon: right, one of the things that you will notice is that it's a very local kind of measure, right? You just


190
00:25:20.680 --> 00:25:30.740
Inderjit Dhillon: have this notion of nearness between a pair of distant clusters. But what about an overall measure of goodness, of the clustering


191
00:25:31.700 --> 00:25:38.860
Inderjit Dhillon: right that is actually not there. In this there is no global objective function that we have used so far


192
00:25:39.450 --> 00:25:49.730
Inderjit Dhillon: in this particular out. What? But then what people then would say is, Well, let's try and get a global objective. Okay?


193
00:25:50.300 --> 00:25:58.550
Inderjit Dhillon: And one such global objective is the following: okay, and this will come up in a, you know, non-exlomerative algorithms also.


194
00:25:59.820 --> 00:26:03.500
Inderjit Dhillon: So what I can have is I can think of


195
00:26:04.980 --> 00:26:11.740
Inderjit Dhillon: an objective function, and this is kind of widely used. Let's denote this objective function by J.


196
00:26:13.610 --> 00:26:18.880
Inderjit Dhillon: Remember, there are end points. And suppose i'm trying to cluster into K. Clusters.


197
00:26:20.030 --> 00:26:31.470
Inderjit Dhillon: Then what I say is that i'm going to define the objective as a sum of objectives for each of the individual clusters. and the way I would do it is, i'll define


198
00:26:31.720 --> 00:26:38.420
Inderjit Dhillon: what's called a scatter or width of each cluster as following X minus M. I


199
00:26:38.920 --> 00:26:40.050
Inderjit Dhillon: Square.


200
00:26:40.360 --> 00:26:45.030
Inderjit Dhillon: So M. I. Is the centroid of cluster. I.


201
00:26:45.850 --> 00:26:48.290
Inderjit Dhillon: X. Belongs to C. I.


202
00:26:49.580 --> 00:26:52.510
Inderjit Dhillon: M. I. Is, as above.


203
00:26:53.870 --> 00:26:57.720
Inderjit Dhillon: Sometimes people call it centroid. Sometimes people will call it mean


204
00:26:58.590 --> 00:27:00.960
Inderjit Dhillon: of


205
00:27:01.690 --> 00:27:05.750
Inderjit Dhillon: so if I think about this this part


206
00:27:06.750 --> 00:27:09.510
Inderjit Dhillon: right? You can think of that as


207
00:27:11.100 --> 00:27:12.120
Inderjit Dhillon: the


208
00:27:13.400 --> 00:27:14.620
Inderjit Dhillon: scatter


209
00:27:16.440 --> 00:27:17.980
Inderjit Dhillon: or with


210
00:27:19.550 --> 00:27:22.360
Inderjit Dhillon: of Cluster I


211
00:27:24.750 --> 00:27:28.720
Inderjit Dhillon: and what i'm trying to do is get a clustering


212
00:27:29.340 --> 00:27:33.760
Inderjit Dhillon: such that the average width or the sum of the widths


213
00:27:33.780 --> 00:27:35.080
Inderjit Dhillon: of the clusters


214
00:27:37.350 --> 00:27:43.300
Inderjit Dhillon: is minimized. Okay. So the goal? Well, the idea is


215
00:27:45.070 --> 00:27:47.790
Inderjit Dhillon: goal. Find


216
00:27:49.940 --> 00:27:51.310
Inderjit Dhillon: clustering


217
00:27:53.880 --> 00:27:54.850
Inderjit Dhillon: with


218
00:27:56.220 --> 00:27:57.870
Inderjit Dhillon: hey? Clusters


219
00:27:59.720 --> 00:28:03.700
Inderjit Dhillon: that minimizes this objective function.


220
00:28:05.600 --> 00:28:08.110
Inderjit Dhillon: Okay, so minimize this.


221
00:28:10.300 --> 00:28:11.120
Inderjit Dhillon: Okay.


222
00:28:12.040 --> 00:28:15.260
Inderjit Dhillon: So we'll actually come to this later


223
00:28:15.320 --> 00:28:16.270
Inderjit Dhillon: will


224
00:28:18.710 --> 00:28:24.560
Inderjit Dhillon: to this later later on. In class we look at a particular algorithm but right now.


225
00:28:24.600 --> 00:28:28.790
Inderjit Dhillon: let's suppose that we are still in this agglomerative framework.


226
00:28:29.280 --> 00:28:32.990
Inderjit Dhillon: Right? So suppose we are doing agglomerative clustering.


227
00:28:41.940 --> 00:28:46.080
Inderjit Dhillon: Okay. And then suppose I want to try and get an algorithm


228
00:28:50.010 --> 00:28:52.850
Inderjit Dhillon: that greedily optimizes, J.


229
00:28:54.700 --> 00:28:55.950
Inderjit Dhillon: Readily


230
00:28:57.240 --> 00:29:01.000
Inderjit Dhillon: optimizes the subjective function or criterion.


231
00:29:05.030 --> 00:29:08.340
Inderjit Dhillon: Well, what does it mean? Let's go back and look at the


232
00:29:09.060 --> 00:29:10.360
Inderjit Dhillon: or I algorithm


233
00:29:11.940 --> 00:29:15.680
Inderjit Dhillon: right? We have. This is the big iteration.


234
00:29:16.660 --> 00:29:20.200
Inderjit Dhillon: and at every step we want to try and mode


235
00:29:21.640 --> 00:29:24.620
Inderjit Dhillon: 2 clusters sub clusters into one cluster.


236
00:29:25.860 --> 00:29:33.540
Inderjit Dhillon: So if I have an objective function over here, I didn't really have an a global object. I was just using some local. you know.


237
00:30:25.100 --> 00:30:26.540
Inderjit Dhillon: Hi, Can you hear me?


238
00:30:29.760 --> 00:30:35.130
Jeongyoon Moon: Yeah, we can. I can hear you now. Yeah, Did Did I get disconnected for a little bit or


239
00:30:36.400 --> 00:30:41.250
Inderjit Dhillon: for like a couple of seconds. Yes.


240
00:30:44.930 --> 00:30:46.460
Christopher Lawson: I believe it was just you.


241
00:30:46.500 --> 00:30:50.100
Inderjit Dhillon: Okay, Not sure what exactly happened. Okay.


242
00:30:51.390 --> 00:30:53.340
Inderjit Dhillon: Okay. Sorry about that.


243
00:30:53.840 --> 00:31:00.010
Inderjit Dhillon: So let's see. I guess what I was saying was that


244
00:31:01.220 --> 00:31:12.300
Inderjit Dhillon: you know, over here there is no objective function. But suppose now we have this objective. so one of the things I can stay over here is that, hey? If I have a global objective


245
00:31:13.600 --> 00:31:20.910
Inderjit Dhillon: right now. just think of this global objective right? If I


246
00:31:22.510 --> 00:31:28.520
Inderjit Dhillon: have this objective. you know, the number of clusters kind of is different at every step.


247
00:31:30.110 --> 00:31:32.890
Inderjit Dhillon: So what I can say is that


248
00:31:33.150 --> 00:31:38.140
Inderjit Dhillon: i'm going to try and do a merge such that.


249
00:31:39.900 --> 00:31:48.230
Inderjit Dhillon: and if you, if you think about it and we'll come to it whenever you do a merge right, the objective function will actually


250
00:31:49.930 --> 00:32:01.310
Inderjit Dhillon: increase slightly because we are decreasing the number of clusters. So, for example, over here. if we start off with end clusters where you know this K. Is equal to N.


251
00:32:01.800 --> 00:32:06.120
Inderjit Dhillon: Right, then each point could be its own centroid.


252
00:32:06.240 --> 00:32:09.670
Inderjit Dhillon: and this objective function will evaluate to 0.


253
00:32:10.620 --> 00:32:13.200
Inderjit Dhillon: But then, as soon as we do one merge.


254
00:32:13.450 --> 00:32:17.730
Inderjit Dhillon: i'll get n minus one clusters, so K. Will become n minus one.


255
00:32:18.350 --> 00:32:22.040
Inderjit Dhillon: and all the clusters


256
00:32:23.420 --> 00:32:28.560
Inderjit Dhillon: except the merged cluster that contains 2 points will have


257
00:32:29.130 --> 00:32:31.850
Inderjit Dhillon: 0 with or 0 scatter.


258
00:32:32.450 --> 00:32:36.620
Inderjit Dhillon: So every time I do a merge. The objective function


259
00:32:37.150 --> 00:32:43.450
Inderjit Dhillon: with that many number of clusters is actually going to increase. So what I can think about doing is.


260
00:32:43.630 --> 00:32:47.850
Inderjit Dhillon: I want to try. Get that move. I want to make that merge


261
00:32:48.160 --> 00:32:51.540
Inderjit Dhillon: that increases the


262
00:32:52.140 --> 00:32:56.090
Inderjit Dhillon: objective function by the smallest possible amount.


263
00:32:56.720 --> 00:33:04.110
Inderjit Dhillon: So I've said that in a lot of words. So let's let's see what that means. Okay, so let's go into detail.


264
00:33:05.460 --> 00:33:14.310
Inderjit Dhillon: Suppose I have clusters, I and clusters L. So suppose I say that? J. I:


265
00:33:14.920 --> 00:33:21.740
Inderjit Dhillon: Okay. So J: I is going to be the objective function for cluster. I:


266
00:33:22.680 --> 00:33:28.860
Inderjit Dhillon: Okay. So J: I is equal to summation of X belongs to C. I.


267
00:33:29.320 --> 00:33:34.210
Inderjit Dhillon: X minus M. I. Where? Okay?


268
00:33:34.250 --> 00:33:37.970
Inderjit Dhillon: And by the way, i'm just using the 2 norm over here.


269
00:33:38.240 --> 00:33:43.780
Inderjit Dhillon: and we'll see that actually makes a difference, and make things a little bit easier.


270
00:33:44.800 --> 00:33:57.230
Inderjit Dhillon: and then I have the it cluster. I have the ellipt Cluster X belongs to Cl. X. Minus M. Oh. it's great.


271
00:34:12.260 --> 00:34:14.159
Inderjit Dhillon: Okay. So


272
00:34:14.670 --> 00:34:16.550
Inderjit Dhillon: now suppose we


273
00:34:18.900 --> 00:34:20.190
Inderjit Dhillon: march.


274
00:34:21.350 --> 00:34:23.800
Inderjit Dhillon: c. I and Cj.


275
00:34:25.139 --> 00:34:31.920
Inderjit Dhillon: Okay. Well, when we remember C, I had the mean, M. I


276
00:34:33.120 --> 00:34:36.520
Inderjit Dhillon: I'm: sorry. Cl: we are moding, c. I and Cl


277
00:34:42.540 --> 00:34:47.270
Inderjit Dhillon: and the elite cluster has mean Ml. Or centroid Ml.


278
00:34:48.659 --> 00:34:52.030
Inderjit Dhillon: When we merge the 2, then i'll get a new me.


279
00:34:55.060 --> 00:34:58.240
Inderjit Dhillon: and what will the new mean? B.


280
00:34:59.290 --> 00:35:11.360
Inderjit Dhillon: Well, remember, the mean is just the average of all the points right? It's the sum of all the points divided by the total number of points. Okay. So if I denote the new mean by M. I. Hat.


281
00:35:12.020 --> 00:35:19.280
Inderjit Dhillon: this will be an I am I as M. L. M. L.


282
00:35:19.800 --> 00:35:22.570
Inderjit Dhillon: Divided by N. I plus email.


283
00:35:26.580 --> 00:35:30.460
Inderjit Dhillon: Okay. So remember that the means am, I


284
00:35:31.120 --> 00:35:35.780
Inderjit Dhillon: is equal to summation of X. X. Belongs to


285
00:35:36.150 --> 00:35:40.540
Inderjit Dhillon: just to I one divided by. and I


286
00:35:44.800 --> 00:35:45.560
Inderjit Dhillon: Okay.


287
00:35:47.530 --> 00:35:57.210
Inderjit Dhillon: It should be obvious why this is true. Remember that, you know when I do, and I am I I just get the sum of all the points.


288
00:35:57.340 --> 00:36:01.880
Inderjit Dhillon: So the numerator is basically just the sum of all the points, and the denominator is


289
00:36:01.970 --> 00:36:14.580
Inderjit Dhillon: now the new number of points in the cluster after we merge them. I can write this as in 2 different ways, and it'll turn out to be helpful when we


290
00:36:14.710 --> 00:36:27.670
Inderjit Dhillon: do the analysis. I can write this as mi. and then a change to the M. I. It to the to that centroid. and the changes and L divided by


291
00:36:28.240 --> 00:36:32.790
Inderjit Dhillon: M. I plus Ml. Times, Ml: minus M. I.


292
00:36:34.220 --> 00:36:35.870
Inderjit Dhillon: It's basically saying.


293
00:36:35.980 --> 00:36:43.240
Inderjit Dhillon: i'm changing, am I to this value? And you can easily verify that. For example, when I, you know.


294
00:36:43.450 --> 00:36:46.610
Inderjit Dhillon: Look at the sum I get Ni m I


295
00:36:48.330 --> 00:36:49.150
Inderjit Dhillon: and


296
00:36:49.280 --> 00:36:56.670
Inderjit Dhillon: an Lml, and then I get an l m I minus mlm i, so that will cancel out.


297
00:36:56.890 --> 00:37:02.020
Inderjit Dhillon: Okay. and then I can say, okay, I can ultimately alter it as a change


298
00:37:02.070 --> 00:37:05.380
Inderjit Dhillon: to the edit cluster. And similarly, it will be


299
00:37:06.210 --> 00:37:09.510
Inderjit Dhillon: the change will be this.


300
00:37:14.900 --> 00:37:15.750
Inderjit Dhillon: Okay.


301
00:37:16.110 --> 00:37:26.380
Inderjit Dhillon: So if I do this merge, then initially, my objective function was, you know, among other things that had Ji plus Jl.


302
00:37:27.740 --> 00:37:31.080
Inderjit Dhillon: Okay. Now, the new objective


303
00:37:34.320 --> 00:37:35.910
Inderjit Dhillon: after merging


304
00:37:39.840 --> 00:37:45.850
Inderjit Dhillon: will be the following: right: I basically take X minus M. I hat.


305
00:37:46.190 --> 00:37:48.630
Inderjit Dhillon: So that's the new cluster.


306
00:37:49.810 --> 00:37:51.000
Inderjit Dhillon: So I enjoyed.


307
00:37:52.310 --> 00:38:03.280
Inderjit Dhillon: This is all the points that belong to the old cluster I and all the points that belong to the nuclei. The old cluster L.


308
00:38:04.240 --> 00:38:06.600
Inderjit Dhillon: Now the new cluster is


309
00:38:07.700 --> 00:38:10.370
Inderjit Dhillon: M. I. Cat.


310
00:38:14.910 --> 00:38:23.920
Inderjit Dhillon: Okay. Hopefully, that's clear. This is just to make sure. I'm not confusing you.


311
00:38:25.600 --> 00:38:28.420
Inderjit Dhillon: I should make sure this is M. I hat.


312
00:38:31.550 --> 00:38:34.130
Inderjit Dhillon: Okay. this is the new centroid


313
00:38:35.530 --> 00:38:37.190
Inderjit Dhillon: for all the points


314
00:38:38.520 --> 00:38:43.230
Inderjit Dhillon: that were in cluster. I that now belong to the merge cluster.


315
00:38:43.440 --> 00:38:49.650
Inderjit Dhillon: I'm. Not looking at the distance to the new centroid similarly plus for L.


316
00:38:49.740 --> 00:38:53.740
Inderjit Dhillon: But looking at the new centroid. But the new centroid


317
00:38:53.770 --> 00:38:58.940
Inderjit Dhillon: has this form. I can think of it as a change. Who am I?


318
00:38:59.260 --> 00:39:04.760
Inderjit Dhillon: And a change or a change to Ml: so the reason I've done it is


319
00:39:05.130 --> 00:39:14.580
Inderjit Dhillon: so that we can compare to the previous values of the objective function. So for I, for points that belong to I I can write it as


320
00:39:15.220 --> 00:39:17.500
Inderjit Dhillon: X minus.


321
00:39:17.960 --> 00:39:26.710
Inderjit Dhillon: No, i'm going to just replace this value. This over i'll just substitute over here. Okay. So it is


322
00:39:27.500 --> 00:39:30.110
Inderjit Dhillon: M. I. Minus


323
00:39:31.750 --> 00:39:39.280
Inderjit Dhillon: and L. Divided by N. I plus and L. Ml. Minus M. I.


324
00:39:41.630 --> 00:39:43.460
Inderjit Dhillon: So you can see that I have


325
00:39:44.170 --> 00:39:51.170
Inderjit Dhillon: this. and then I have a change. And this was what was there originally? Right?


326
00:39:51.450 --> 00:39:59.360
Inderjit Dhillon: Similarly, for points and cluster. L. I'll have X minus. I'm not going to copy


327
00:39:59.610 --> 00:40:03.660
Inderjit Dhillon: this down over here. Maybe I actually I should just write this.


328
00:40:04.060 --> 00:40:07.070
Inderjit Dhillon: So it's basically this


329
00:40:08.180 --> 00:40:10.140
Inderjit Dhillon: is over here.


330
00:40:11.250 --> 00:40:12.400
Inderjit Dhillon: and


331
00:40:13.010 --> 00:40:17.170
Inderjit Dhillon: this i'm gonna put over to one line.


332
00:40:18.950 --> 00:40:21.100
Inderjit Dhillon: This I'm going to put over here.


333
00:40:23.510 --> 00:40:25.760
Inderjit Dhillon: What is this Ml.


334
00:40:26.520 --> 00:40:33.630
Inderjit Dhillon: minus? And I divided by N. I plus n L. M. I.


335
00:40:38.470 --> 00:40:46.920
Inderjit Dhillon: Okay. And you can see again. This is like the previous value, because Ml. Was the world centroid


336
00:40:48.500 --> 00:40:50.110
Inderjit Dhillon: and a slight change.


337
00:40:52.800 --> 00:41:03.180
Inderjit Dhillon: So I can let me just quickly try and do this. If I do the algebra, if it becomes, you know this is the 2 norm, remember. So the 2 norm I can write as


338
00:41:03.700 --> 00:41:16.680
Inderjit Dhillon: x minus M. I square plus, you know it's essentially like a minus B squared is equal to a square plus b square minus 2 a. B. That's essentially what i'm doing.


339
00:41:17.110 --> 00:41:21.570
Inderjit Dhillon: So I have nl square divided by N. I plus ml


340
00:41:22.140 --> 00:41:26.530
Inderjit Dhillon: square times, M. L. Minus M. I


341
00:41:26.780 --> 00:41:34.140
Inderjit Dhillon: square, and then I have the cross term. Okay. and the cross term is minus 2


342
00:41:34.600 --> 00:41:38.230
Inderjit Dhillon: and L divided by Ni plus ml


343
00:41:38.240 --> 00:41:41.020
Inderjit Dhillon: Ml. Minus m. I


344
00:41:42.810 --> 00:41:44.680
Inderjit Dhillon: transpose with


345
00:41:45.300 --> 00:41:47.320
Inderjit Dhillon: X minus M. I.


346
00:41:48.950 --> 00:41:52.340
Inderjit Dhillon: That's the first term, and then the second term


347
00:41:54.570 --> 00:41:59.870
Inderjit Dhillon: is very similar, except it is x minus M.


348
00:42:00.130 --> 00:42:02.590
Inderjit Dhillon: L. Square


349
00:42:03.110 --> 00:42:07.850
Inderjit Dhillon: plus M. I square, divided by n I plus ml


350
00:42:08.290 --> 00:42:11.860
Inderjit Dhillon: square Ml: minus m. I


351
00:42:13.080 --> 00:42:26.930
Inderjit Dhillon: square minus 2, and I divided my Ni plus ml m I minus m l transpose times x minus. And


352
00:42:28.020 --> 00:42:30.100
Inderjit Dhillon: okay. Now.


353
00:42:31.700 --> 00:42:36.970
Inderjit Dhillon: if I look over here at this term. this is actually just the same as jail.


354
00:42:39.490 --> 00:42:40.520
Inderjit Dhillon: Yeah.


355
00:42:42.590 --> 00:42:45.650
Inderjit Dhillon: okay. And similarly, this is J. L.


356
00:42:47.150 --> 00:42:51.420
Inderjit Dhillon: So I basically. Have. You know, J. I. Over here


357
00:42:52.920 --> 00:43:03.810
Inderjit Dhillon: J. L. Over here, and then I have this additional part and this additional part, this additional part. this additional part, and this additional part.


358
00:43:04.100 --> 00:43:05.660
Inderjit Dhillon: Let's look at this part.


359
00:43:06.830 --> 00:43:15.330
Inderjit Dhillon: Well, this is X minus M. I. And the summation is over. All the points belong to in cluster. I.


360
00:43:17.390 --> 00:43:23.220
Inderjit Dhillon: Okay, that's the only point that depends upon the terms in the summation. The rest, in some sense, are constant.


361
00:43:24.020 --> 00:43:25.020
Inderjit Dhillon: But


362
00:43:25.400 --> 00:43:30.710
Inderjit Dhillon: if you think about it. X Minus M. I. Because M. I. Is the average


363
00:43:30.980 --> 00:43:34.780
Inderjit Dhillon: summed over all x or 4 points in the cluster.


364
00:43:34.940 --> 00:43:38.260
Inderjit Dhillon: This evaluates to 0


365
00:43:39.830 --> 00:43:43.530
Inderjit Dhillon: This equals 0, And similarly over here


366
00:43:43.710 --> 00:43:52.030
Inderjit Dhillon: you have X minus Ml. Summation is overall. Points belong to L. These are essentially constants.


367
00:43:52.140 --> 00:43:55.870
Inderjit Dhillon: and X minus Ml. Is 0.


368
00:43:57.360 --> 00:43:59.150
Inderjit Dhillon: So these 2 cancel out


369
00:43:59.740 --> 00:44:06.350
Inderjit Dhillon: or or sorry they they become zeros. And now i'm left with these additional terms.


370
00:44:08.350 --> 00:44:09.110
Inderjit Dhillon: Okay.


371
00:44:09.180 --> 00:44:17.570
Inderjit Dhillon: So let's see what those evaluate to. Okay, so what I have is I have J. I plus J. L plus. I have


372
00:44:17.770 --> 00:44:18.820
Inderjit Dhillon: no


373
00:44:25.070 --> 00:44:28.160
Inderjit Dhillon: Now think about this. Let's let's look at this part.


374
00:44:28.660 --> 00:44:30.080
Inderjit Dhillon: This


375
00:44:31.120 --> 00:44:37.470
Inderjit Dhillon: it does not depend upon X belongs to Ci. It's actually a constant.


376
00:44:38.970 --> 00:44:41.790
Inderjit Dhillon: How many terms are there in the summation? And I


377
00:44:43.280 --> 00:44:47.970
Inderjit Dhillon: So this value will actually be just N. I. Times this value.


378
00:44:48.400 --> 00:44:53.120
Inderjit Dhillon: Similarly, this is going to be an L. Times. This value.


379
00:44:53.220 --> 00:44:57.480
Inderjit Dhillon: Okay. so it will be. and I


380
00:44:58.470 --> 00:45:02.600
Inderjit Dhillon: and L. Square divided by N. I plus nl


381
00:45:03.270 --> 00:45:11.540
Inderjit Dhillon: square Ml. Minus m I square. and then this part. So that's this part.


382
00:45:13.630 --> 00:45:16.280
Inderjit Dhillon: and then this part is.


383
00:45:16.350 --> 00:45:18.750
Inderjit Dhillon: and elsewhere, and I


384
00:45:19.510 --> 00:45:22.040
Inderjit Dhillon: sorry, and I Times and elsewhere.


385
00:45:24.670 --> 00:45:27.820
Inderjit Dhillon: and I times and elsewhere.


386
00:45:29.190 --> 00:45:36.440
Inderjit Dhillon: divided by N. I. Plus Ml. Where M. L. Minus m by.


387
00:45:37.000 --> 00:45:37.770
But


388
00:45:41.980 --> 00:45:47.140
Inderjit Dhillon: now again, if you kind of simplify this this part.


389
00:45:48.420 --> 00:45:53.130
Inderjit Dhillon: and add this with this you'll see that


390
00:45:53.190 --> 00:45:56.410
Inderjit Dhillon: you know this is the same right. This is the same.


391
00:45:57.030 --> 00:46:01.040
Inderjit Dhillon: So I get, an I and L square


392
00:46:01.580 --> 00:46:05.800
Inderjit Dhillon: divided by an I plus nl square plus


393
00:46:06.370 --> 00:46:10.990
Inderjit Dhillon: an il square divided by an I plus nl square


394
00:46:12.910 --> 00:46:14.230
Inderjit Dhillon: times


395
00:46:14.360 --> 00:46:17.090
Inderjit Dhillon: m I minus Mj.


396
00:46:18.760 --> 00:46:19.570
Inderjit Dhillon: So well.


397
00:46:20.690 --> 00:46:22.520
Inderjit Dhillon: and if I look at this


398
00:46:23.160 --> 00:46:28.390
Inderjit Dhillon: it'll be an I and L. Times ml plus m I


399
00:46:29.230 --> 00:46:34.630
Inderjit Dhillon: and I plus nl square. So one of these queries will cancel with this.


400
00:46:37.920 --> 00:46:43.400
Inderjit Dhillon: and what I will get is J. I.


401
00:46:44.050 --> 00:46:48.780
Inderjit Dhillon: And I M. L. Divided by 9 plus Ml.


402
00:46:49.310 --> 00:46:53.940
Inderjit Dhillon: Times M. I minus Mj. Square.


403
00:46:55.340 --> 00:47:01.870
Inderjit Dhillon: Okay. So so I did all this lot of algebra, and it actually is nice, because it actually simplified


404
00:47:02.000 --> 00:47:07.380
Inderjit Dhillon: to saying that the objective function actually increases by this amount.


405
00:47:08.860 --> 00:47:12.480
Inderjit Dhillon: So what I can say is, I can define a new distance measure.


406
00:47:12.920 --> 00:47:20.550
Inderjit Dhillon: Let me just call it as opt, because this optimizes this is the objective function.


407
00:47:20.750 --> 00:47:24.580
Inderjit Dhillon: and since there's a square, and I just want it to be in distance.


408
00:47:24.740 --> 00:47:31.750
Inderjit Dhillon: and all our you know, over here. When we talked about things we were all talking about distances and not squares.


409
00:47:33.540 --> 00:47:36.950
Inderjit Dhillon: Then I can take the square root of this quantity. I can say


410
00:47:37.200 --> 00:47:43.090
Inderjit Dhillon: it is square root, and I and L


411
00:47:43.400 --> 00:47:47.180
Inderjit Dhillon: divided by N. I plus Ml. Times.


412
00:47:47.820 --> 00:47:50.020
Inderjit Dhillon: M. I. Minus Mj.


413
00:47:56.460 --> 00:48:06.740
Inderjit Dhillon: So you can see that by defining a global objective function I can actually get an objective function, all of our criterion for distance, which is very similar.


414
00:48:07.710 --> 00:48:10.250
Just look at this right. I mean, this is.


415
00:48:11.050 --> 00:48:13.140
Inderjit Dhillon: am I minus Mj.


416
00:48:14.320 --> 00:48:19.480
Inderjit Dhillon: And if you look over here. We also have the mean as M. I. Minus Mt.


417
00:48:20.290 --> 00:48:21.320
Inderjit Dhillon: but


418
00:48:23.690 --> 00:48:26.950
Inderjit Dhillon: it actually takes the number of points in the cluster.


419
00:48:28.730 --> 00:48:38.270
Inderjit Dhillon: Okay. So this has the so. So what this means is that the smaller this value is


420
00:48:39.790 --> 00:48:42.200
Inderjit Dhillon: okay. the more


421
00:48:43.140 --> 00:48:48.270
Inderjit Dhillon: those are the clusters that will be merged, and you can go through an exercise.


422
00:48:48.350 --> 00:48:59.050
Inderjit Dhillon: and you can see that this criterion will actually tend to favor clusters where M. I and Nl. Are about the same.


423
00:48:59.060 --> 00:48:59.860
Inderjit Dhillon: If


424
00:49:00.130 --> 00:49:14.170
Inderjit Dhillon: am I and Mg. Are not for, or am I? And so if you have 2 pairs of clusters, so so, maybe maybe I can illustrate it right. So if I have. let's say, one cluster with just 1 point.


425
00:49:15.390 --> 00:49:17.710
Inderjit Dhillon: and the other with


426
00:49:17.800 --> 00:49:29.240
Inderjit Dhillon: 99 points. So suppose I have n I equals. One. Nl. Is 99 as opposed to the case where I have.


427
00:49:31.750 --> 00:49:40.640
Inderjit Dhillon: and I equals 50 and L equals 50. And suppose that am I an Mj.


428
00:49:41.860 --> 00:49:46.650
Inderjit Dhillon: M. I. And Mj. We're the same distance.


429
00:49:46.770 --> 00:49:48.750
Inderjit Dhillon: M. I m day.


430
00:49:49.330 --> 00:49:53.330
Inderjit Dhillon: so both m I minus Mj. In both the cases are the same.


431
00:49:53.670 --> 00:49:58.100
Inderjit Dhillon: but in one case the distance will be square root of N. I.


432
00:49:58.910 --> 00:50:05.710
Inderjit Dhillon: And J. Divided by not plus Nj. Times M I. Minus Mj.


433
00:50:06.390 --> 00:50:11.510
Inderjit Dhillon: And that'll be square root of 99 divided by


434
00:50:11.830 --> 00:50:15.230
Inderjit Dhillon: 100 right so one times 99


435
00:50:15.520 --> 00:50:19.580
Inderjit Dhillon: times one plus 99, because over here


436
00:50:19.950 --> 00:50:24.640
Inderjit Dhillon: it will be 50 times 50, divided by 100.


437
00:50:25.680 --> 00:50:29.580
Inderjit Dhillon: So this is smaller. This is larger.


438
00:50:30.560 --> 00:50:34.040
Inderjit Dhillon: so it will end up merging these 2.


439
00:50:35.430 --> 00:50:39.580
Inderjit Dhillon: As a result, they won't be imbalanced clusters.


440
00:50:41.950 --> 00:50:50.820
Inderjit Dhillon: so it ends, tends to. even though superficially, this this objective looks very similar


441
00:50:51.940 --> 00:50:59.210
Inderjit Dhillon: to this objective This will tend to favor. so the opt


442
00:51:00.990 --> 00:51:02.410
Inderjit Dhillon: tends to.


443
00:51:04.030 --> 00:51:05.200
Inderjit Dhillon: We were


444
00:51:08.840 --> 00:51:10.950
Inderjit Dhillon: of equal sizes.


445
00:51:13.530 --> 00:51:18.700
Inderjit Dhillon: All right. Doesn't mean that you won't. Get imbalance. For this also depends upon Mi and Mt.


446
00:51:22.000 --> 00:51:23.640
Inderjit Dhillon: So any questions so far.


447
00:51:34.570 --> 00:51:38.910
Inderjit Dhillon: Okay. So a nominative algorithms.


448
00:51:39.020 --> 00:51:45.310
Inderjit Dhillon: Yes, I mean people use them. There is a drawback to the collaborative algorithms in that


449
00:51:45.760 --> 00:51:50.510
Inderjit Dhillon: the competition can actually tend to be quite large. because if you think about it


450
00:51:51.530 --> 00:51:54.460
Inderjit Dhillon: when you do this nearest pair of clusters.


451
00:51:57.470 --> 00:52:02.280
Inderjit Dhillon: In the beginning there are end clusters. So you need to do n square competition


452
00:52:03.660 --> 00:52:08.880
Inderjit Dhillon: and you have n kind of steps. This can actually be a cubic time. Algorithm


453
00:52:10.070 --> 00:52:11.700
Inderjit Dhillon: It can be quite expensive


454
00:52:13.140 --> 00:52:16.100
Inderjit Dhillon: because you're building up the clusters from bottom up.


455
00:52:17.400 --> 00:52:20.010
Inderjit Dhillon: So this is also sometimes called


456
00:52:21.600 --> 00:52:22.790
Inderjit Dhillon: bottom up.


457
00:52:24.510 --> 00:52:25.480
Inderjit Dhillon: So, strength


458
00:52:30.880 --> 00:52:35.740
Inderjit Dhillon: Well, what are the other alternatives we have? Can we actually do top down first?


459
00:52:36.880 --> 00:52:45.320
Inderjit Dhillon: And it turns out that we can. And so there are these algorithms called or additional algorithms.


460
00:52:52.060 --> 00:52:54.270
Inderjit Dhillon: and they produce top down.


461
00:53:01.320 --> 00:53:07.550
Inderjit Dhillon: So the most kind of well known or famous algorithm is something called the k-means.


462
00:53:09.080 --> 00:53:14.510
Inderjit Dhillon: so you'll see that quite a lot of times. Okay, it's algorithm


463
00:53:16.660 --> 00:53:25.740
Inderjit Dhillon: okay. So there's an algorithm which we'll get to it. And then there's a k-means objective. So the k-means algorithm tries to optimize


464
00:53:27.000 --> 00:53:38.960
Inderjit Dhillon: the K. Means all good. So the came in. The I'm. Trying to optimize the key means objective. One thing that I should let you know. I think maybe I should have mentioned it a little bit above is


465
00:53:39.510 --> 00:53:47.700
Inderjit Dhillon: so there's There's a many, many, many different possibilities right for clustering. So suppose if you have


466
00:53:48.190 --> 00:53:53.900
Inderjit Dhillon: and points, and if you like to try and look at, you know 2 clusters


467
00:53:54.340 --> 00:54:01.890
Inderjit Dhillon: for these endpoints. Well, to each point you can assign 2 possible values, right, whether it belongs to cluster one or close to 2.


468
00:54:02.160 --> 00:54:05.760
Inderjit Dhillon: So there are an exponentially many possible plus


469
00:54:06.070 --> 00:54:14.950
Inderjit Dhillon: to the power. And for 2 clusters. As a result, all these problems are, you know, very hard


470
00:54:15.010 --> 00:54:17.980
Inderjit Dhillon: to get global optimizers.


471
00:54:18.380 --> 00:54:22.760
Inderjit Dhillon: They're pretty much all np-hard problems.


472
00:54:23.010 --> 00:54:30.260
Inderjit Dhillon: So we are trying to get heuristics. So the glomerative algorithms. we're one heuristic


473
00:54:31.040 --> 00:54:34.810
Inderjit Dhillon: and the partitional algorithms are another heuristic.


474
00:54:35.170 --> 00:54:39.350
Inderjit Dhillon: So the K-means objective function can means objective function


475
00:54:39.840 --> 00:54:47.680
Inderjit Dhillon: is actually something that we've already seen. It's the exact same objective that I showed to you before over here.


476
00:54:55.980 --> 00:54:58.840
Inderjit Dhillon: Okay. And remember, I said, we'll come to this later.


477
00:54:58.950 --> 00:55:05.210
Inderjit Dhillon: So the glomerative clustering algorithm which uses the subjective tries to make local local decisions


478
00:55:05.780 --> 00:55:10.180
Inderjit Dhillon: right to try and get the value. But if you, it's not directly


479
00:55:10.250 --> 00:55:15.330
Inderjit Dhillon: trying to optimize this functions, whereas the key means objective function priced directly


480
00:55:15.940 --> 00:55:21.410
Inderjit Dhillon: decrease this objective functions. Okay, so let me write the objective from before


481
00:55:21.680 --> 00:55:33.540
Inderjit Dhillon: I equals one to K. So. looking at the K clusters and then looking at the scatter or the with of each cluster, using the criterion that we have before.


482
00:55:45.760 --> 00:55:56.670
Inderjit Dhillon: Okay, and one important thing that I would like you to notice that the K-means objective function is this. And then there is a I many times people will conflate the 2.


483
00:55:57.170 --> 00:56:04.090
Inderjit Dhillon: But the K-means objective function is what the key means algorithm tries to


484
00:56:04.250 --> 00:56:07.510
Inderjit Dhillon: optimize, but because there are


485
00:56:07.850 --> 00:56:12.270
Inderjit Dhillon: an exponential number of possibilities. It's an Np-hard problem.


486
00:56:12.590 --> 00:56:17.570
Inderjit Dhillon: It does not guarantee that it minimizes. The key means object different


487
00:56:21.540 --> 00:56:23.120
Inderjit Dhillon: any questions so far.


488
00:56:25.310 --> 00:56:33.530
Inderjit Dhillon: So it came in. The algorithm. Is, you know, it's it's a very simple looking algorithm. But actually it works really quite well in many cases.


489
00:56:34.210 --> 00:56:37.080
Inderjit Dhillon: So let me describe the gaming's. Algorithm


490
00:56:39.450 --> 00:56:43.770
Inderjit Dhillon: there's an initialization where you start with some partitioning.


491
00:56:44.930 --> 00:56:49.750
Inderjit Dhillon: and there are some heuristics that work well. But for now we'll just kind of


492
00:56:50.360 --> 00:56:52.940
Inderjit Dhillon: assume that you know you can either


493
00:56:52.960 --> 00:56:57.000
Inderjit Dhillon: randomly assigned points to clusters


494
00:56:58.830 --> 00:57:05.710
Inderjit Dhillon: or randomly choose case centroids and assign points to the corresponding clusters.


495
00:57:07.930 --> 00:57:15.050
Inderjit Dhillon: Okay, so first is kind of the initialization. Start with some partitioning of the data into K. Clusters.


496
00:57:16.770 --> 00:57:26.510
Inderjit Dhillon: Okay. So suppose you have. So which means. let me actually denote the kick clusters as


497
00:57:28.540 --> 00:57:30.870
Inderjit Dhillon: C, one.


498
00:57:31.550 --> 00:57:42.060
Inderjit Dhillon: C, 2, 3, K. And since these clusters will change every time. Step. I'll put a time step over here.


499
00:57:47.640 --> 00:57:58.410
Inderjit Dhillon: Okay, and i'll say that t is set to be equal to 0. T is the iteration count? Okay. So once we start with some conditioning.


500
00:57:58.490 --> 00:58:01.130
Inderjit Dhillon: I will calculate the mean of each cluster


501
00:58:06.980 --> 00:58:08.720
Inderjit Dhillon: or centride


502
00:58:09.750 --> 00:58:11.540
Inderjit Dhillon: of each close to.


503
00:58:16.510 --> 00:58:19.580
Inderjit Dhillon: Okay. So we have, c. I. T


504
00:58:20.580 --> 00:58:26.930
Inderjit Dhillon: right. So the mean of thruster I will be denoted by M. I. T.


505
00:58:27.280 --> 00:58:31.100
Inderjit Dhillon: And this is just one divided by an I


506
00:58:31.820 --> 00:58:39.030
Inderjit Dhillon: summation of X. X. Belongs to C. I but remember we have a superscript D.


507
00:58:39.220 --> 00:58:42.810
Inderjit Dhillon: For each of the time steps right so at


508
00:58:43.100 --> 00:58:47.710
Inderjit Dhillon: at step D. The clusters are cits.


509
00:58:47.870 --> 00:58:53.060
Inderjit Dhillon: and the number of points within each cluster is an it so? N. I.


510
00:58:53.260 --> 00:58:54.200
Inderjit Dhillon: The


511
00:58:54.540 --> 00:58:59.570
Inderjit Dhillon: is the absolute value of the set, so the number of elements


512
00:58:59.600 --> 00:59:01.300
Inderjit Dhillon: in the cluster.


513
00:59:02.440 --> 00:59:05.120
Inderjit Dhillon: So that's the first step that you do in this algorithm


514
00:59:06.150 --> 00:59:10.720
Inderjit Dhillon: and then the second step, the next step. So this was initialization.


515
00:59:11.570 --> 00:59:17.360
Inderjit Dhillon: Then you compute the centroid of each cluster. and then each point does the following.


516
00:59:18.360 --> 00:59:20.460
Inderjit Dhillon: E. X. Says.


517
00:59:21.420 --> 00:59:23.280
Inderjit Dhillon: What is my nearest android?


518
00:59:27.290 --> 00:59:30.930
Inderjit Dhillon: Okay. So remember, this point is actually in a


519
00:59:32.100 --> 00:59:34.010
Inderjit Dhillon: The cluster has a centroid.


520
00:59:35.020 --> 00:59:38.480
Inderjit Dhillon: but it's not necessary, and we'll see with an example


521
00:59:38.990 --> 00:59:47.070
Inderjit Dhillon: that the centroid of its cluster is the nearest centroid that it belongs nearer Android


522
00:59:47.150 --> 00:59:48.590
Inderjit Dhillon: to that point.


523
00:59:50.840 --> 01:00:00.420
Inderjit Dhillon: So let me. Maybe. Let's see. Maybe I can take an example over here right? So suppose I have. I have


524
01:00:04.030 --> 01:00:05.770
Inderjit Dhillon: these points.


525
01:00:12.550 --> 01:00:14.590
Inderjit Dhillon: How many clusters do you see? By the way.


526
01:00:16.220 --> 01:00:22.040
Inderjit Dhillon: just so that I know people are listening. Maybe somebody can say how many they see on the screen


527
01:00:22.260 --> 01:00:24.910
Inderjit Dhillon: 2 clusters, right? You see 2 clusters. But


528
01:00:25.080 --> 01:00:31.010
Inderjit Dhillon: the algorithm needs to find these out right. So suppose the algorithm actually has


529
01:00:32.450 --> 01:00:37.550
Inderjit Dhillon: this has cluster, one at 1 point of time, and this is cluster 2,


530
01:00:39.600 --> 01:00:41.670
Inderjit Dhillon: right? So this is cluster, 2


531
01:00:42.980 --> 01:00:46.060
Inderjit Dhillon: at 1 point of time this is cluster one


532
01:00:47.920 --> 01:01:00.130
Inderjit Dhillon: We know that it is kind of wrong. Right? What will the the centroid of this cluster be? Well, the center of this cluster will be something like maybe i'll just change my color.


533
01:01:00.870 --> 01:01:07.510
Inderjit Dhillon: and the centroid will be somewhere here. and the centroid for this cluster will be somewhere here right?


534
01:01:09.600 --> 01:01:11.310
Inderjit Dhillon: So this would be


535
01:01:13.890 --> 01:01:19.730
Inderjit Dhillon: m 2 D. And this will be m one. D.


536
01:01:25.020 --> 01:01:32.900
Inderjit Dhillon: So let me now, with this. You know that you know this is a bad clustering. But remember, this is some point in the algorithm


537
01:01:33.290 --> 01:01:38.820
Inderjit Dhillon: So what kings does in the others in this kind of alternate steps?


538
01:01:39.400 --> 01:01:41.990
Inderjit Dhillon: What we'll do is for each X.


539
01:01:44.640 --> 01:01:51.740
Inderjit Dhillon: It'll find it's new cluster Centroid ejac, says, what cluster am I? Closes? What plus the center of my closest to.


540
01:01:52.620 --> 01:02:03.060
Inderjit Dhillon: and will be long start belonging to that cluster. So let me just write it. What he checks find its new cluster index.


541
01:02:03.700 --> 01:02:04.860
Inderjit Dhillon: as


542
01:02:06.120 --> 01:02:08.590
Inderjit Dhillon: I. Star X


543
01:02:08.720 --> 01:02:10.150
Inderjit Dhillon: is equal to


544
01:02:11.580 --> 01:02:19.030
Inderjit Dhillon: argument. So look at all the distances to the K cluster centroids


545
01:02:23.010 --> 01:02:34.110
Inderjit Dhillon: the case to us and try it. It looks at the distance from the first plus to centroid, second cluster centroid all the way to the K Cluster centroids and then chooses the minimum.


546
01:02:35.710 --> 01:02:42.110
Inderjit Dhillon: So for each point it does that. And then, after each point has done that. you


547
01:02:45.440 --> 01:02:47.310
Inderjit Dhillon: update the clusters.


548
01:02:52.720 --> 01:02:57.550
Inderjit Dhillon: and you say that, c. I. Now, at this new iteration.


549
01:02:57.980 --> 01:02:59.930
Inderjit Dhillon: the next set of clusters.


550
01:03:01.390 --> 01:03:04.440
Inderjit Dhillon: the it cluster is such that


551
01:03:04.680 --> 01:03:10.170
Inderjit Dhillon: all the points x, so that X. I. Star X equals sorry.


552
01:03:11.820 --> 01:03:15.280
Inderjit Dhillon: And then what it does is it basically repeats.


553
01:03:19.330 --> 01:03:22.950
Inderjit Dhillon: steps 2, 3,


554
01:03:25.090 --> 01:03:27.500
Inderjit Dhillon: and for until


555
01:03:30.140 --> 01:03:33.170
Inderjit Dhillon: convergence, and we need to talk about what?


556
01:03:38.500 --> 01:03:40.090
Inderjit Dhillon: So that is the algorithm.


557
01:03:50.230 --> 01:03:51.580
Inderjit Dhillon: Any questions.


558
01:04:05.860 --> 01:04:10.740
Inderjit Dhillon: Well, let's see what happens in this case. This example is a nice illustrative example. Right?


559
01:04:11.010 --> 01:04:17.830
Inderjit Dhillon: So let's suppose you know I have this as m one t this as M. 2 t


560
01:04:18.080 --> 01:04:25.580
Inderjit Dhillon: every point x. Ask the question. Am I close to this point or this point.


561
01:04:27.220 --> 01:04:38.440
Inderjit Dhillon: Currently all these points are labeled by cluster one. These points are labeled as just a 2. And now we are relabeling in this step.


562
01:04:40.510 --> 01:04:46.480
Inderjit Dhillon: So this point says, Well, i'm still closest to one. So this points labeled remains as one.


563
01:04:46.730 --> 01:04:54.810
Inderjit Dhillon: so does this, so does this similarly here. Similarly here, similarly here, similarly. Here. look at this.


564
01:04:55.450 --> 01:04:58.410
Inderjit Dhillon: Is it close to this or this?


565
01:04:58.890 --> 01:05:05.270
Inderjit Dhillon: It's close to 2. So this point remains in 2, this 2. This stays us to


566
01:05:05.500 --> 01:05:09.180
Inderjit Dhillon: so, for nothing has changed. What about this point?


567
01:05:11.040 --> 01:05:17.020
Inderjit Dhillon: This is asking the question: am I closest to one one of my closest to 2?


568
01:05:19.710 --> 01:05:31.040
Inderjit Dhillon: Okay. So clearly this point is closest to 2. So this point I still X. Now it changes from one to 2.


569
01:05:33.580 --> 01:05:43.140
Inderjit Dhillon: Okay. So if I kind of describe the algorithm in the next step. What will end up happening is, you know, if I kind of redraw the points.


570
01:05:49.850 --> 01:05:53.360
Inderjit Dhillon: then my cluster one will be this


571
01:05:54.040 --> 01:05:56.220
Inderjit Dhillon: and my cluster.


572
01:05:59.480 --> 01:06:07.080
Inderjit Dhillon: So this is a simple case. Of course it's a very simple example. but it shows how the K means out with homework, so it basically keeps on iterating.


573
01:06:07.490 --> 01:06:10.390
Inderjit Dhillon: It has Kay clusters.


574
01:06:11.040 --> 01:06:13.610
Inderjit Dhillon: it computes the mean center of each cluster.


575
01:06:14.220 --> 01:06:16.850
Inderjit Dhillon: Then each of the points


576
01:06:17.170 --> 01:06:31.400
Inderjit Dhillon: you go through all the points you find the cluster index You reassigned to the clusters, and you keep on repeated if you think about the amount of work. Oh, sorry if you think about the amount of work that is done.


577
01:06:32.080 --> 01:06:41.940
Inderjit Dhillon: This is the most expensive step of the algorithm for each of the point there are endpoints. It has to do this for all the K clusters.


578
01:06:42.920 --> 01:06:48.180
Inderjit Dhillon: So basically it ends up doing N. Times, K. Distance computations.


579
01:06:48.560 --> 01:06:58.350
Inderjit Dhillon: And if D is the dimensionality, the complexity of the algorithm is N. Times, K. Times, d. which actually can be much better than N. Square.


580
01:06:59.550 --> 01:07:03.060
Inderjit Dhillon: because there it's not just n square or N. Cube.


581
01:07:03.110 --> 01:07:10.710
Inderjit Dhillon: you know there's a dimensionality also over there. So N. Times, K. Is typically much much smaller than N. Square.


582
01:07:12.590 --> 01:07:18.500
Inderjit Dhillon: So what are the properties of this? Algorithm Well, I told you about the Kvm's objective function.


583
01:07:19.270 --> 01:07:24.370
Inderjit Dhillon: Is there any guarantee that this algorithm is actually decreasing this?


584
01:07:26.430 --> 01:07:29.440
Inderjit Dhillon: Okay, so the way I can.


585
01:07:32.300 --> 01:07:37.750
Inderjit Dhillon: right? That is that. Suppose I have the objective function at the


586
01:07:38.910 --> 01:07:44.820
Inderjit Dhillon: Okay. So the objective function at the T iteration is, I equals one through K.


587
01:07:45.950 --> 01:07:49.490
Inderjit Dhillon: Summation X belongs to C. I. D.


588
01:07:50.700 --> 01:07:52.750
Inderjit Dhillon: X minus M. I.


589
01:07:54.680 --> 01:07:55.490
Inderjit Dhillon: The


590
01:07:58.710 --> 01:07:59.530
Inderjit Dhillon: Okay.


591
01:07:59.720 --> 01:08:04.830
Inderjit Dhillon: and then i'll have a different clustering at t plus one given by the algorithm


592
01:08:05.920 --> 01:08:08.640
Inderjit Dhillon: Well, the question is, is


593
01:08:15.640 --> 01:08:18.899
Inderjit Dhillon: the objective function monotonically decreasing.


594
01:08:21.050 --> 01:08:24.600
Inderjit Dhillon: And it turns out that in K-means that does happen


595
01:08:24.859 --> 01:08:26.779
Inderjit Dhillon: so it monitoronically


596
01:08:26.870 --> 01:08:29.710
Inderjit Dhillon: decreases the objective function.


597
01:08:31.000 --> 01:08:36.569
Inderjit Dhillon: The proof is not that hard. So let me write it down. So J. T.


598
01:08:36.660 --> 01:08:42.080
Inderjit Dhillon: Is equal to summation. I I'm. Just repeating what I have from above.


599
01:08:46.779 --> 01:08:47.569
Inderjit Dhillon: Sorry


600
01:08:51.950 --> 01:08:54.130
Inderjit Dhillon: X minus M. I.


601
01:08:55.529 --> 01:08:56.859
Inderjit Dhillon: E. Square.


602
01:08:58.130 --> 01:09:01.029
Inderjit Dhillon: Okay, and Remember, this is still the 2 now, okay.


603
01:09:01.720 --> 01:09:06.060
Inderjit Dhillon: So now. just because of the algorithm step


604
01:09:07.490 --> 01:09:14.479
Inderjit Dhillon: over here, it finds I. Star X is the minimum among all the distances.


605
01:09:15.240 --> 01:09:18.939
Inderjit Dhillon: So I have the inequality that


606
01:09:19.960 --> 01:09:30.270
Inderjit Dhillon: this is summation over. I equals one through K. I still have it from the same way of doing the sum.


607
01:09:30.850 --> 01:09:33.540
Inderjit Dhillon: except now I have M.


608
01:09:35.600 --> 01:09:37.779
Inderjit Dhillon: I Star X over here.


609
01:09:41.670 --> 01:09:46.069
Inderjit Dhillon: Okay, this is by let's see what was the step of the algorithm


610
01:09:46.359 --> 01:09:48.779
Inderjit Dhillon: It was step 3, right.


611
01:09:50.370 --> 01:09:58.890
Inderjit Dhillon: so the reason is by step 3 of it means all good.


612
01:10:06.720 --> 01:10:14.350
Inderjit Dhillon: So now i'm going to rewrite this objective function. So so so take a node of how the objective function is written over here.


613
01:10:14.610 --> 01:10:28.270
Inderjit Dhillon: right? It's basically written in terms of the clusters at step. This is the it cluster at step. T: so i'm going to rewrite it in terms of the


614
01:10:28.290 --> 01:10:30.530
Inderjit Dhillon: clusters at step. T plus one


615
01:10:31.610 --> 01:10:33.730
Inderjit Dhillon: I equals one to K.


616
01:10:35.150 --> 01:10:38.180
Inderjit Dhillon: I'm. Just rearranging my summation.


617
01:10:39.880 --> 01:10:41.860
Inderjit Dhillon: not making any other change.


618
01:10:46.160 --> 01:10:48.060
Inderjit Dhillon: Notice that over here


619
01:10:49.730 --> 01:10:51.220
Inderjit Dhillon: I still have tea.


620
01:10:51.660 --> 01:10:58.460
Inderjit Dhillon: I've just rewritten it in a different order in terms of the cluster t one. Okay.


621
01:10:58.760 --> 01:10:59.790
Inderjit Dhillon: But


622
01:11:02.660 --> 01:11:12.560
Inderjit Dhillon: now that I have these points. suppose I ask you a question. Okay, let me actually write it down over here. Suppose I have points.


623
01:11:13.130 --> 01:11:17.180
Inderjit Dhillon: Y one y 2 through Y. N.


624
01:11:17.630 --> 01:11:21.140
Inderjit Dhillon: And I ask you the question, what is the minimizer


625
01:11:22.290 --> 01:11:30.500
Inderjit Dhillon: among all vector z summation I equals one through N. Why, I'm. Honestly


626
01:11:32.230 --> 01:11:38.330
Inderjit Dhillon: okay. If you look at this, this is just the very definition of an average or a android.


627
01:11:39.040 --> 01:11:47.770
Inderjit Dhillon: So that means that the D star that minimizes this is just one divided by n summation of


628
01:11:49.680 --> 01:11:51.440
Inderjit Dhillon: summation of why I,


629
01:11:53.940 --> 01:11:58.040
Inderjit Dhillon: as a result. what you can show is that this


630
01:11:58.460 --> 01:12:00.930
Inderjit Dhillon: is greater than equal to.


631
01:12:03.210 --> 01:12:03.910
Inderjit Dhillon: and


632
01:12:06.660 --> 01:12:14.840
Inderjit Dhillon: this is greater than equal to summation. I equals one through n permission, same order


633
01:12:17.210 --> 01:12:19.250
Inderjit Dhillon: t plus one. Now


634
01:12:20.840 --> 01:12:26.980
Inderjit Dhillon: I just replace it by the centroid and deepest one.


635
01:12:27.280 --> 01:12:30.310
Inderjit Dhillon: And this is just the objective function


636
01:12:30.960 --> 01:12:32.230
Inderjit Dhillon: and take this one.


637
01:12:39.230 --> 01:12:44.430
Inderjit Dhillon: So we have shown that the K-means algorithm has this nice property


638
01:12:45.690 --> 01:12:50.130
Inderjit Dhillon: that it actually monotonically decreases the objective function.


639
01:12:50.760 --> 01:12:55.980
Inderjit Dhillon: by the way, that doesn't mean that it will actually get to the global optimal it won't.


640
01:12:57.410 --> 01:13:09.560
Inderjit Dhillon: Most likely it won't. But it has the desirable property that it will actually go towards the it will actually monotonically decrease. Object to function.


641
01:13:10.720 --> 01:13:14.620
Inderjit Dhillon: What I would urge you to do is just kind of, you know.


642
01:13:15.750 --> 01:13:21.090
Inderjit Dhillon: Be a little bit careful. This this proof is kind of, you know, seductively simple.


643
01:13:21.240 --> 01:13:24.260
Inderjit Dhillon: but just trying to make sure that you understand each step.


644
01:13:25.700 --> 01:13:31.080
Inderjit Dhillon: Okay, here. I've written it in terms of the clusters at time. At Step T.


645
01:13:31.440 --> 01:13:35.580
Inderjit Dhillon: I just rewrite it in terms of clusters at t plus one.


646
01:13:36.020 --> 01:13:40.140
Inderjit Dhillon: And then I realized that the mean is the best estimator


647
01:13:41.470 --> 01:13:46.300
Inderjit Dhillon: for the clusters t plus one. And so that's why the 2 inequalities come up.


648
01:13:47.990 --> 01:13:48.780
Inderjit Dhillon: Okay.


649
01:13:49.990 --> 01:13:55.630
Inderjit Dhillon: I know that we are kind of almost at time. But let me leave you with kind of one


650
01:13:55.940 --> 01:13:59.440
Inderjit Dhillon: fact, which is that K-means


651
01:14:01.560 --> 01:14:03.260
Inderjit Dhillon: separates


652
01:14:05.610 --> 01:14:06.900
Inderjit Dhillon: data


653
01:14:09.070 --> 01:14:10.930
Inderjit Dhillon: by hyperplanes.


654
01:14:17.120 --> 01:14:19.220
Inderjit Dhillon: So suppose


655
01:14:21.100 --> 01:14:22.950
Inderjit Dhillon: K. Equals 2.


656
01:14:23.790 --> 01:14:27.310
Inderjit Dhillon: Okay, then, if I look at the locus of


657
01:14:28.350 --> 01:14:30.340
Inderjit Dhillon: all points


658
01:14:31.870 --> 01:14:33.640
Inderjit Dhillon: equally distant.


659
01:14:35.990 --> 01:14:39.610
Inderjit Dhillon: 2 cluster, C, one and C 2,


660
01:14:39.920 --> 01:14:46.340
Inderjit Dhillon: it will be X minus M. One square is equal to X minus m 2.


661
01:14:46.840 --> 01:14:47.700
Inderjit Dhillon: Where?


662
01:14:48.340 --> 01:14:51.100
Inderjit Dhillon: And that is the question of a hyperplane.


663
01:14:56.050 --> 01:15:03.030
Inderjit Dhillon: We've kind of discussed this in the context of classification. But one thing that you can take away is


664
01:15:03.240 --> 01:15:09.640
Inderjit Dhillon: that came. Means will not have the ability. So in your homework you have this problem with 2 concentric circles.


665
01:15:09.830 --> 01:15:15.490
Inderjit Dhillon: If you were to say, hey, can my algorithm discover these 2 concentric circles as clusters.


666
01:15:15.810 --> 01:15:16.920
Inderjit Dhillon: then


667
01:15:16.980 --> 01:15:20.660
Inderjit Dhillon: okay means will not be able to do that


668
01:15:22.130 --> 01:15:30.890
Inderjit Dhillon: in the Svm example or the In the Cm. Example, we saw that we could actually do something called Kernel Svm.


669
01:15:31.400 --> 01:15:39.600
Inderjit Dhillon: Similarly, over here, if you wanted to have a nonlinear separator. You can do kernels. Kernel came in.


670
01:15:41.290 --> 01:15:45.140
Inderjit Dhillon: Okay. But this one fact, remember, is that key means


671
01:15:45.400 --> 01:15:48.860
Inderjit Dhillon: will separate the data by hyperplanes.


672
01:15:49.930 --> 01:16:03.120
Inderjit Dhillon: So K-means is, you know, an extremely kind of effective algorithm it's actually, if you're trying to do clustering it is one of the first things that you should try


673
01:16:03.220 --> 01:16:12.780
Inderjit Dhillon: before trying any any other. Algorithm it gets a lot of grief. A lot of people will say that you know their algorithm is better than k-means.


674
01:16:12.800 --> 01:16:21.670
Inderjit Dhillon: but it is definitely the first method that you should try. If you're looking at doing unsupervised learning, and then cluster


675
01:16:22.530 --> 01:16:25.640
Inderjit Dhillon: any questions.


676
01:16:25.840 --> 01:16:32.930
Inderjit Dhillon: So in the Cardinals K. Means i'll call it some. Yeah.


677
01:16:33.290 --> 01:16:36.530
Chitrank: Can we like. do like.


678
01:16:36.650 --> 01:16:41.530
Inderjit Dhillon: Can it be reformulated purely in terms of Graham matrix?


679
01:16:42.860 --> 01:16:45.900
Inderjit Dhillon: Exactly. And so I haven't shown that.


680
01:16:46.420 --> 01:16:49.040
Inderjit Dhillon: But that's something that actually you can think about it


681
01:16:49.060 --> 01:16:53.380
Inderjit Dhillon: right? Because think about how the steps are


682
01:16:55.230 --> 01:16:57.690
Inderjit Dhillon: right. So, for example, over here.


683
01:17:02.390 --> 01:17:05.290
Inderjit Dhillon: here, right X minus M. I.


684
01:17:07.010 --> 01:17:15.700
Inderjit Dhillon: It seems like well, but think about it right. This is Mi is the summation of all the points. and if it is the


685
01:17:16.490 --> 01:17:20.500
Inderjit Dhillon: the square Euclidean distance, you can expand it out.


686
01:17:21.560 --> 01:17:27.570
Inderjit Dhillon: and essentially you will get something in terms of the inner products. Can you? Can you see that happening, or


687
01:17:28.100 --> 01:17:37.380
Chitrank: some what? But yeah, so you'll have to carry out the details, and you'll never be able to compute a cluster. Send for it explicitly.


688
01:17:39.700 --> 01:17:43.080
Inderjit Dhillon: but you know it can be done.


689
01:17:44.830 --> 01:17:52.370
Inderjit Dhillon: So you can have a kernel, came in the algorithm and it's basically just as one step. because there is no explicit computation of this.


690
01:17:52.980 --> 01:17:53.790
Chitrank: Hmm.


691
01:17:56.460 --> 01:17:58.450
Inderjit Dhillon: Any other questions.


692
01:18:03.850 --> 01:18:04.790
Yuhan Gao: Okay.


693
01:18:05.070 --> 01:18:06.190
Inderjit Dhillon: Yeah, go ahead.


694
01:18:06.400 --> 01:18:13.360
Yuhan Gao: I have a question. Can you go back to the notes where you trying to minimize?


695
01:18:13.470 --> 01:18:19.780
Yuhan Gao: I mean the I'll optimize the J. I. Plus J. L.


696
01:18:20.150 --> 01:18:22.650
Yuhan Gao: Screw up a little bit


697
01:18:23.320 --> 01:18:25.440
Yuhan Gao: so for


698
01:18:25.610 --> 01:18:30.430
Yuhan Gao: the step. That's 3 I plus


699
01:18:30.460 --> 01:18:32.920
Yuhan Gao: and that times, and now square


700
01:18:33.660 --> 01:18:37.170
Yuhan Gao: I I I don't know why


701
01:18:37.210 --> 01:18:42.000
Yuhan Gao: it's an I times and elsewhere in the in the denominator.


702
01:18:44.060 --> 01:18:49.930
Inderjit Dhillon: Oh, so this is because because there is a square here.


703
01:18:52.630 --> 01:18:55.620
Inderjit Dhillon: So it's basically you can think of this as


704
01:18:56.280 --> 01:19:00.980
Inderjit Dhillon: it's basically like a minus B square.


705
01:19:01.090 --> 01:19:01.800
Yuhan Gao: Hmm.


706
01:19:02.310 --> 01:19:05.990
Inderjit Dhillon: Right? This is a: so it's a minus B


707
01:19:07.210 --> 01:19:09.750
Yuhan Gao: Transpose a minus B.


708
01:19:10.620 --> 01:19:14.380
Inderjit Dhillon: That's a square plus B square.


709
01:19:15.540 --> 01:19:17.950
Inderjit Dhillon: minus 2, a transpose. B:


710
01:19:22.210 --> 01:19:24.570
Yuhan Gao: Basically, Taking this.


711
01:19:24.880 --> 01:19:26.850
Inderjit Dhillon: it becomes the square. Here


712
01:19:26.860 --> 01:19:30.260
Yuhan Gao: I take the square of this.


713
01:19:30.380 --> 01:19:36.450
Inderjit Dhillon: It becomes Nl square divided by an I plus nl square Ml: minus M. I square


714
01:19:41.440 --> 01:19:43.410
Inderjit Dhillon: this transpose.


715
01:19:43.800 --> 01:19:47.160
Inderjit Dhillon: It's basically a blindness.


716
01:19:48.260 --> 01:19:58.090
Yuhan Gao: The the next step, the now and now square, becomes to, an I am so now square.


717
01:19:58.880 --> 01:20:02.400
Yuhan Gao: There's a summation of X. Belongs to Ci. Right?


718
01:20:03.320 --> 01:20:06.510
Inderjit Dhillon: This is constant, for all X belong to Cr.


719
01:20:07.760 --> 01:20:09.890
Inderjit Dhillon: Sorry over Here we are here


720
01:20:10.680 --> 01:20:16.120
Inderjit Dhillon: right? So this term appears every time, for each X belongs to C. I


721
01:20:16.720 --> 01:20:18.890
Inderjit Dhillon: and there are.


722
01:20:20.700 --> 01:20:24.050
Inderjit Dhillon: so it'll become an I times this.


723
01:20:24.160 --> 01:20:25.890
Yuhan Gao: Oh, okay.


724
01:20:26.490 --> 01:20:27.860
Yuhan Gao: okay, that makes sense.


725
01:20:28.300 --> 01:20:29.790
Inderjit Dhillon: Okay.


726
01:20:30.100 --> 01:20:30.990
Yuhan Gao: Thank you.


727
01:20:31.630 --> 01:20:33.360
Inderjit Dhillon: Sure. Thanks for the question.


728
01:20:34.810 --> 01:20:36.860
Inderjit Dhillon: Okay. Any other questions.


729
01:20:38.480 --> 01:20:43.530
Inderjit Dhillon: I actually love it. When you guys ask questions, because it makes me realize that people are listening


730
01:20:43.700 --> 01:20:47.860
Inderjit Dhillon: otherwise, you know, since there's not like a face to face class, it becomes sort.


731
01:20:48.310 --> 01:20:54.120
Inderjit Dhillon: So thank you for asking those questions. I think we are a little bit over time. So


732
01:20:54.330 --> 01:21:03.510
Inderjit Dhillon: remember that Homework 3 has been assigned. Remember that on Wednesday, by Wednesday we would like all of you to


733
01:21:03.650 --> 01:21:11.400
Inderjit Dhillon: submit your project. What product you are taking. and


734
01:21:11.420 --> 01:21:14.630
Inderjit Dhillon: I will see you on Wednesday, and on Wednesday


735
01:21:14.740 --> 01:21:24.830
Inderjit Dhillon: we'll talk about graph clustering. So this was about data clustering when data are given as points within our in Rd.


736
01:21:25.290 --> 01:21:35.260
Inderjit Dhillon: Many times the data can be presented as graphs such as you know, friendship networks and social networks, let's say. And so we'll talk a little bit about.


737
01:21:36.400 --> 01:21:43.670
Inderjit Dhillon: Yep, thank you. I have a another question. The in that project submission thing.


738
01:21:45.810 --> 01:21:49.820
Chitrank: So is it? Okay, if you know, like towards the


739
01:21:49.980 --> 01:22:01.260
Chitrank: and of the exactly, I mean, we like slightly. or like kind of shift away from what we submitted in the


740
01:22:01.350 --> 01:22:03.800
Inderjit Dhillon: think of this as a project proposal.


741
01:22:04.390 --> 01:22:06.750
Inderjit Dhillon: It's not a bending contract.


742
01:22:06.910 --> 01:22:10.030
So so you're just. I just want to know.


743
01:22:12.430 --> 01:22:14.070
Inderjit Dhillon: Initial thoughts are


744
01:22:14.210 --> 01:22:25.570
Inderjit Dhillon: this is definitely not a graded part. Right? I'm not. We're not going to grad you on this part at all right. I mean that as you keep on working your You know what you are going to do is going to evolve.


745
01:22:26.120 --> 01:22:33.230
Inderjit Dhillon: and so it's totally fine for there to be. you know. Obviously I don't want, you know. Suppose you are doing a project on


746
01:22:36.280 --> 01:22:41.090
Inderjit Dhillon: just string? I don't want you to switch to something, you know totally different


747
01:22:41.300 --> 01:22:50.830
Inderjit Dhillon: right. But it should it should be related, obviously. But you know your approach can change what you datasets can change. So so so that is obviously fine.


748
01:22:51.230 --> 01:22:52.050
Chitrank: Okay.


749
01:22:52.070 --> 01:22:55.370
Inderjit Dhillon: it's Basically, to give you some hands on experience.


750
01:22:56.000 --> 01:23:00.560
Inderjit Dhillon: right in machine learning, you know, you know, in a in a project setting.


751
01:23:00.910 --> 01:23:01.520
Chitrank: Hmm.


752
01:23:03.550 --> 01:23:04.190
Thank you.


753
01:23:06.670 --> 01:23:11.770
Inderjit Dhillon: Okay, Thank you. I will see you all on Wednesday. Thank you


WEBVTT

1
00:01:09.520 --> 00:01:11.440
Inderjit Dhillon: Hey? Good afternoon, everybody.

2
00:01:49.150 --> 00:01:51.530
Inderjit Dhillon: Okay? Any questions before we start.

3
00:01:54.830 --> 00:01:56.190
Yung-Chi Kung: Yeah.

4
00:01:56.680 --> 00:01:59.720
Yung-Chi Kung: or the assignment here today, like the abstract.

5
00:02:00.210 --> 00:02:05.200
Yung-Chi Kung: Did you want one submission per group or everybody submits.

6
00:02:05.570 --> 00:02:08.930
Inderjit Dhillon: Oh, no, no, no. It will be one submission per group.

7
00:02:08.990 --> 00:02:09.729
Yung-Chi Kung: Okay.

8
00:02:10.360 --> 00:02:16.850
Inderjit Dhillon: Yeah, there's no need to replicate that. You know. Everybody is in the group. Everybody is.

9
00:02:18.480 --> 00:02:21.400
Inderjit Dhillon: Everybody would have contributed towards that. Right?

10
00:02:21.490 --> 00:02:23.750
Yung-Chi Kung: Okay, just wanted to chat.

11
00:02:24.650 --> 00:02:26.280
Inderjit Dhillon: Yeah, just one. Yeah.

12
00:02:27.660 --> 00:02:29.090
Any other questions.

13
00:02:30.310 --> 00:02:37.280
Inderjit Dhillon: So remember, you know, we have your choice of class projects due today. You should write down

14
00:02:37.470 --> 00:02:46.320
Inderjit Dhillon: obviously the name of the project. Brief description couple of paragraphs, the name of the people partition participating in the project.

15
00:02:46.350 --> 00:02:48.830
Inderjit Dhillon: and then some ideas about how you would

16
00:02:49.060 --> 00:02:51.160
Inderjit Dhillon: go about doing that work.

17
00:02:52.350 --> 00:02:56.150
Inderjit Dhillon: And so it will be about one to 2 pages, and that is due today.

18
00:03:00.700 --> 00:03:02.280
Inderjit Dhillon: Any other questions.

19
00:03:06.280 --> 00:03:23.160
Inderjit Dhillon: Okay, If not, let's start today's lecture. So if you recall what we've been doing after spring break, it's been looking at unsupervised learning in particular, clustering and last lecture

20
00:03:23.230 --> 00:03:28.120
Inderjit Dhillon: I went through. Maybe I would actually put it pull it up

21
00:03:30.660 --> 00:03:36.840
Inderjit Dhillon: last like Sure, we talked about. You know, different kinds of ways to do clustering.

22
00:03:36.940 --> 00:03:48.550
Inderjit Dhillon: One was hierarchical, agglomerative testing, which is a bottom up plus playing. Looked at how one can actually have an objective function and drive the

23
00:03:48.870 --> 00:03:56.500
Inderjit Dhillon: data to the string, using that. So we get a kind of a criterion by that.

24
00:03:56.650 --> 00:04:03.480
Inderjit Dhillon: But what if you wanted to just directly try and optimize the criterion that leads to the

25
00:04:04.710 --> 00:04:19.070
Inderjit Dhillon: They means objective function, and in some sense the famous K-means, algorithm. You know, it actually is also called the Lloyd Max, algorithm. more than 50 years old, but actually very, very useful, and works quite well.

26
00:04:19.360 --> 00:04:27.550
Inderjit Dhillon: And then we discuss the algorithm, which has these alternating steps of computing the centroid of each cluster, and then each point

27
00:04:27.750 --> 00:04:37.760
Inderjit Dhillon: gets reassigned to the clusters depending upon which centroid it is closest, to and we looked at how the

28
00:04:38.430 --> 00:04:48.720
Inderjit Dhillon: give me the objective function gets incrementally improved. So the the gaming, the objective function monotonically decreases.

29
00:04:48.760 --> 00:05:03.900
Inderjit Dhillon: As the algorithm progresses, it still has kind of It's a a fair share of drawbacks initialization as an issue. Sometimes you can get empty clusters if you ask for many clusters.

30
00:05:04.070 --> 00:05:10.180
Inderjit Dhillon: and just to kind of recall. This problem is hard because it's an np-complete problem.

31
00:05:10.200 --> 00:05:17.940
So you know there's no guarantee that you will actually get the global minimizer of this objective function.

32
00:05:18.190 --> 00:05:29.250
Inderjit Dhillon: but they mean the still kind of a very effective method in practice. And sometimes, you know, the implementation can actually make a big big difference.

33
00:05:29.370 --> 00:05:43.800
Inderjit Dhillon: And then at the end of last lecture we also saw that K means is another of these algorithms that will partition your data by linear separators, right? And so.

34
00:05:43.800 --> 00:05:54.600
Inderjit Dhillon: if you have data which is nonlinearly separable, there was a question at the end. You can actually develop like a colonelized version of this algorithm and that's called

35
00:05:54.650 --> 00:05:56.440
Inderjit Dhillon: Don't know.

36
00:05:57.540 --> 00:06:04.360
Inderjit Dhillon: So i'm not going to spend too much time talking about unsupervised learning in this class.

37
00:06:04.710 --> 00:06:12.320
Inderjit Dhillon: Maybe just today's lectures. But today. What we will talk about is graft or strength.

38
00:06:23.600 --> 00:06:24.310
Inderjit Dhillon: Oh.

39
00:06:24.640 --> 00:06:25.990
it sounds like so

40
00:06:29.250 --> 00:06:31.000
Inderjit Dhillon: okay. So we'll talk about.

41
00:06:31.720 --> 00:06:39.580
Inderjit Dhillon: Suppose your data is given in form of a graph, or maybe you can actually construct the graph from the data.

42
00:06:39.770 --> 00:06:42.400
Inderjit Dhillon: So suppose you have. Let's say a graph.

43
00:06:44.860 --> 00:06:47.820
Inderjit Dhillon: Let me just kind of draw a graph.

44
00:06:52.700 --> 00:06:57.790
Inderjit Dhillon: And suppose there are edges. It's a kind of you see, kind of 2 partitions.

45
00:07:05.400 --> 00:07:16.400
Inderjit Dhillon: right? And so a graph could actually have a weights attached to it. So let's suppose the weights are No, some values. I'll just put values over here.

46
00:07:16.780 --> 00:07:20.810
Inderjit Dhillon: So these are the weights on the edges.

47
00:07:23.890 --> 00:07:30.460
Inderjit Dhillon: Okay. So i'll define a graph by a triple. the set of vertices.

48
00:07:31.260 --> 00:07:40.490
Inderjit Dhillon: the set of edges, and the set of weights associated with each of the edges. Okay. So the vertices are

49
00:07:40.800 --> 00:07:43.980
Inderjit Dhillon: typically denoted by V. One

50
00:07:44.670 --> 00:07:54.920
Inderjit Dhillon: through the N. So, assuming that the graph has n vertices. If it's a complete graph, it'll have and choose 2 edges right. We are all.

51
00:07:55.140 --> 00:08:08.260
Inderjit Dhillon: We're pretty much going to assume that the graph is undirected. But of course there may not be and choose to edges that are in the given graph. So suppose you consider, let's say, a friendship graph in a social network.

52
00:08:08.350 --> 00:08:10.930
Inderjit Dhillon: Clearly, everybody is not everybody's friend.

53
00:08:11.250 --> 00:08:18.300
Inderjit Dhillon: And this graph actually might be quite sports, so the edges will be denoted by

54
00:08:18.870 --> 00:08:25.730
Inderjit Dhillon: E. One, E, 2. And suppose the number of edges is absolute value of E,

55
00:08:25.760 --> 00:08:26.930
Inderjit Dhillon: the set. P.

56
00:08:27.760 --> 00:08:39.980
Inderjit Dhillon: Okay, and then a Ij is what's called the adjacency matrix. But the way it adjacency matrix. So i'll say that a is a matrix such that so a is.

57
00:08:41.080 --> 00:08:42.669
Inderjit Dhillon: and by end.

58
00:08:43.909 --> 00:08:52.850
Inderjit Dhillon: and the Igf element is wid. If I. J. Belongs to the LED set

59
00:08:53.190 --> 00:08:55.810
Inderjit Dhillon: and 0 otherwise.

60
00:08:57.390 --> 00:08:59.440
Inderjit Dhillon: and typically many of these

61
00:08:59.730 --> 00:09:08.850
Inderjit Dhillon: graphs end up being sparse. And so there are lots and lots of zeros in this graph, so wid

62
00:09:08.980 --> 00:09:12.280
Inderjit Dhillon: essentially corresponds to weight of

63
00:09:14.040 --> 00:09:16.910
Inderjit Dhillon: edge A. It

64
00:09:18.580 --> 00:09:23.500
Inderjit Dhillon: apologies. I'm. Slightly under the weather today, so

65
00:09:23.710 --> 00:09:31.230
Inderjit Dhillon: hopefully, i'll make it to the end of class. I think I will. But I make coffee in between.

66
00:09:31.250 --> 00:09:43.590
Inderjit Dhillon: So so this is the setting. So, for example, in the graph above, you can see that this edge weight is 1.5. This is 2. This is one. This is one. And so

67
00:09:43.900 --> 00:09:55.160
Inderjit Dhillon: okay. So that's what I mean by this Wij: it corresponds to these numbers, and the location of these wid is corresponds to the corresponding edge.

68
00:09:55.740 --> 00:10:01.450
Inderjit Dhillon: So given this kind of graph, let's think about

69
00:10:01.510 --> 00:10:03.200
Inderjit Dhillon: partitioning goal

70
00:10:03.220 --> 00:10:05.150
Inderjit Dhillon: like a clustering goal.

71
00:10:05.480 --> 00:10:08.750
Inderjit Dhillon: So one goal would be Partition

72
00:10:10.640 --> 00:10:15.670
Inderjit Dhillon: V, which is the set of edges. Sorry the set of vertices into

73
00:10:15.980 --> 00:10:17.260
Inderjit Dhillon: into.

74
00:10:20.160 --> 00:10:22.420
Inderjit Dhillon: Excuse me, Kay. Clusters

75
00:10:23.430 --> 00:10:24.790
Inderjit Dhillon: of notes.

76
00:10:26.610 --> 00:10:31.530
Inderjit Dhillon: Okay. So, for example, if I say that K is equal to 2,

77
00:10:34.620 --> 00:10:41.820
Inderjit Dhillon: which means I'm. Separating it into 2 clusters. Then I would. Partition B is equal to V. One

78
00:10:41.950 --> 00:10:47.600
Inderjit Dhillon: Union V. 2, so each of the partitions is we. The first partition is V. One.

79
00:10:47.760 --> 00:11:00.970
Inderjit Dhillon: the second is V 2, and these are not overlapping, so there is no vertex in common or node in common between the 2. So the intersection is 0 or the mill set.

80
00:11:01.710 --> 00:11:06.800
Inderjit Dhillon: Now this arises in many contexts and applications.

81
00:11:10.310 --> 00:11:15.820
Inderjit Dhillon: many contexts. applications.

82
00:11:17.290 --> 00:11:26.200
Inderjit Dhillon: actually, and it precedes machine learning actually right. So machine learning applications are kind of social

83
00:11:26.440 --> 00:11:29.780
Inderjit Dhillon: network analysis, like I said, a friendship graph.

84
00:11:29.940 --> 00:11:33.260
Inderjit Dhillon: It could be a trust graph where

85
00:11:33.390 --> 00:11:43.710
Inderjit Dhillon: the edge weight could be plus one for trust and minus one for not trust. This obviously is in web data in terms of hyperlinks.

86
00:11:44.050 --> 00:11:48.450
Inderjit Dhillon: It comes up in segmentation

87
00:11:48.860 --> 00:12:02.440
Inderjit Dhillon: where you can form a graph between the pixels of an image. but it actually is predated by applications, and you know, circuit partitioning.

88
00:12:02.840 --> 00:12:06.610
Inderjit Dhillon: So, for example, you know, when you are trying to lay out

89
00:12:07.410 --> 00:12:14.770
Inderjit Dhillon: lots and lots of circuit elements as is done in reals. I like, you know, Chips.

90
00:12:14.810 --> 00:12:26.940
Inderjit Dhillon: Then you need to have automated ways of partitioning the circuit elements, so that there are no so that the wireless between the circuit elements are not very wrong.

91
00:12:27.280 --> 00:12:33.650
Inderjit Dhillon: So that's why I say it kind of predates applications in machine learning. Now.

92
00:12:33.750 --> 00:12:38.210
Inderjit Dhillon: I've only said Partition V into K. Clusters. But

93
00:12:38.420 --> 00:12:44.480
Inderjit Dhillon: what do you want? You basically again Want this close string kind of behavior, which is that

94
00:12:45.120 --> 00:12:57.440
Inderjit Dhillon: within each cluster there are lots of it is so. The the notes, in some sense, are similar to each other, or are connected more to each other. Suppose you have a friendship graph.

95
00:12:57.450 --> 00:13:08.930
Inderjit Dhillon: then you have a cluster where many of the people within this cluster are friends. And then between the partitions of between the clusters there are not that many address.

96
00:13:09.320 --> 00:13:15.920
Inderjit Dhillon: So you could define a criterion call cut, which is actually a fundamental concept in computer science.

97
00:13:16.320 --> 00:13:26.450
Inderjit Dhillon: So if I have cut, we want B 2. This is just defined as the sum of the weighted edges where

98
00:13:28.390 --> 00:13:40.340
Inderjit Dhillon: V. I. Belongs to V. One and V. J. Belongs to V. 2. So the crossing it. So, for example. if over here I have.

99
00:13:40.940 --> 00:13:44.940
Inderjit Dhillon: this is one partition. V. One.

100
00:13:46.320 --> 00:13:52.670
Inderjit Dhillon: Excuse me. this is another partition. V: 2.

101
00:13:53.750 --> 00:13:56.620
Inderjit Dhillon: Okay. So in this case cut off.

102
00:13:56.960 --> 00:14:01.270
Inderjit Dhillon: V. One V. 2 is equal to this

103
00:14:02.240 --> 00:14:04.570
Inderjit Dhillon: one plus

104
00:14:05.000 --> 00:14:06.180
Inderjit Dhillon: this one

105
00:14:07.640 --> 00:14:15.790
Inderjit Dhillon: is equal to, and that's what this is saying. It is summation of the weighted rough edges

106
00:14:16.170 --> 00:14:21.460
Inderjit Dhillon: edge weights where you are moving from one partition to the other partition.

107
00:14:29.750 --> 00:14:41.680
Inderjit Dhillon: Okay. So you can think that that is a criterion that you could do. You can basically try to get partitions. V: one and V 2 such that the cut between V. One V 2 is small.

108
00:14:42.500 --> 00:14:51.750
Inderjit Dhillon: Okay, now, typically, what could happen in if I just use cut is that you can get in situations like this, where you know you have.

109
00:14:53.880 --> 00:14:55.330
Inderjit Dhillon: You know, lots of

110
00:14:57.150 --> 00:15:00.940
Inderjit Dhillon: edges over here. Sorry. This is a repeated edge

111
00:15:04.920 --> 00:15:07.290
Inderjit Dhillon: you can have. It is.

112
00:15:07.670 --> 00:15:11.110
Inderjit Dhillon: And then you just have one edge over here

113
00:15:12.060 --> 00:15:19.150
Inderjit Dhillon: right? So you don't want partitions of this form where this is one partition. and this is another partition

114
00:15:20.130 --> 00:15:22.290
Inderjit Dhillon: that's not good. So

115
00:15:22.490 --> 00:15:25.390
Inderjit Dhillon: just minimizing

116
00:15:29.960 --> 00:15:35.280
Inderjit Dhillon: cut we one, we 2 actually does not work.

117
00:15:39.840 --> 00:15:42.180
It does not work well.

118
00:15:43.130 --> 00:15:46.830
Inderjit Dhillon: and the reason is because in many applications.

119
00:15:47.480 --> 00:15:49.820
Inderjit Dhillon: because it leads to

120
00:15:50.510 --> 00:16:00.740
Inderjit Dhillon: extremely unbalanced clusters. And when I say extremely unbalanced, I mean that one partition could be very big, and the other partition could just be a few notes.

121
00:16:08.090 --> 00:16:20.690
Inderjit Dhillon: Okay. So an extreme way of trying to ensure balance would be to say that, hey? I actually want partitions which are equal in size. That means the number of North and V. One

122
00:16:21.000 --> 00:16:25.540
Inderjit Dhillon: is the same as the number of nodes in V 2. Okay. So

123
00:16:25.750 --> 00:16:27.160
Inderjit Dhillon: one

124
00:16:27.830 --> 00:16:29.280
Inderjit Dhillon: classical way

125
00:16:32.390 --> 00:16:34.900
Inderjit Dhillon: off and forcing

126
00:16:36.390 --> 00:16:37.670
balance

127
00:16:39.780 --> 00:16:46.110
Inderjit Dhillon: is to say, okay, i'm going to minimize the cut, minimize over all partitions. V. One. V. 2.

128
00:16:47.270 --> 00:16:49.450
Inderjit Dhillon: That cut is minimized.

129
00:16:50.690 --> 00:16:51.920
Inderjit Dhillon: That's that.

130
00:16:54.050 --> 00:17:02.350
Inderjit Dhillon: Suppose we have an even number of vertices. that the number of vertices in v. One is equal to the number of vertices

131
00:17:03.410 --> 00:17:05.220
Inderjit Dhillon: in v. 2.

132
00:17:06.960 --> 00:17:14.740
Inderjit Dhillon: This turns out to be, you know, like most really clustering formulation, this problem is also hard to solve. It's an Np. Hardp.

133
00:17:15.130 --> 00:17:15.869
Inderjit Dhillon: Okay.

134
00:17:16.619 --> 00:17:26.050
Inderjit Dhillon: Now, this kind of clustering actually could make sense in many situations. For example, in the graph layout problem that I talked.

135
00:17:26.069 --> 00:17:29.190
Inderjit Dhillon: this was actually desirable because they wanted to partition the chip

136
00:17:29.280 --> 00:17:35.820
Inderjit Dhillon: into subset the number of circuit elements on each partition were exactly equal.

137
00:17:35.930 --> 00:17:45.740
Inderjit Dhillon: Okay, as a result, there are, you know, this heuristic methods that were developed. Okay. So, for example, there is a classical method.

138
00:17:45.920 --> 00:17:48.890
Inderjit Dhillon: all the kony handling heuristic

139
00:17:49.540 --> 00:17:53.280
Inderjit Dhillon: in case you might be wondering. This is the same

140
00:17:53.460 --> 00:18:00.300
Inderjit Dhillon: of the who came up with the C language. I think it was right. Okay.

141
00:18:00.400 --> 00:18:02.200
Inderjit Dhillon: no.

142
00:18:02.930 --> 00:18:13.560
Inderjit Dhillon: And so it's the currently hand and then heuristic. So what does it do? Right? So suppose you're trying to develop an algorithm for this right? I have V. One. I have a. V.

143
00:18:13.580 --> 00:18:18.720
Inderjit Dhillon: I want to divide it into V. One and V. 2 such that the partitions are equal in size.

144
00:18:22.030 --> 00:18:25.540
Inderjit Dhillon: so what you could do is you could say, hey.

145
00:18:27.160 --> 00:18:31.030
Inderjit Dhillon: let me just start off with some partitioning into equal halves.

146
00:18:31.980 --> 00:18:37.010
Inderjit Dhillon: so I have a V one. However, we do, and I incrementally need to improve the cut.

147
00:18:38.660 --> 00:18:45.640
Inderjit Dhillon: and what I can then do is I can try to see that if I take a pair of vertices, one in V, 1, one and v 2,

148
00:18:45.810 --> 00:18:54.220
Inderjit Dhillon: and I SW them. I move them from one partition to another. What happens to the cut.

149
00:18:56.080 --> 00:19:02.360
Inderjit Dhillon: If the cut decreases. That means it's a move that will decrease the cut.

150
00:19:02.740 --> 00:19:05.680
Inderjit Dhillon: If it increases, then you do not want to do this move.

151
00:19:06.400 --> 00:19:16.020
Inderjit Dhillon: So that's what the currently handled in heuristic does, except it's a little bit more sophisticated. So let me start off with a kind of simple version of the heuristic.

152
00:19:16.220 --> 00:19:18.090
Inderjit Dhillon: So it starts

153
00:19:19.660 --> 00:19:23.110
Inderjit Dhillon: with some partitioning

154
00:19:27.900 --> 00:19:29.170
Inderjit Dhillon: V one.

155
00:19:31.190 --> 00:19:33.900
Inderjit Dhillon: and we, too.

156
00:19:35.080 --> 00:19:38.300
Inderjit Dhillon: and it basically calculates

157
00:19:40.540 --> 00:19:43.280
Inderjit Dhillon: the change in cut value

158
00:19:43.870 --> 00:19:49.600
Inderjit Dhillon: change and cut. If the 2 vertices are swapped.

159
00:19:52.380 --> 00:19:55.200
Inderjit Dhillon: So remember, this is one in V, one

160
00:19:56.230 --> 00:19:58.770
Inderjit Dhillon: and one vertex in V 2.

161
00:19:59.760 --> 00:20:07.170
Inderjit Dhillon: Okay, and i'm saying. I want to talk, See if swapping. Actually, what changed in the cut?

162
00:20:09.120 --> 00:20:15.450
Inderjit Dhillon: What is the change in the cut? And the algorithm would be swap the vertices

163
00:20:18.080 --> 00:20:19.100
Inderjit Dhillon: that

164
00:20:21.010 --> 00:20:22.790
Inderjit Dhillon: decrease

165
00:20:23.920 --> 00:20:27.380
Inderjit Dhillon: the cut the most.

166
00:20:28.910 --> 00:20:33.490
Inderjit Dhillon: So you basically. Look at every pair off notes.

167
00:20:33.780 --> 00:20:36.510
Inderjit Dhillon: One will be 1 one, and we 2.

168
00:20:36.710 --> 00:20:39.710
Inderjit Dhillon: You see what is the change in the cut value.

169
00:20:39.980 --> 00:20:42.530
Inderjit Dhillon: and then you make a move

170
00:20:43.100 --> 00:20:45.970
Inderjit Dhillon: which decreases the cut the most.

171
00:20:47.930 --> 00:20:55.020
Inderjit Dhillon: the actually the the kernel handling algorithm, is a little bit more sophisticated, which means that.

172
00:20:55.960 --> 00:20:57.190
Inderjit Dhillon: in fact.

173
00:20:58.000 --> 00:21:00.810
Inderjit Dhillon: it actually performs

174
00:21:01.800 --> 00:21:04.500
Inderjit Dhillon: sequence of these swaps.

175
00:21:10.310 --> 00:21:14.070
Inderjit Dhillon: not just exchanging one vertex with another.

176
00:21:14.320 --> 00:21:22.950
Inderjit Dhillon: but a sequence in a greedy manner that leads to the most decrease in the cut.

177
00:21:23.430 --> 00:21:31.050
Inderjit Dhillon: Okay, so clearly this. And when I say it, i'm talking about the kernel heuristic.

178
00:21:31.360 --> 00:21:37.130
Inderjit Dhillon: So clearly, the currently handling heuristic is a greedy. Algorithm

179
00:21:39.450 --> 00:21:46.110
Inderjit Dhillon: And then there are more sophisticated videos. More sophisticated

180
00:21:47.730 --> 00:21:49.500
Inderjit Dhillon: variants have been developed.

181
00:21:50.600 --> 00:21:58.820
Inderjit Dhillon: But this algorithm actually works reasonably well. It was actually used quite a bit for the circuit partitioning application, because.

182
00:21:58.900 --> 00:22:08.720
Inderjit Dhillon: you know, in many cases do you develop these heuristics? Sometimes you don't know if they work well in practice or not, but in practical or at graphs that came up in

183
00:22:08.780 --> 00:22:19.100
Inderjit Dhillon: circuit partitioning applications. It actually worked reasonably well. Okay. But and and then there were some more sophisticated variants.

184
00:22:19.320 --> 00:22:20.500
I mean

185
00:22:21.750 --> 00:22:24.450
Inderjit Dhillon: developed there will.

186
00:22:25.750 --> 00:22:33.490
Inderjit Dhillon: Okay. And one of them, you know, just for reference, is due to Fiducia. And

187
00:22:33.600 --> 00:22:35.200
Inderjit Dhillon: no, this is

188
00:22:40.600 --> 00:22:46.920
Inderjit Dhillon: okay. Now, in many machine learning applications, you might say. Hey, you know why

189
00:22:47.200 --> 00:22:50.390
Inderjit Dhillon: is why do you need a exact balance?

190
00:22:51.520 --> 00:23:06.470
Inderjit Dhillon: But they may not be exactly, you know. Suppose you have a friendship graph, and there's no reason to believe that the cluster should be exactly balanced. Maybe there's a one larger group of friends and one smaller group of friends.

191
00:23:06.770 --> 00:23:12.690
Inderjit Dhillon: So then the question is, how do you? But but you can't just use cut, because if you use cut.

192
00:23:12.820 --> 00:23:15.680
Inderjit Dhillon: you get this case.

193
00:23:16.860 --> 00:23:19.400
Inderjit Dhillon: You will get these undesirable partitions.

194
00:23:19.670 --> 00:23:27.410
Inderjit Dhillon: so can you have some other criterion which allows you to kind of minimize cut while keeping

195
00:23:27.470 --> 00:23:31.160
Inderjit Dhillon: the cluster sizes to be reasonably. But

196
00:23:32.560 --> 00:23:39.950
Inderjit Dhillon: and this can be done by defining what are called the weighted graph cuts.

197
00:23:47.770 --> 00:23:51.650
Inderjit Dhillon: Okay. So one of them is

198
00:23:52.500 --> 00:23:54.650
Inderjit Dhillon: what's called the ratio cut.

199
00:23:56.100 --> 00:24:01.720
Inderjit Dhillon: Okay, so let me define racial cut or the graph G, and we'll call it Rc.

200
00:24:01.850 --> 00:24:05.680
Inderjit Dhillon: Off. G: Okay, and that is equal to

201
00:24:05.960 --> 00:24:11.800
Inderjit Dhillon: what you wanted to to try and do. The ratio cut is defined as following. Let me first define it.

202
00:24:12.810 --> 00:24:15.990
Inderjit Dhillon: So cut v. One v. 2

203
00:24:16.410 --> 00:24:23.800
Inderjit Dhillon: divided by the size of V. One plus same thing Cut V. One V. 2

204
00:24:24.080 --> 00:24:26.230
Inderjit Dhillon: divided by the size of V 2,

205
00:24:28.880 --> 00:24:37.100
Inderjit Dhillon: and what you can do is you can, I think it's very similar to one of the the the things that we looked at last yesterday or 2 days ago.

206
00:24:37.120 --> 00:24:48.620
Inderjit Dhillon: where you had this n one and 2 come into the Criterion right? So if you think about this, this is also like the you know. If if I take 2, cut V, one v 2,

207
00:24:48.760 --> 00:24:50.410
Inderjit Dhillon: this will be cut

208
00:24:51.320 --> 00:24:58.060
Inderjit Dhillon: V. One, v, 2 times, one divided by V, one plus one divided by

209
00:24:58.420 --> 00:25:00.770
Inderjit Dhillon: we 2, and this quantity

210
00:25:01.240 --> 00:25:05.370
Inderjit Dhillon: is equal to V, 2 plus v, one

211
00:25:09.770 --> 00:25:18.690
Inderjit Dhillon: v, one divided by V. One me, too. and not that this is exactly equal to M. So this is the concept

212
00:25:19.730 --> 00:25:23.120
Inderjit Dhillon: cool, so I can find this as cut off

213
00:25:23.280 --> 00:25:27.140
Inderjit Dhillon: V. One B 2 divided by.

214
00:25:27.270 --> 00:25:28.920
Inderjit Dhillon: We want.

215
00:25:30.250 --> 00:25:38.220
Inderjit Dhillon: We know similar to that right here, and we came, and you can see you can take the case when let's say v. One and V. 2 are equal in size

216
00:25:38.640 --> 00:25:52.670
Inderjit Dhillon: versus when they are not equal to the size. Okay. and you'll see that this will prefer more equal, equally balanced clusters. But there is no explicit

217
00:25:53.740 --> 00:25:57.600
Inderjit Dhillon: and forcing that the clusters be of the same size.

218
00:25:57.900 --> 00:26:06.360
Inderjit Dhillon: Then there's another cut. There's another notion of a weighted graph cut which is called

219
00:26:06.410 --> 00:26:08.270
Inderjit Dhillon: normalized.

220
00:26:09.540 --> 00:26:10.730
Inderjit Dhillon: Well

221
00:26:14.920 --> 00:26:16.400
normalize cut.

222
00:26:18.610 --> 00:26:28.620
Inderjit Dhillon: Oh, and we'll call it Nc. Of G. And there we have a slightly different normalization. So it's again similar to

223
00:26:28.930 --> 00:26:33.550
Inderjit Dhillon: dot v. One V. 2 divided by a denominator, and cut

224
00:26:34.150 --> 00:26:39.680
Inderjit Dhillon: V. One V. 2 divided by a denominator for the denominator. Now.

225
00:26:39.830 --> 00:26:42.760
Inderjit Dhillon: because not the number of nodes in V one.

226
00:26:42.770 --> 00:26:46.250
Inderjit Dhillon: but it's actually degree of people.

227
00:26:47.510 --> 00:26:53.970
Inderjit Dhillon: Okay. And i'll let me explain what degree of V. One and V 2 is degree of V. One.

228
00:26:54.140 --> 00:26:56.430
Inderjit Dhillon: So we wanted to set.

229
00:26:57.240 --> 00:26:59.510
Inderjit Dhillon: It has a number of vertices.

230
00:27:00.250 --> 00:27:07.730
Inderjit Dhillon: so the degree of each node is the total sum of all the edge weights

231
00:27:08.420 --> 00:27:12.960
Inderjit Dhillon: that are incident on this or note.

232
00:27:15.350 --> 00:27:20.780
Inderjit Dhillon: Okay, so it's the sum of all edge weights. So, for example, if I look over here

233
00:27:22.230 --> 00:27:24.420
Inderjit Dhillon: the degree of this node

234
00:27:25.700 --> 00:27:35.570
Inderjit Dhillon: would be 1.5 plus one plus 1.5. The degree of this node would be 1.5 plus one plus 2 plus one.

235
00:27:36.970 --> 00:27:44.240
Inderjit Dhillon: The nice thing about this weighted draft card is, you know, the cut is this right? The cut is 2

236
00:27:44.660 --> 00:27:45.540
Inderjit Dhillon: right

237
00:27:46.600 --> 00:27:53.460
Inderjit Dhillon: in ratio current. You are just looking at the number of vertices. So that's 1 2 3 4, 1 2 3 4,

238
00:27:55.710 --> 00:28:02.830
Inderjit Dhillon: So the numerator doesn't quite match the denominator. whereas when you look at the normalization by the degree

239
00:28:03.410 --> 00:28:09.140
Inderjit Dhillon: well, the numerator is cut which is depending, not dependent upon the edge weights.

240
00:28:09.600 --> 00:28:20.690
Inderjit Dhillon: and I should. The denominator is also dependent on the So I actually very much like this normalized criteria, because it's essentially a dimensionless quantity.

241
00:28:21.190 --> 00:28:21.900
Inderjit Dhillon: What?

242
00:28:22.130 --> 00:28:24.550
Inderjit Dhillon: So so?

243
00:28:25.380 --> 00:28:30.020
Inderjit Dhillon: And and and let me kind of make sure that I define degree.

244
00:28:30.610 --> 00:28:34.650
Inderjit Dhillon: I'm just going to write down what I said. So degree of

245
00:28:34.680 --> 00:28:36.800
Inderjit Dhillon: we suppose I say.

246
00:28:37.170 --> 00:28:40.070
Inderjit Dhillon: see it could be one or 2

247
00:28:40.250 --> 00:28:43.580
Inderjit Dhillon: is equal to summation of a. I. J.

248
00:28:46.550 --> 00:28:49.180
Inderjit Dhillon: V. I belong to Vc.

249
00:28:50.470 --> 00:28:53.720
Inderjit Dhillon: And I. J. Belongs to

250
00:28:55.570 --> 00:28:56.780
Inderjit Dhillon: what we call.

251
00:29:00.220 --> 00:29:13.270
Inderjit Dhillon: and sometimes you can just call this. As you know, it's basically the links okay between we see and all of the so links

252
00:29:13.350 --> 00:29:16.150
Inderjit Dhillon: of what to see in Vc.

253
00:29:16.320 --> 00:29:21.840
Inderjit Dhillon: Within the cluster and outside the just.

254
00:29:23.130 --> 00:29:29.800
Inderjit Dhillon: So once we define these weighted graph cuts. then you know what I can say that

255
00:29:29.850 --> 00:29:36.070
Inderjit Dhillon: I want to minimize, not just the cuts so earlier we have this right that

256
00:29:36.710 --> 00:29:38.800
Inderjit Dhillon: minimize the cut subject

257
00:29:39.070 --> 00:29:50.780
Inderjit Dhillon: to this construct. Now I have to kind of one right here, so I can say I want to minimize this over all we want to. That will be the ratio card.

258
00:29:51.470 --> 00:29:53.400
Inderjit Dhillon: and this would be

259
00:29:55.660 --> 00:29:57.570
the normal is correct.

260
00:29:59.340 --> 00:30:01.090
Inderjit Dhillon: So any questions so far.

261
00:30:02.400 --> 00:30:11.280
Inderjit Dhillon: Not that other than looking at the and the Lynn algorithm we haven't actually talked too much about algorithms on this. Yet we've been mainly discussing kind of criteria.

262
00:30:15.630 --> 00:30:16.890
Inderjit Dhillon: Any questions.

263
00:30:21.040 --> 00:30:24.620
Inderjit Dhillon: I'm just looking at what I wrote, and I think this is not.

264
00:30:25.880 --> 00:30:29.940
Inderjit Dhillon: Let me just change this to No.

265
00:30:32.420 --> 00:30:33.240
okay.

266
00:30:34.140 --> 00:30:37.910
Inderjit Dhillon: So I can generalize this now to K. Clusters.

267
00:30:43.770 --> 00:30:50.860
Inderjit Dhillon: Not just 2 clusters. Okay. So Rc. The.

268
00:30:51.040 --> 00:30:54.620
Inderjit Dhillon: I can see. I want to find. I have

269
00:30:55.240 --> 00:30:56.430
Inderjit Dhillon: we one

270
00:30:56.550 --> 00:31:01.990
Inderjit Dhillon: V, 2 through V. K. And what I want to find is

271
00:31:02.560 --> 00:31:07.280
Inderjit Dhillon: I equals 1, 2, K. I want to minimize the

272
00:31:08.490 --> 00:31:12.130
Inderjit Dhillon: V. I. The J.

273
00:31:12.610 --> 00:31:16.870
Inderjit Dhillon: Actually let me just say it to be consistent. Let me say that this is C.

274
00:31:18.970 --> 00:31:20.260
Inderjit Dhillon: Sorry.

275
00:31:22.160 --> 00:31:24.160
Inderjit Dhillon: c. Equals one

276
00:31:27.370 --> 00:31:28.690
I have

277
00:31:29.010 --> 00:31:29.900
Inderjit Dhillon: cut

278
00:31:30.420 --> 00:31:36.570
Inderjit Dhillon: of. We see with the rest of the graph we minus DC:

279
00:31:38.710 --> 00:31:41.470
Okay. And then this is the ratio card.

280
00:31:41.700 --> 00:31:44.290
Inderjit Dhillon: So I'm: dividing by DC:

281
00:31:45.490 --> 00:31:52.230
Inderjit Dhillon: Okay, you can check to make sure that this is exactly the same as the above. When

282
00:31:52.970 --> 00:31:57.340
Inderjit Dhillon: when I have 2 clusters, that means K is equal to 2.

283
00:31:58.750 --> 00:32:04.020
Inderjit Dhillon: Okay. And I can kind of define. I can look at this quantity

284
00:32:04.680 --> 00:32:15.850
Inderjit Dhillon: this quantity, and actually, maybe I should not circle it. But you know, this quantity is also. I can see this is the cut between Vc.

285
00:32:16.100 --> 00:32:17.760
And the rest of the graph.

286
00:32:18.240 --> 00:32:21.620
Inderjit Dhillon: So what I can do is I can say that this is equal to

287
00:32:21.780 --> 00:32:24.990
Inderjit Dhillon: the degree of we see.

288
00:32:25.500 --> 00:32:34.610
Inderjit Dhillon: so that includes the edges within the gr. Within the cluster, and outside the cluster the outside the cluster are the ones that correspond to the cut.

289
00:32:34.910 --> 00:32:43.580
Inderjit Dhillon: So I can say that this is degree minus links within the graph within the cluster we see.

290
00:32:46.150 --> 00:32:47.020
Okay.

291
00:32:47.620 --> 00:32:51.010
Inderjit Dhillon: Similarly, I can do normalize. Cut

292
00:32:53.390 --> 00:32:55.360
again.

293
00:32:55.650 --> 00:33:00.350
Inderjit Dhillon: I have K. Clusters V: One V. 2 through. V. K.

294
00:33:01.890 --> 00:33:06.080
And I can define this criterion, which is, I have cut.

295
00:33:07.070 --> 00:33:15.590
Inderjit Dhillon: We see B minus Vc. Same thing as above, except the denominator, is now degree of we see.

296
00:33:16.540 --> 00:33:23.950
Inderjit Dhillon: Okay, but but not one thing over here. And this is where you know the fact that it is dimensionless comes in right.

297
00:33:24.010 --> 00:33:36.030
If I look at this criterion. If I look at this just as above, I can write it as degree of Vc. Minus links.

298
00:33:36.910 --> 00:33:43.480
Inderjit Dhillon: we see, we see. and the denominator is also a degree of BC.

299
00:33:45.740 --> 00:33:50.590
Inderjit Dhillon: So I can write it actually as one for degree of Vc. Divided by

300
00:33:50.800 --> 00:33:56.430
Inderjit Dhillon: degree of Vc. Divided by degree of Vc. That's one. And then I have minus links

301
00:33:57.500 --> 00:34:02.060
Inderjit Dhillon: within the cluster divided by degree. Okay.

302
00:34:02.350 --> 00:34:05.350
Inderjit Dhillon: So minus links.

303
00:34:06.110 --> 00:34:12.270
Inderjit Dhillon: we see Vc. Divided by the degree of.

304
00:34:13.130 --> 00:34:14.550
Inderjit Dhillon: and the goal

305
00:34:16.350 --> 00:34:17.590
is

306
00:34:18.060 --> 00:34:19.750
Inderjit Dhillon: minimize

307
00:34:23.469 --> 00:34:25.610
Inderjit Dhillon: many lies.

308
00:34:26.159 --> 00:34:27.719
Inderjit Dhillon: These

309
00:34:29.159 --> 00:34:30.520
Inderjit Dhillon: we did

310
00:34:33.699 --> 00:34:34.920
Inderjit Dhillon: the

311
00:34:36.480 --> 00:34:37.300
Inderjit Dhillon: good

312
00:34:37.610 --> 00:34:39.060
Inderjit Dhillon: objects.

313
00:34:40.330 --> 00:34:44.290
Inderjit Dhillon: Okay. So, for example, this could be ratio cut

314
00:34:46.100 --> 00:34:48.880
Inderjit Dhillon: or it could be normalized.

315
00:35:01.780 --> 00:35:03.510
Inderjit Dhillon: Okay, any questions.

316
00:35:04.960 --> 00:35:14.400
Edward Leung: Yeah. I I wanted to ask. What does it mean to for the the links of Vc. With what exactly does that mean?

317
00:35:14.430 --> 00:35:19.740
Inderjit Dhillon: I kind of just introduced notation. But the links of Vc. To Vc. Is

318
00:35:20.960 --> 00:35:26.890
Inderjit Dhillon: basically I look at each note and look at all the

319
00:35:26.980 --> 00:35:30.110
Inderjit Dhillon: edges within this cluster.

320
00:35:32.320 --> 00:35:34.230
Inderjit Dhillon: So, for example, over here

321
00:35:34.900 --> 00:35:36.710
Inderjit Dhillon: the links of this

322
00:35:36.980 --> 00:35:38.920
Inderjit Dhillon: to the

323
00:35:39.080 --> 00:35:44.900
Inderjit Dhillon: 2 V. One is going to be 1.5 1 1.5,

324
00:35:45.560 --> 00:35:48.180
Inderjit Dhillon: and then the link of this

325
00:35:48.530 --> 00:35:55.160
Inderjit Dhillon: within the cluster would be 1.5, one, and 2. But we would not count this because this is outside.

326
00:35:58.370 --> 00:36:01.680
Okay. So maybe

327
00:36:01.990 --> 00:36:04.140
Inderjit Dhillon: did I define links.

328
00:36:07.350 --> 00:36:10.250
Inderjit Dhillon: So I I kind of define

329
00:36:13.140 --> 00:36:14.610
Inderjit Dhillon: this over here

330
00:36:15.210 --> 00:36:21.910
Inderjit Dhillon: links of Vc. To be right. and if I now look at links of

331
00:36:23.340 --> 00:36:25.760
we see, we see

332
00:36:26.280 --> 00:36:29.940
Inderjit Dhillon: I can write it as a summation of a. It.

333
00:36:30.480 --> 00:36:35.150
Inderjit Dhillon: V. I. V. J. Belongs to Vc.

334
00:36:38.900 --> 00:36:46.030
Inderjit Dhillon: This is the definition of next we see. Do we see, basically it's the sum of all the edge weights

335
00:36:46.610 --> 00:36:48.190
Inderjit Dhillon: within the cluster.

336
00:36:49.130 --> 00:36:50.840
Does that make sense? So

337
00:36:52.470 --> 00:36:58.370
Edward Leung: yeah, yeah, it so so the degree is the it's the links with like of the

338
00:36:59.120 --> 00:37:07.570
Edward Leung: I guess the graph with the other cuts. And then, yeah, exactly so. If I think about it, right

339
00:37:09.710 --> 00:37:13.670
Inderjit Dhillon: degree of Vc. Is

340
00:37:14.240 --> 00:37:17.530
Inderjit Dhillon: more than links of Vc. To Vc.

341
00:37:19.800 --> 00:37:21.860
Inderjit Dhillon: Right. So degree of Vc.

342
00:37:25.100 --> 00:37:27.750
Inderjit Dhillon: Is links within

343
00:37:29.710 --> 00:37:37.300
Inderjit Dhillon: the cluster plus links. We see 2 V minus Vc.

344
00:37:40.850 --> 00:37:42.100
Inderjit Dhillon: So degree

345
00:37:43.090 --> 00:37:46.430
Inderjit Dhillon: we will. So suppose i'm looking at V. One

346
00:37:47.460 --> 00:37:52.230
Inderjit Dhillon: right links of Vc. We see, is basically some of all edges over here.

347
00:37:53.770 --> 00:38:00.440
Inderjit Dhillon: Links of V. One to V 2 is basically this

348
00:38:02.050 --> 00:38:06.480
Inderjit Dhillon: and the degree includes this and these 2 others.

349
00:38:06.970 --> 00:38:08.990
Inderjit Dhillon: If I'm: looking at a degree of you want

350
00:38:09.120 --> 00:38:09.980
Edward Leung: makes sense.

351
00:38:10.360 --> 00:38:11.920
Inderjit Dhillon: make sense. Okay.

352
00:38:12.370 --> 00:38:13.240
Inderjit Dhillon: Thanks.

353
00:38:15.190 --> 00:38:18.450
Inderjit Dhillon: Okay, so so far, we've kind of just been

354
00:38:19.070 --> 00:38:22.190
Inderjit Dhillon: mean, defining things.

355
00:38:23.440 --> 00:38:31.640
Inderjit Dhillon: And now let's come to an algorithm Okay. And this is kind of a celebrated algorithm comes up in

356
00:38:32.310 --> 00:38:37.520
Inderjit Dhillon: It's it's kind of a neat algorithm. And this is called Spectral Close Friend.

357
00:38:44.500 --> 00:38:54.040
Inderjit Dhillon: Okay. and spectral clustering. Let me just say it's a heuristic. Remember that these the objectives are Np. Hard to solve.

358
00:38:54.260 --> 00:38:58.940
Inderjit Dhillon: So all we can get our heuristics that attempt to minimize these objective

359
00:38:59.260 --> 00:39:07.800
Inderjit Dhillon: and spectral clustering again is a heuristic that twice to minimize

360
00:39:11.120 --> 00:39:12.670
Inderjit Dhillon: these objectives.

361
00:39:17.760 --> 00:39:28.130
Inderjit Dhillon: But it is really a nice heuristic, because it's kind of has this global flavor, and and we'll talk a little bit about why it has global flavor. What do I mean by global flavor?

362
00:39:28.190 --> 00:39:38.900
Inderjit Dhillon: Well, if you look at this heuristic right, it's very local. It it kind of just makes these small swaps

363
00:39:39.420 --> 00:39:47.910
Inderjit Dhillon: right without being able to look at the graph as a whole. Okay, what spectral clustering does it kind of looks at the whole.

364
00:39:47.980 --> 00:39:50.250
Inderjit Dhillon: they would look at the graph as a whole

365
00:39:50.860 --> 00:39:59.020
Inderjit Dhillon: and does close spring. It's actually a very neat okay. And the way the algorithm proceeds to be as follows.

366
00:39:59.320 --> 00:40:09.740
Inderjit Dhillon: So before I go into detail and kind of justify spectral clustering, let me kind of just describe how the algorithm will work. So what we will do is we will define a graph.

367
00:40:10.760 --> 00:40:20.090
Inderjit Dhillon: We'll define a matrix which is associated with the adjacency matrix of the graph. Okay. And this will be called the Graph Laplacian.

368
00:40:21.490 --> 00:40:33.630
Inderjit Dhillon: Okay. So we and it's essentially the it's basically the degree matrix minus the a matrix which is the weighted adjacency.

369
00:40:34.570 --> 00:40:37.990
Inderjit Dhillon: So you take that matrix, the algorithm is actually extremely simple.

370
00:40:38.470 --> 00:40:42.900
Inderjit Dhillon: Take this matrix. L is equal to D minus a

371
00:40:44.190 --> 00:40:48.420
Inderjit Dhillon: and computers smallest eigenvectors.

372
00:40:49.750 --> 00:40:59.240
Inderjit Dhillon: It turns out that the smallest eigenvector is no use because of a reason, i'll tell you. and you look at the second smallest I can get.

373
00:41:01.070 --> 00:41:06.420
Inderjit Dhillon: There was a a check mathematician called

374
00:41:06.540 --> 00:41:12.510
Inderjit Dhillon: Who I, who wrote about this in a paper. So sometimes it's called the Feed Lerbit.

375
00:41:13.680 --> 00:41:16.120
Inderjit Dhillon: So you compute this eigenvector.

376
00:41:16.350 --> 00:41:20.670
Inderjit Dhillon: and then you start partitioning the graph based upon the

377
00:41:21.910 --> 00:41:32.610
Inderjit Dhillon: is that a one eigenvector you can use, or instead of the second smallest eigenvector, you can use the smallest Yeah, I can vectors. And then there's a question of how you go from the eigenvectors

378
00:41:32.790 --> 00:41:34.410
Inderjit Dhillon: to partitioning.

379
00:41:34.780 --> 00:41:37.810
Inderjit Dhillon: but an extremely simple, algorithm

380
00:41:38.450 --> 00:41:41.780
Inderjit Dhillon: extremely popular and effective.

381
00:41:43.030 --> 00:41:57.680
Inderjit Dhillon: But what i'm doing over here is kind of justifying why you can actually use that algorithm. Okay. And the reason will be that just like I just over here, I define these objectives

382
00:41:58.010 --> 00:41:58.860
Inderjit Dhillon: correct.

383
00:41:59.330 --> 00:42:01.940
Inderjit Dhillon: I raise your cut and normalize cut.

384
00:42:02.200 --> 00:42:05.250
Inderjit Dhillon: I will write this in terms of the graph laughter.

385
00:42:06.660 --> 00:42:12.470
Inderjit Dhillon: and then this partitioning will be like a discrete

386
00:42:14.270 --> 00:42:21.320
Inderjit Dhillon: constraint on how to partition the graph. and it turns out that when you relax this discrete constraint.

387
00:42:21.830 --> 00:42:27.120
Inderjit Dhillon: the minimizer of the objective turns out to be given by the eigenvectors

388
00:42:29.450 --> 00:42:39.060
Inderjit Dhillon: eigenvectors of the You have flash. so I kind of I've given you a preview of how spectrum the question works. So now let's

389
00:42:39.330 --> 00:42:42.930
Inderjit Dhillon: dive down into the details. Okay.

390
00:42:43.060 --> 00:42:47.710
Inderjit Dhillon: so spectrum to string. What I will do is I will

391
00:42:48.900 --> 00:42:56.840
Inderjit Dhillon: Let's let's I I i'm. Now, not giving the algorithm but I'm: trying to motivate the algorithm okay and justify Why, it is okay.

392
00:43:01.680 --> 00:43:05.770
Inderjit Dhillon: So what i'll do is I will associate

393
00:43:08.050 --> 00:43:09.770
Inderjit Dhillon: an indicator vector

394
00:43:12.900 --> 00:43:16.430
Inderjit Dhillon: indicator. But

395
00:43:18.030 --> 00:43:19.350
Inderjit Dhillon: with each cluster.

396
00:43:19.490 --> 00:43:25.300
Inderjit Dhillon: So there are skate clusters. So I have y one, y 2,

397
00:43:25.500 --> 00:43:29.090
Inderjit Dhillon: why, K. For each cluster.

398
00:43:32.590 --> 00:43:38.700
Inderjit Dhillon: and the fact that i'm calling it indicator vector means the following: that if I have

399
00:43:39.030 --> 00:43:41.880
Inderjit Dhillon: partition, see

400
00:43:44.820 --> 00:43:51.200
Inderjit Dhillon: then why c. Of the J. Component of this vector

401
00:43:51.370 --> 00:43:55.630
Inderjit Dhillon: is one. If the corresponding vertex v. J.

402
00:43:56.120 --> 00:44:00.700
Inderjit Dhillon: Belongs to Vc. And it is 0, otherwise.

403
00:44:04.670 --> 00:44:08.400
Inderjit Dhillon: So it's an indicator. Vector so you look at the vertices

404
00:44:08.540 --> 00:44:14.660
Inderjit Dhillon: depending upon what partition it belongs to. You. Put a one in the corresponding location.

405
00:44:16.540 --> 00:44:26.990
Inderjit Dhillon: Okay. And you can note that if I have Yc: one and by C 2, and I take the inner product

406
00:44:27.040 --> 00:44:31.400
Inderjit Dhillon: between Y. C, one and y C 2. This is equal to 0,

407
00:44:32.040 --> 00:44:34.460
Inderjit Dhillon: c. One is not equal to c. 2

408
00:44:34.970 --> 00:44:38.750
Inderjit Dhillon: because of this requirement, right that

409
00:44:41.300 --> 00:44:46.430
Inderjit Dhillon: cool the clusters are disjoint

410
00:44:46.860 --> 00:44:48.530
Inderjit Dhillon: partitions are disjoint.

411
00:44:49.090 --> 00:44:50.610
Inderjit Dhillon: As a result

412
00:44:51.670 --> 00:44:56.400
Inderjit Dhillon: the indicator vectors will be kind of in disjoint positions.

413
00:44:56.470 --> 00:44:59.890
Inderjit Dhillon: which means that the inner product

414
00:45:01.000 --> 00:45:03.970
Inderjit Dhillon: between the indicator vector for cluster, one

415
00:45:04.140 --> 00:45:10.040
Inderjit Dhillon: will actually be orthogonal to so sorry the inner product will be 0, which means

416
00:45:10.160 --> 00:45:13.860
Inderjit Dhillon: so. The 2 indicator vectors are orthogonal to each other.

417
00:45:19.030 --> 00:45:21.500
Inderjit Dhillon: Then I will define a matrix.

418
00:45:24.960 --> 00:45:27.260
Inderjit Dhillon: a diagonal matrix D:

419
00:45:28.230 --> 00:45:32.590
Inderjit Dhillon: And this is going to be the diagonal matrix of degrees.

420
00:45:39.850 --> 00:45:42.940
Inderjit Dhillon: Okay. So this is

421
00:45:43.370 --> 00:45:45.110
N. By N. Matrix.

422
00:45:45.710 --> 00:45:57.530
Inderjit Dhillon: and there's a number of vertices. and this is degree of the first vertex degree of the second vertex degree of the

423
00:45:58.460 --> 00:46:02.540
Inderjit Dhillon: and cortex. and everywhere else is a 0

424
00:46:10.440 --> 00:46:19.200
Inderjit Dhillon: note that why c transpose Yc. is the number of vertices in basic

425
00:46:25.430 --> 00:46:27.300
Inderjit Dhillon: Okay. So

426
00:46:29.120 --> 00:46:32.010
Inderjit Dhillon: then you can look at other relationships.

427
00:46:34.240 --> 00:46:37.100
Inderjit Dhillon: I'm: going to try and connect what

428
00:46:37.370 --> 00:46:39.080
Inderjit Dhillon: I've written over here

429
00:46:40.410 --> 00:46:46.010
Inderjit Dhillon: in terms of this kind of notation in terms of the indicator, vector in terms of the

430
00:46:47.080 --> 00:46:49.940
Inderjit Dhillon: so it turns out that.

431
00:46:51.370 --> 00:46:55.780
Inderjit Dhillon: and you can verify this Yc. Transpose. D

432
00:46:55.790 --> 00:46:57.180
Inderjit Dhillon: Yc.

433
00:46:58.910 --> 00:47:01.070
Inderjit Dhillon: Is that actually the degree

434
00:47:02.220 --> 00:47:03.610
Inderjit Dhillon: of Vc.

435
00:47:09.550 --> 00:47:14.250
Inderjit Dhillon: Hopefully, that's clear? Right? So remember that Vc. Is an indicator, vector

436
00:47:14.550 --> 00:47:25.790
Inderjit Dhillon: and so Ycd. Times Yc. Is actually going to be my degree of V. One plus degree of V. 2

437
00:47:26.500 --> 00:47:29.890
Inderjit Dhillon: last degree of the

438
00:47:30.280 --> 00:47:37.110
Inderjit Dhillon: L. Where we want V 2 through the L. Belongs to Vc.

439
00:47:41.380 --> 00:47:42.620
On the edge.

440
00:47:45.810 --> 00:47:48.080
Inderjit Dhillon: Okay, As a result, this is true.

441
00:47:52.950 --> 00:47:53.930
Inderjit Dhillon: and

442
00:47:55.000 --> 00:48:02.700
Inderjit Dhillon: Yc. Transpose a of Y. C is equal to links

443
00:48:03.140 --> 00:48:05.290
Inderjit Dhillon: off Vc. With itself.

444
00:48:08.690 --> 00:48:15.610
Inderjit Dhillon: and to see that, you know. Remember that, you know. Suppose I have my a matrix.

445
00:48:16.840 --> 00:48:22.210
Inderjit Dhillon: and suppose all my nodes that correspond to V. One or over here.

446
00:48:23.680 --> 00:48:27.170
Inderjit Dhillon: So this part is V. One.

447
00:48:28.710 --> 00:48:31.750
Inderjit Dhillon: and if I consider why one.

448
00:48:32.960 --> 00:48:37.510
Inderjit Dhillon: this has 1 1 1 2, one, and zeros everywhere else.

449
00:48:37.990 --> 00:48:41.850
Inderjit Dhillon: And similarly, this is 1 1 1 0 is everywhere else.

450
00:48:42.710 --> 00:48:45.950
Inderjit Dhillon: So what Yc: transpose a

451
00:48:48.100 --> 00:48:52.510
Inderjit Dhillon: little y one. So what? Why, one transpose

452
00:48:54.440 --> 00:49:00.810
Inderjit Dhillon: a y does, is it? Basically, picks up all

453
00:49:01.950 --> 00:49:04.340
Inderjit Dhillon: the words, the weights over here.

454
00:49:05.030 --> 00:49:09.770
Inderjit Dhillon: And that's exactly equal to links of

455
00:49:10.990 --> 00:49:13.170
Inderjit Dhillon: we on.

456
00:49:14.690 --> 00:49:15.610
Inderjit Dhillon: Okay.

457
00:49:15.730 --> 00:49:17.470
So in general, this is true.

458
00:49:17.650 --> 00:49:19.290
Inderjit Dhillon: but for all C

459
00:49:20.220 --> 00:49:23.560
Inderjit Dhillon: Yc. Transpose a Yc.

460
00:49:23.860 --> 00:49:26.890
Inderjit Dhillon: Is equal to the links of Vc.

461
00:49:32.690 --> 00:49:36.760
Inderjit Dhillon: Okay. So then I can look at the ratio cut

462
00:49:38.550 --> 00:49:39.810
Inderjit Dhillon: of G.

463
00:49:42.370 --> 00:49:50.040
Inderjit Dhillon: Let me write it as it's cut. We see V minus Vc.

464
00:49:52.000 --> 00:49:56.610
Inderjit Dhillon: Divided by the absolute value of Vc. C. Equal to one to K.

465
00:49:57.570 --> 00:50:04.450
Inderjit Dhillon: Remember that this is the same as degree. Off we see minus links

466
00:50:04.720 --> 00:50:09.770
Inderjit Dhillon: off, we see, we see. So i'm just kind of repeating what we have done before

467
00:50:10.930 --> 00:50:16.970
Inderjit Dhillon: before we get introduced by the indicator vector walk. But now.

468
00:50:17.310 --> 00:50:18.860
Inderjit Dhillon: from what I have.

469
00:50:19.380 --> 00:50:23.140
degree of Vc. Is equal to this

470
00:50:24.580 --> 00:50:27.120
Inderjit Dhillon: links of the Cbc.

471
00:50:27.320 --> 00:50:31.560
Inderjit Dhillon: Is equal to this. so if I substitute that.

472
00:50:32.730 --> 00:50:34.110
Inderjit Dhillon: I will get

473
00:50:36.130 --> 00:50:39.820
Inderjit Dhillon: this expression in terms of vectors and matrices.

474
00:50:39.940 --> 00:50:45.250
Inderjit Dhillon: i'll get that. This is Y. C transpose d y c

475
00:50:45.880 --> 00:50:52.710
Inderjit Dhillon: minus Yc. Transpose a Vc. Divided by.

476
00:50:53.300 --> 00:50:55.330
Inderjit Dhillon: If you notice the

477
00:50:56.520 --> 00:50:58.670
Inderjit Dhillon: this relationship by hand.

478
00:51:01.740 --> 00:51:02.990
Inderjit Dhillon: I have

479
00:51:03.000 --> 00:51:17.400
Inderjit Dhillon: Vc. Transpose. What is it which means that I can write this as summation of C equals one through K Yc. Transpose D minus a

480
00:51:18.120 --> 00:51:22.910
Inderjit Dhillon: Yc. Divided by Yc. Transpose Yc.

481
00:51:24.320 --> 00:51:34.580
Inderjit Dhillon: And this is what I was saying is the the graph law version. So I can write this as a summation of a. C is a good one through K

482
00:51:35.490 --> 00:51:37.590
Inderjit Dhillon: Yc: transpose.

483
00:51:40.320 --> 00:51:42.070
Inderjit Dhillon: Yeah. Sorry

484
00:51:42.690 --> 00:51:46.670
Inderjit Dhillon: i'm at all. Yc: divided by

485
00:51:46.820 --> 00:51:48.970
Inderjit Dhillon: Why is he

486
00:51:49.790 --> 00:51:51.580
Inderjit Dhillon: okay? Where L

487
00:51:53.020 --> 00:51:55.970
Inderjit Dhillon: is D minus a.

488
00:51:56.940 --> 00:51:58.490
Inderjit Dhillon: and it's called

489
00:52:00.260 --> 00:52:08.080
Inderjit Dhillon: It's very important. Construct in computer science. Even in graph theory, it's called the graph Laplace.

490
00:52:13.770 --> 00:52:18.810
Inderjit Dhillon: So just think about what else is it is essentially

491
00:52:21.100 --> 00:52:22.410
minus a

492
00:52:23.460 --> 00:52:32.380
Inderjit Dhillon: But remember, if the graph has no self loops which we will, for now I assume that the graph has no self loops. The diagonal is 0. So you put

493
00:52:32.640 --> 00:52:36.720
Inderjit Dhillon: and d all the

494
00:52:37.140 --> 00:52:42.090
Inderjit Dhillon: degree, and the degree is actually the sum of all the elements in a

495
00:52:45.080 --> 00:52:46.480
Inderjit Dhillon: in that particular role.

496
00:52:48.800 --> 00:52:49.650
Inderjit Dhillon: Okay.

497
00:52:51.270 --> 00:52:54.740
Inderjit Dhillon: So the graph Laplaceian has.

498
00:52:55.560 --> 00:53:00.710
Inderjit Dhillon: Well, let me write a few properties of the graph, so you can show

499
00:53:02.440 --> 00:53:03.590
Inderjit Dhillon: that

500
00:53:04.380 --> 00:53:08.800
Inderjit Dhillon: L is equal to E. E: transpose

501
00:53:09.860 --> 00:53:12.090
Inderjit Dhillon: where he

502
00:53:15.140 --> 00:53:17.120
Inderjit Dhillon: is, the

503
00:53:17.480 --> 00:53:20.330
Inderjit Dhillon: node Edge adjacency matrix.

504
00:53:29.530 --> 00:53:36.920
Inderjit Dhillon: Okay. So in particular, e is node edge, right. So it's basically vertices over here

505
00:53:37.590 --> 00:53:40.340
Inderjit Dhillon: and then edges over here.

506
00:53:40.570 --> 00:53:50.670
Inderjit Dhillon: So each edge is between one vertex and the other, and it says square square root Wij and minus square root. W. I. T.

507
00:53:50.930 --> 00:53:54.510
Inderjit Dhillon: Okay, this is I. This is

508
00:53:57.630 --> 00:54:08.120
Inderjit Dhillon: It's it's not that important. But what this means is because I can write L. As E. E Transpose then, if you now

509
00:54:08.230 --> 00:54:17.000
Inderjit Dhillon: talking back to the you know one of the first few lectures we had or back, you know, background ted in in linear linear algebra.

510
00:54:17.170 --> 00:54:20.000
Inderjit Dhillon: L is a symmetric.

511
00:54:22.200 --> 00:54:23.430
Inderjit Dhillon: all the of

512
00:54:25.040 --> 00:54:26.640
Inderjit Dhillon: definite matrix.

513
00:54:30.390 --> 00:54:32.600
Inderjit Dhillon: Actually, it is

514
00:54:34.730 --> 00:54:45.490
Inderjit Dhillon: okay, and it is positive semi-definite, because times the ones vector is d minus a times, the ones vector is actually equal to 0.

515
00:54:47.280 --> 00:54:49.740
Inderjit Dhillon: So so 0 is an eigenvalue.

516
00:54:49.760 --> 00:54:52.170
Inderjit Dhillon: and all the one the one vector

517
00:54:52.240 --> 00:54:59.220
Inderjit Dhillon: is I. It essentially says that if I put all the vertices

518
00:55:00.750 --> 00:55:04.450
Inderjit Dhillon: in one partition. then there is no cut

519
00:55:05.180 --> 00:55:15.520
Inderjit Dhillon: right, because everything is in one partition. We V. One is V. We 2 is essentially no

520
00:55:16.560 --> 00:55:19.450
Inderjit Dhillon: Okay. So

521
00:55:22.650 --> 00:55:26.760
Inderjit Dhillon: so here's the let me see.

522
00:55:28.260 --> 00:55:33.290
Inderjit Dhillon: Then there are some other properties. You can show that for any

523
00:55:35.510 --> 00:55:38.900
Inderjit Dhillon: vector x.

524
00:55:39.720 --> 00:55:47.710
Inderjit Dhillon: X. Transpose L. X is equal to x transpose e. E. Transpose x is equal to

525
00:55:47.960 --> 00:55:50.680
Inderjit Dhillon: permission of Wid.

526
00:55:51.720 --> 00:55:54.830
Inderjit Dhillon: X. I. Minus x k square

527
00:55:55.850 --> 00:55:59.940
Inderjit Dhillon: that I have I They belongs to E.

528
00:56:05.780 --> 00:56:08.610
Inderjit Dhillon: What did my screen go away?

529
00:56:12.350 --> 00:56:14.840
Nilesh Gupta: No, we can see that. See the screen.

530
00:56:15.710 --> 00:56:16.750
So

531
00:56:17.330 --> 00:56:21.600
Inderjit Dhillon: I don't know. Maybe I did something where I can't see the screen on my

532
00:56:36.410 --> 00:56:39.050
Inderjit Dhillon: sorry I am. Okay.

533
00:56:47.410 --> 00:56:50.380
Inderjit Dhillon: So let me also write down what?

534
00:56:52.680 --> 00:56:59.220
Inderjit Dhillon: Let me actually just write down. I don't want to go like overwhelming you a little bit with too much algebra.

535
00:56:59.550 --> 00:57:04.210
Inderjit Dhillon: So what you can show is that the normalized cut of G.

536
00:57:05.710 --> 00:57:09.220
Inderjit Dhillon: Okay, that is the same as

537
00:57:21.140 --> 00:57:26.520
Inderjit Dhillon: it is essentially Yc transpose d minus a.

538
00:57:26.580 --> 00:57:27.850
Inderjit Dhillon: Why, I see

539
00:57:28.170 --> 00:57:41.910
Inderjit Dhillon: the numerator is slightly different. It is yc transpose dyc. So it goes one to so subtle difference. Yc: transpose Dyc over here.

540
00:57:42.260 --> 00:57:44.440
Inderjit Dhillon: whereas in racial cut

541
00:57:48.380 --> 00:57:50.560
Inderjit Dhillon: it is why I see.

542
00:57:51.620 --> 00:57:54.310
Inderjit Dhillon: Okay, so this is

543
00:57:58.590 --> 00:58:04.090
Inderjit Dhillon: this is what Horatio Card is and

544
00:58:05.520 --> 00:58:11.010
Inderjit Dhillon: normalized cut is summation of C goes one through K.

545
00:58:11.350 --> 00:58:15.540
Inderjit Dhillon: Yc.

546
00:58:15.900 --> 00:58:19.890
Inderjit Dhillon: divided by Yc: transpose.

547
00:58:22.960 --> 00:58:26.800
Inderjit Dhillon: Okay, so just slightly different can actually make a big difference. But

548
00:58:29.850 --> 00:58:36.660
Inderjit Dhillon: in practice. Okay. So this contrast this with each other.

549
00:58:42.520 --> 00:58:44.060
Inderjit Dhillon: This is the difference.

550
00:58:49.610 --> 00:58:50.570
Inderjit Dhillon: So

551
00:58:50.790 --> 00:58:54.720
Inderjit Dhillon: what you can essentially show is that

552
00:58:59.620 --> 00:59:01.070
Inderjit Dhillon: racial cut

553
00:59:03.210 --> 00:59:04.240
Inderjit Dhillon: G

554
00:59:05.590 --> 00:59:08.260
Inderjit Dhillon: is minimum

555
00:59:08.640 --> 00:59:13.730
Inderjit Dhillon: over, you know. Now y one, y 2, through y k

556
00:59:14.600 --> 00:59:18.200
Inderjit Dhillon: summation of C equals one through K.

557
00:59:19.250 --> 00:59:21.620
Inderjit Dhillon: Y. C. Transpose L.

558
00:59:21.960 --> 00:59:35.630
Inderjit Dhillon: Yc. And why a C of I is equal to one if we I belong to cluster, C

559
00:59:35.830 --> 00:59:38.110
Inderjit Dhillon: and a 0, otherwise.

560
00:59:40.430 --> 00:59:41.250
Inderjit Dhillon: Okay.

561
00:59:43.300 --> 00:59:45.580
Inderjit Dhillon: So again, let's let's see what this means.

562
00:59:47.630 --> 00:59:53.550
Inderjit Dhillon: This is saying that I have taken my ratio cut problem.

563
00:59:54.290 --> 00:59:59.180
Inderjit Dhillon: and now posted as finding these vectors.

564
00:59:59.450 --> 01:00:07.830
Inderjit Dhillon: these indicator vectors such that this quantity is. I want this to be minimized.

565
01:00:09.500 --> 01:00:15.460
Inderjit Dhillon: Okay. So one way to think about spectral

566
01:00:21.540 --> 01:00:26.740
Inderjit Dhillon: spectral clustering is heuristic

567
01:00:30.040 --> 01:00:31.060
Inderjit Dhillon: that

568
01:00:34.210 --> 01:00:35.660
Inderjit Dhillon: removes

569
01:00:37.440 --> 01:00:38.900
Inderjit Dhillon: or drops

570
01:00:40.040 --> 01:00:41.030
Inderjit Dhillon: the

571
01:00:42.670 --> 01:00:50.480
Inderjit Dhillon: indicator. Vector Constraint. indicator vector constraint.

572
01:00:52.140 --> 01:00:54.250
Inderjit Dhillon: What do I mean? I mean.

573
01:00:54.530 --> 01:00:56.560
Inderjit Dhillon: Take a look at this quantity.

574
01:01:01.240 --> 01:01:02.610
Inderjit Dhillon: this quantity.

575
01:01:05.830 --> 01:01:20.840
Inderjit Dhillon: so i'm trying to minimize y one, y 2, y k c equal to one to k yc transpose L. Y. C. Divided by Yc: transpose Yc.

576
01:01:21.570 --> 01:01:22.460
Inderjit Dhillon: Okay.

577
01:01:22.800 --> 01:01:25.660
Inderjit Dhillon: This is given by

578
01:01:26.310 --> 01:01:29.380
Inderjit Dhillon: is given

579
01:01:29.580 --> 01:01:30.970
Inderjit Dhillon: Bye.

580
01:01:31.020 --> 01:01:35.950
and but you still keep the the following constraint: Sorry that yc transpose

581
01:01:37.140 --> 01:01:39.580
Inderjit Dhillon: Y. I see

582
01:01:40.700 --> 01:01:50.760
Inderjit Dhillon: I Yc. J. Is equal to 0 for all I not equal to that. That means that these vectors are orthogonal to each other.

583
01:01:51.390 --> 01:01:52.910
Inderjit Dhillon: So this problem.

584
01:01:54.140 --> 01:01:55.410
Inderjit Dhillon: the

585
01:01:56.830 --> 01:02:03.920
Inderjit Dhillon: solution to this problem. So note the difference between the the problem over

586
01:02:06.170 --> 01:02:07.430
Inderjit Dhillon: over here.

587
01:02:08.770 --> 01:02:12.500
Inderjit Dhillon: I have this: this implies that this is true.

588
01:02:13.280 --> 01:02:21.090
Inderjit Dhillon: but i'm removing this constraint, and this is one or 0. I've dropped that so these could actually these vectors. Why, it could be real numbers.

589
01:02:21.830 --> 01:02:24.200
Inderjit Dhillon: Okay, so once I do that

590
01:02:24.480 --> 01:02:30.680
Inderjit Dhillon: this solution is given by eigenvectors

591
01:02:32.780 --> 01:02:34.140
Inderjit Dhillon: all

592
01:02:43.910 --> 01:02:45.140
Inderjit Dhillon: okay.

593
01:02:45.310 --> 01:02:49.220
Inderjit Dhillon: eigenvectors corresponding to the smallest. I

594
01:02:49.560 --> 01:02:51.930
Inderjit Dhillon: Okay. So here is the

595
01:02:52.090 --> 01:02:54.110
spectral.

596
01:02:57.970 --> 01:02:59.790
Inderjit Dhillon: It's actually kind of simple.

597
01:03:02.170 --> 01:03:04.310
Inderjit Dhillon: and it says, form

598
01:03:05.420 --> 01:03:07.720
Inderjit Dhillon: the raft of.

599
01:03:12.410 --> 01:03:15.480
Inderjit Dhillon: and that's L is equal to D minus a

600
01:03:16.400 --> 01:03:17.680
Inderjit Dhillon: compute.

601
01:03:24.320 --> 01:03:27.650
Inderjit Dhillon: Okay. smallest

602
01:03:29.740 --> 01:03:31.300
Inderjit Dhillon: eigenvectors

603
01:03:34.190 --> 01:03:35.290
Inderjit Dhillon: of L

604
01:03:38.140 --> 01:03:39.340
Inderjit Dhillon: except

605
01:03:41.160 --> 01:03:42.620
all ones fit.

606
01:03:44.200 --> 01:03:51.270
Inderjit Dhillon: Okay, because all all one vector is really the smallest I can. Vector but it doesn't give you any information about the cluster.

607
01:03:51.710 --> 01:03:55.560
Inderjit Dhillon: So compute the K smallest eigenvectors of L.

608
01:03:57.680 --> 01:04:03.110
Inderjit Dhillon: Okay. And suppose now, so that gives us the solution to this problem

609
01:04:05.180 --> 01:04:13.830
Inderjit Dhillon: that gives a solution to this problem. Okay. call that matrix.

610
01:04:16.030 --> 01:04:17.340
Inderjit Dhillon: Why, Star?

611
01:04:18.440 --> 01:04:22.240
Inderjit Dhillon: Okay, so y star belongs to our

612
01:04:22.970 --> 01:04:28.450
Inderjit Dhillon: and by okay eigenvectors. each of them mentioned. And

613
01:04:29.570 --> 01:04:39.400
Inderjit Dhillon: but the problem is that we still need to go from here to the clustering. These are real numbers. These eigenvectors are real numbers.

614
01:04:39.680 --> 01:04:42.070
Inderjit Dhillon: So how do we go from

615
01:04:44.640 --> 01:04:46.420
Inderjit Dhillon: the eigenvectors

616
01:04:47.990 --> 01:04:50.310
Inderjit Dhillon: to the classroom? So extract

617
01:04:56.640 --> 01:04:57.570
Inderjit Dhillon: well

618
01:05:02.460 --> 01:05:04.380
Inderjit Dhillon: from Why, store.

619
01:05:07.850 --> 01:05:15.720
Inderjit Dhillon: Okay, so what you can do is you can think about that. This is what's happened. You have end by K.

620
01:05:16.320 --> 01:05:20.710
Inderjit Dhillon: This is what I star. You can think of each role

621
01:05:23.350 --> 01:05:25.240
Inderjit Dhillon: of this matrix.

622
01:05:25.710 --> 01:05:28.760
Inderjit Dhillon: You can think of it as embedding

623
01:05:31.460 --> 01:05:34.160
Inderjit Dhillon: all the corresponding

624
01:05:35.580 --> 01:05:36.920
Inderjit Dhillon: vortex.

625
01:05:41.680 --> 01:05:46.520
Inderjit Dhillon: And so one way to extract the clustering is to actually just run kings

626
01:05:49.850 --> 01:05:53.640
Inderjit Dhillon: over the end rules

627
01:05:54.660 --> 01:05:56.080
Inderjit Dhillon: off my store.

628
01:06:01.510 --> 01:06:02.960
Inderjit Dhillon: and that'll give you a close look.

629
01:06:05.170 --> 01:06:09.230
Inderjit Dhillon: So kind of wild, I think right that you we started off

630
01:06:10.570 --> 01:06:12.120
Inderjit Dhillon: with this.

631
01:06:13.240 --> 01:06:14.590
Inderjit Dhillon: You know, graph

632
01:06:15.930 --> 01:06:17.440
Inderjit Dhillon: we talked about

633
01:06:17.620 --> 01:06:18.600
cut.

634
01:06:20.740 --> 01:06:22.530
Inderjit Dhillon: We talked about

635
01:06:22.890 --> 01:06:25.390
Inderjit Dhillon: some weighted graph cuts.

636
01:06:29.110 --> 01:06:30.470
Inderjit Dhillon: and then

637
01:06:31.860 --> 01:06:33.230
Inderjit Dhillon: from that

638
01:06:35.360 --> 01:06:38.590
Inderjit Dhillon: I can actually get an algorithm like this

639
01:06:40.590 --> 01:06:43.000
Inderjit Dhillon: to try and minimize that weighted draft cut

640
01:06:46.010 --> 01:06:50.830
Inderjit Dhillon: except so what I did over here was justify

641
01:06:51.080 --> 01:06:54.550
Inderjit Dhillon: Why, this is an algorithm

642
01:06:54.980 --> 01:06:56.910
Inderjit Dhillon: that can actually be effective.

643
01:06:57.600 --> 01:07:00.350
But the algorithm, actually is, you know, fairly simple

644
01:07:00.590 --> 01:07:03.410
Inderjit Dhillon: and kind of really, while right, I mean you.

645
01:07:03.460 --> 01:07:05.730
Inderjit Dhillon: You have a graph, but

646
01:07:06.340 --> 01:07:12.700
Inderjit Dhillon: you get the what's called the graph Laplace and your computer eigenvectors of this graph

647
01:07:13.400 --> 01:07:17.840
Inderjit Dhillon: essentially get an embedding, and that's how you get. You get the

648
01:07:18.660 --> 01:07:26.620
Inderjit Dhillon: So one thing that I did not say over here is. I do not want to confuse you. Okay.

649
01:07:26.780 --> 01:07:35.100
Inderjit Dhillon: the eigenvectors are, you know, when we talked about the eigenvalue problem, it is L. Of X is equal to Lambda of X. Right?

650
01:07:35.140 --> 01:07:39.920
Inderjit Dhillon: That's the eigenvalue problem. This will do racial cut.

651
01:07:43.660 --> 01:07:48.960
Inderjit Dhillon: but the normalized cut is Lx is equal to Lambda, D. Of X.

652
01:07:49.700 --> 01:07:54.610
Inderjit Dhillon: Okay, this is the normally it's got.

653
01:07:57.510 --> 01:08:02.020
Inderjit Dhillon: And this is also sometimes called the generalized eigenvalue problem.

654
01:08:10.540 --> 01:08:14.150
Inderjit Dhillon: We call over here. There's a matrix over here. D,

655
01:08:15.690 --> 01:08:17.160
Inderjit Dhillon: which is not there before.

656
01:08:18.930 --> 01:08:21.430
Inderjit Dhillon: Okay. So

657
01:08:21.700 --> 01:08:30.420
Inderjit Dhillon: this is like a, you know, very classical kind of algorithm that is used widely for clustering graphs.

658
01:08:30.560 --> 01:08:36.279
Inderjit Dhillon: and can actually be also used for data closely. Right? So suppose I have

659
01:08:36.649 --> 01:08:48.729
Inderjit Dhillon: X, which is, I have X one next to X. M. What you can do is you can do the following. which is E to the power minus

660
01:08:49.649 --> 01:08:55.460
Inderjit Dhillon: Xi minus X. J. Square, divided by, you know, some bit parameter.

661
01:08:56.010 --> 01:09:01.680
Inderjit Dhillon: Okay, I can do this and so

662
01:09:07.260 --> 01:09:11.130
Inderjit Dhillon: and then wine clusters

663
01:09:12.740 --> 01:09:15.450
Inderjit Dhillon: bye Spectral.

664
01:09:21.260 --> 01:09:24.830
Inderjit Dhillon: Well, essentially, you're forming a graph over here.

665
01:09:26.760 --> 01:09:38.029
Inderjit Dhillon: based upon some d-dimensional representation of your data points. So spectral testing can be used for obviously for graph restrict, but can actually also be used for

666
01:09:38.260 --> 01:09:41.700
Inderjit Dhillon: data clustering where the data is not intrinsically

667
01:09:41.770 --> 01:09:48.149
Inderjit Dhillon: given by a graph. But you construct a graph, and then you do a That's true.

668
01:09:52.310 --> 01:09:53.920
Inderjit Dhillon: Okay, questions.

669
01:10:06.460 --> 01:10:15.570
Inderjit Dhillon: And it turns out that, for example, by doing this, algorithm. Remember. you know, data clustering. We also had looked at gaming right?

670
01:10:19.550 --> 01:10:23.390
Inderjit Dhillon: So spectral clustering, I would say, is a better algorithm

671
01:10:24.680 --> 01:10:35.070
Inderjit Dhillon: that k-means, but could actually be more expensive because you have to form this N. By N. Matrix, you have to form this matrix, which itself the construction of it needs n square time.

672
01:10:35.360 --> 01:10:36.950
Inderjit Dhillon: right. But.

673
01:10:37.360 --> 01:10:41.410
Inderjit Dhillon: for example, one property that will have is that

674
01:10:43.130 --> 01:10:45.300
Inderjit Dhillon: if I can select the graph like this.

675
01:10:47.990 --> 01:10:52.920
Inderjit Dhillon: then the spectral classroom will be able to find clusters that are.

676
01:10:54.170 --> 01:10:59.040
Inderjit Dhillon: you know, a a linearly separable algorithm like K-means could not find.

677
01:10:59.400 --> 01:11:07.940
Inderjit Dhillon: So the separators don't have to be linear separator. So, for example, if you look at your homework problem. Your homework problem, of course, is a supervised

678
01:11:08.210 --> 01:11:13.820
Inderjit Dhillon: learning problem, and the way we've given it in the homework. But suppose you were given those X's.

679
01:11:14.160 --> 01:11:20.580
Inderjit Dhillon: and you will say, hey, fine clusters, will you be able to recover the 2 concentric circles?

680
01:11:21.800 --> 01:11:24.240
Inderjit Dhillon: Whenever a K-means will not be able to do it.

681
01:11:26.150 --> 01:11:31.170
Inderjit Dhillon: Colonel K-means will be able to do it, provided you use the right kernel

682
01:11:31.770 --> 01:11:34.550
Inderjit Dhillon: and definitely spectral plus we will be able to do it.

683
01:11:36.730 --> 01:11:41.390
Inderjit Dhillon: Okay, and in some sense this Aij: You can think about it as a particular kernel.

684
01:11:47.900 --> 01:11:50.500
Inderjit Dhillon: Okay, any questions.

685
01:11:58.370 --> 01:12:01.110
Inderjit Dhillon: Okay. So oh.

686
01:12:01.190 --> 01:12:07.420
this will. Actually, I think, you know. Maybe i'll talk a little bit about another part about

687
01:12:07.740 --> 01:12:20.980
Inderjit Dhillon: unsupervised learning which is just unsupervised dimensionality reduction. So some of you have made may have heard of the term principal components analysis, right? So principal components, analysis is a way

688
01:12:21.080 --> 01:12:25.070
Inderjit Dhillon: to reduce the dimensionality of data

689
01:12:26.640 --> 01:12:36.780
Inderjit Dhillon: where no supervision is no supervised labels. Argument, right, and actually also uses eigenvectors of the essentially the sample Covariance matrix.

690
01:12:37.340 --> 01:12:41.700
Inderjit Dhillon: And so maybe i'll talk a little bit about that at some point of time.

691
01:12:42.790 --> 01:12:50.550
Inderjit Dhillon: But now, you know, I've kind of covered this only in 2 lectures. I could have spent many more lectures, but

692
01:12:52.250 --> 01:12:53.320
Inderjit Dhillon: so

693
01:12:53.920 --> 01:12:59.110
Inderjit Dhillon: unsupervised. Learning, you know, sometimes is really a means towards an end.

694
01:12:59.140 --> 01:13:07.620
Inderjit Dhillon: Many times people don't necessarily need to look at the clusters, but the clustering can actually help in downstream tasks in supervision.

695
01:13:07.960 --> 01:13:14.870
Inderjit Dhillon: Right. So, for example, some of the work that research that Meage does, and there's a couple of projects

696
01:13:15.400 --> 01:13:26.720
Inderjit Dhillon: Oops, or at least one project where you have a huge number of classes, and it's the extreme classification problem. You can actually first do a clustering of the the the the labels.

697
01:13:26.750 --> 01:13:35.920
Inderjit Dhillon: and then use it within supervised learning to get a more efficient and better solution. Yeah. So

698
01:13:35.930 --> 01:13:44.870
Inderjit Dhillon: So with this, you know, we kind of conclude the clustering part. So I've talked about data clustering in one lecture graph, listing in another lecture pretty quickly.

699
01:13:44.990 --> 01:13:50.560
Inderjit Dhillon: And now we will kind of.

700
01:13:51.110 --> 01:14:09.870
Inderjit Dhillon: you know, move on in this class to consider it a little bit more on deep learning. Right Deep learning is kind of the the the methods that give you the best results right now. So we will move towards that.

701
01:14:10.050 --> 01:14:25.620
Inderjit Dhillon: and I think many of your projects are also using deep learning. So this will be a good time to kind of transition over. We? We've already talked about deep learning a little bit right, and you're already kind of doing that in homework 3. But we'll pretty much start talking about that

702
01:14:25.620 --> 01:14:32.590
Inderjit Dhillon: almost exclusively now from now to the end of the class, it really some other material that we will have.

703
01:14:32.600 --> 01:14:35.990
But mostly we will talk about deep learning from now on.

704
01:14:36.420 --> 01:14:38.820
Inderjit Dhillon: So any questions.

705
01:14:39.450 --> 01:14:40.890
Chitrank: Yeah, I do a question.

706
01:14:40.950 --> 01:14:44.940
Inderjit Dhillon: Remember that like to submit your products.

707
01:14:46.200 --> 01:14:48.920
Chitrank: Yeah, go ahead. So

708
01:14:48.980 --> 01:14:53.540
Chitrank: so essentially in the spectrum clustering. What you have done is like.

709
01:14:53.790 --> 01:14:57.150
Chitrank: Transform the

710
01:14:57.470 --> 01:15:05.120
Chitrank: the rock costs into the Euclidean distances, on which K. Means question can be done.

711
01:15:06.210 --> 01:15:08.650
Inderjit Dhillon: You know you can think of it that way. Yeah.

712
01:15:09.980 --> 01:15:13.860
Inderjit Dhillon: but it's done using this method like using spectral.

713
01:15:15.160 --> 01:15:17.000
Inderjit Dhillon: So there's no supervision given.

714
01:15:19.450 --> 01:15:22.000
Inderjit Dhillon: I get an embedding, and then I can run came in.

715
01:15:24.910 --> 01:15:32.190
Chitrank: but I was just wondering that. Why can't we use the raw costs themselves?

716
01:15:33.920 --> 01:15:38.540
Inderjit Dhillon: You mean the raw? What I mean one is, if the data is given in terms of a graph.

717
01:15:39.670 --> 01:15:43.980
Inderjit Dhillon: And then there is a second part, which is this part right, which is the data that is given

718
01:15:44.390 --> 01:15:50.270
Inderjit Dhillon: in terms of the vectors, and I instruct a graph. So which one I talking about.

719
01:15:53.420 --> 01:15:54.980
Chitrank: So

720
01:16:01.740 --> 01:16:11.460
Inderjit Dhillon: I mean, if you think about the graph problem, I that's very like, hey? You were given a graph. And now you are getting embeddings from this graph for each vertex.

721
01:16:13.760 --> 01:16:18.810
Chitrank: And yeah, I actually I was talking about the specter. Interesting thing on the but where?

722
01:16:18.890 --> 01:16:30.930
Chitrank: What the costs included, like the cost that you are showing in the very first example on the slide, one on on your page one when I say cost these were the edge rates. You mean, right? Oh, yeah, exactly to

723
01:16:31.620 --> 01:16:38.450
Inderjit Dhillon: yeah. So this is actually the member. Remember that a is here.

724
01:16:38.710 --> 01:16:42.960
Inderjit Dhillon: Yeah, yeah, I got it all right. So

725
01:16:44.070 --> 01:16:46.190
Inderjit Dhillon: form this matrix

726
01:16:46.520 --> 01:16:50.620
Inderjit Dhillon: d minus a computed eigenvectors and get a number.

727
01:16:51.390 --> 01:16:55.800
Chitrank: No, I just wondering that like

728
01:16:56.210 --> 01:17:08.630
Chitrank: like? What if I were to like? I mean like so just use those weights in view of Euclidean distance that we use in the kings?

729
01:17:08.860 --> 01:17:09.790
Chitrank: I've got it.

730
01:17:09.910 --> 01:17:15.500
Chitrank: I mean, like what I'm trying to ask is that you know. Why did we have to like?

731
01:17:16.020 --> 01:17:20.320
Chitrank: I mean, go through this complex transformation

732
01:17:22.820 --> 01:17:23.650
Chitrank: so

733
01:17:24.000 --> 01:17:28.640
Chitrank: to get like. I don't know if you are able to understand what i'm.

734
01:17:28.780 --> 01:17:38.710
Inderjit Dhillon: Well, I mean, I think I I see what you're trying to say, but you know. I think what you are saying is. how can I go from the eigenvectors to a partitioning right?

735
01:17:40.140 --> 01:17:51.270
Inderjit Dhillon: Is that what you're planning on? I mean, I just asking some other thing that like, Why can't we use those costs directly in you of this? What they gave me?

736
01:17:51.680 --> 01:17:54.590
Inderjit Dhillon: This is one algorithm right where I,

737
01:17:55.270 --> 01:17:58.150
Inderjit Dhillon: you use the costs or the edge weights

738
01:17:58.310 --> 01:18:03.380
Inderjit Dhillon: the kernel and learning, and then heuristic. And then there are other heuristics. And then there are other methods.

739
01:18:03.450 --> 01:18:13.900
Chitrank: Obviously I haven't done a full survey. But there are other methods that direct you. So you You meant to say that like this. So what so what is the objective that this particular Gotham reduces finally like

740
01:18:14.320 --> 01:18:19.600
Inderjit Dhillon: that? Yeah. So ratio

741
01:18:20.200 --> 01:18:21.680
Chitrank: Okay.

742
01:18:22.460 --> 01:18:24.320
Chitrank: done. Okay, yeah. I got him.

743
01:18:24.650 --> 01:18:25.400
Inderjit Dhillon: Okay.

744
01:18:28.410 --> 01:18:29.080
bye.

745
01:18:29.820 --> 01:18:33.170
Inderjit Dhillon: Okay. Any more questions.

746
01:18:36.910 --> 01:18:41.830
Inderjit Dhillon: Okay, If not, I'll see you on Monday. Then. Okay, bye, bye.

WEBVTT

1
00:01:17.530 --> 00:01:19.210
Inderjit Dhillon: Hey? Good afternoon.

2
00:01:30.640 --> 00:01:32.530
Inderjit Dhillon: Okay?

3
00:01:32.680 --> 00:01:39.990
Inderjit Dhillon: Any questions for me. We did just send out an announcement

4
00:01:40.520 --> 00:01:48.420
Inderjit Dhillon: extending or making the homework to homework. 3 is due on Friday, March 30 first. Now.

5
00:01:57.800 --> 00:01:59.570
Inderjit Dhillon: any other questions

6
00:02:00.620 --> 00:02:04.720
Inderjit Dhillon: project submissions, I think i'm just waiting on.

7
00:02:04.910 --> 00:02:17.180
Inderjit Dhillon: We're just waiting on a couple of people to submit their projects, and then also they sent out. you know, presentations which will be, you know, 1012min.

8
00:02:17.320 --> 00:02:22.220
Inderjit Dhillon: and I think most many of you have started to choose your presentations. So that's good.

9
00:02:22.430 --> 00:02:25.090
Inderjit Dhillon: But if you haven't please. you know.

10
00:02:25.290 --> 00:02:28.280
Inderjit Dhillon: sign up, and this is going to be over

11
00:02:28.450 --> 00:02:31.200
Inderjit Dhillon: 4 lectures later in April.

12
00:05:01.940 --> 00:05:08.020
Inderjit Dhillon: Hey? Sorry, I was just checking to see if I guess I got kicked off.

13
00:05:09.240 --> 00:05:14.270
Inderjit Dhillon: Maybe I lost connection with an email. Seems like i'm back now. So

14
00:05:24.090 --> 00:05:27.490
Inderjit Dhillon: Okay. so let me

15
00:05:30.370 --> 00:05:32.510
Inderjit Dhillon: let me share my

16
00:05:34.510 --> 00:05:36.500
Inderjit Dhillon: I bed, screams.

17
00:05:38.680 --> 00:05:41.050
Inderjit Dhillon: Oh, is this what's happening.

18
00:05:49.370 --> 00:05:51.020
Niles? Are you on?

19
00:05:52.880 --> 00:05:53.680
Nilesh Gupta: Yes.

20
00:05:54.870 --> 00:05:59.720
Inderjit Dhillon: when I try to share screen it, says host, disabled participants screen sharing.

21
00:06:03.730 --> 00:06:05.090
Inderjit Dhillon: and I

22
00:06:07.150 --> 00:06:09.290
Inderjit Dhillon: I'm: sure.

23
00:06:14.240 --> 00:06:15.600
Inderjit Dhillon: Okay, yeah.

24
00:06:38.300 --> 00:06:40.250
Nilesh Gupta: is it Still, not a long you to?

25
00:07:35.680 --> 00:07:40.820
Inderjit Dhillon: Okay. i'm back. Let's play a game

26
00:08:15.500 --> 00:08:17.670
Nilesh Gupta: for some reason I can only see a black.

27
00:08:18.030 --> 00:08:25.450
Nilesh Gupta: Okay. The screen is being shared. I can't see that. Yeah, it says that

28
00:08:26.940 --> 00:08:29.880
Nilesh Gupta: he started screen sharing, but I can't see anything

29
00:08:30.040 --> 00:08:31.950
Nilesh Gupta: when you see a top background.

30
00:08:32.900 --> 00:08:35.159
Inderjit Dhillon: So you don't see my ipad.

31
00:08:35.970 --> 00:08:36.610
Nilesh Gupta: No.

32
00:08:38.669 --> 00:08:39.960
Inderjit Dhillon: Okay.

33
00:09:08.820 --> 00:09:10.810
Inderjit Dhillon: Okay. I'm. Still trying to fix it.

34
00:10:07.790 --> 00:10:11.010
Inderjit Dhillon: Okay, hopefully. You can see your screen now.

35
00:10:11.250 --> 00:10:12.090
Nilesh Gupta: Yes.

36
00:10:13.930 --> 00:10:14.690
Nilesh Gupta: I guess.

37
00:10:15.780 --> 00:10:24.040
Inderjit Dhillon: Okay. Sorry about the trouble, but I was trying to use a slightly different way of sharing my ipad, but

38
00:10:25.660 --> 00:10:41.560
Inderjit Dhillon: gone back to the older way. Okay, so today. So so you know. Now we are coming kind of. Towards the end of the lecture we will talk a lot about, you know, deep learning, and as you have seen in your

39
00:10:41.680 --> 00:10:48.100
Inderjit Dhillon: presentations, many of the presentations that you'll be making are on aspects of deep learning.

40
00:10:48.340 --> 00:10:54.600
Inderjit Dhillon: And hopefully, this will be, you know, of a great interest to all of you. So remember the course is not

41
00:10:54.650 --> 00:11:05.760
Inderjit Dhillon: a course on deep learning. The course is a general course on machine learning. So in the beginning of the course, we are, you know, till now we've gone and really titled a lot of the foundations.

42
00:11:05.850 --> 00:11:13.910
Inderjit Dhillon: especially the mathematical foundations behind the machine learning. So today, also.

43
00:11:13.940 --> 00:11:27.680
Inderjit Dhillon: today i'll talk about optimization or the stochastic optimization methods that I used in deep learning. We've already mentioned it. We've mentioned stochastic, gradient descent a few times.

44
00:11:27.780 --> 00:11:29.350
Inderjit Dhillon: but

45
00:11:29.880 --> 00:11:40.060
Inderjit Dhillon: so what's coming up in classes? I'll talk about stochastic optimization, and then in subsequent lectures. We'll talk more about our deep learning architectures.

46
00:11:40.300 --> 00:11:53.040
Inderjit Dhillon: you know. Obviously mult their perceptrons. Cnn's are an end. Lsdms. And of course, the architecture that is used a lot. These days are transformers.

47
00:11:53.190 --> 00:11:55.850
Inderjit Dhillon: And then.

48
00:11:56.960 --> 00:12:06.990
Inderjit Dhillon: as you might have seen, and there are various aspects of this that will be done through student presentations. And of course you all are exploring various

49
00:12:07.020 --> 00:12:12.760
Inderjit Dhillon: projects, so many of which touch up on deep learning and their applications.

50
00:12:13.570 --> 00:12:28.690
Inderjit Dhillon: Okay. But we'll talk a little bit today about stochastic optimization which is important, you know, in the non deep learning setting also in general machine learning. And we've already talked about it a little bit, but definitely in the kind of machine learning setting.

51
00:12:29.570 --> 00:12:32.580
Inderjit Dhillon: So today the focus will be

52
00:12:32.700 --> 00:12:34.780
Inderjit Dhillon: stochastic, gradient descent

53
00:12:35.060 --> 00:12:46.100
Inderjit Dhillon: instead of writing out things. So the course we'll also see a transition where i'm not. You know I felt like I've bombarded you with a lot of math in my lecture notes.

54
00:12:47.570 --> 00:12:49.890
Inderjit Dhillon: but i'll try, and

55
00:12:51.440 --> 00:13:00.120
Inderjit Dhillon: you know, not do so much in the next remaining part of the of the class. So today i'll actually use slides

56
00:13:00.380 --> 00:13:03.280
Inderjit Dhillon: and talk about the Std.

57
00:13:07.050 --> 00:13:11.450
Inderjit Dhillon: So we've already talked about this this

58
00:13:12.990 --> 00:13:16.280
Inderjit Dhillon: erm kind of framework which is

59
00:13:16.530 --> 00:13:23.700
Inderjit Dhillon: empirical risk minimization. So capital n is the total number of data point training data points.

60
00:13:24.130 --> 00:13:30.000
Inderjit Dhillon: and then W. Is the parameter that you're trying to optimize.

61
00:13:30.320 --> 00:13:42.370
Inderjit Dhillon: And suppose I have a linear model right? And I look at the loss between the prediction and the label. So we are looking at supervised learning.

62
00:13:43.220 --> 00:13:53.460
Inderjit Dhillon: And we've looked at kind of this formulation right before the context of linear regression in the context of linear classification, where the loss function can be.

63
00:13:53.550 --> 00:14:02.670
Inderjit Dhillon: you know, either squared loss or regression, or a classification oriented loss. For example, in Svm. It is

64
00:14:02.740 --> 00:14:04.480
Inderjit Dhillon: the hinge loss.

65
00:14:04.700 --> 00:14:17.130
Inderjit Dhillon: and then, of course, you can have something which is not necessarily linear. So. The difference between this and this is that over here we could potentially have, like a nonlinear function

66
00:14:17.500 --> 00:14:19.610
Inderjit Dhillon: over here

67
00:14:19.680 --> 00:14:26.930
Inderjit Dhillon: that nonlinear function of W obviously depending upon the training points.

68
00:14:27.120 --> 00:14:32.970
Inderjit Dhillon: So this is kind of the empirical risk minimization, framework. You're given a sample

69
00:14:33.180 --> 00:14:47.160
Inderjit Dhillon: as your training data, and you are trying to optimize this function F, which is an average over the losses, whether it be for a nonlinear model or a linear model.

70
00:14:47.280 --> 00:14:51.880
Inderjit Dhillon: And like I said, you know your loss function could be

71
00:14:52.790 --> 00:14:54.330
Inderjit Dhillon: squared loss.

72
00:14:55.080 --> 00:14:58.480
for now think of the last function as something which is

73
00:14:58.500 --> 00:15:01.700
Inderjit Dhillon: continuous and differentiable.

74
00:15:02.630 --> 00:15:16.500
Inderjit Dhillon: So if the last function is differentiable again, we've mentioned it a couple of times, then one way to optimize the parameters right to solve this kind of minimum problem is to you, use what's called gradient descent.

75
00:15:17.060 --> 00:15:18.360
Inderjit Dhillon: So over here

76
00:15:18.530 --> 00:15:23.300
Inderjit Dhillon: we are going to update our parameters by taking a step

77
00:15:23.720 --> 00:15:28.740
Inderjit Dhillon: in the negative of the gradient. Okay, and that step size is called Ada.

78
00:15:28.940 --> 00:15:32.930
Inderjit Dhillon: and this denotes the gradient of the function. F

79
00:15:33.100 --> 00:15:35.050
Inderjit Dhillon: with respect to W.

80
00:15:35.200 --> 00:15:38.570
Inderjit Dhillon: And so the main competition over here

81
00:15:38.600 --> 00:15:39.740
Inderjit Dhillon: in this

82
00:15:40.950 --> 00:15:46.940
Inderjit Dhillon: is in figuring out what the gradient is now. F is given above.

83
00:15:47.460 --> 00:15:50.930
Inderjit Dhillon: We need to find out the gradient with respect to W.

84
00:15:51.260 --> 00:15:54.960
Inderjit Dhillon: So that depends upon the loss function.

85
00:15:56.190 --> 00:15:56.890
Inderjit Dhillon: Okay.

86
00:15:57.110 --> 00:16:08.330
Inderjit Dhillon: So as you will kind of notice this F, I can break it down into these individual functions which are defined over the corresponding

87
00:16:08.450 --> 00:16:11.160
Inderjit Dhillon: training point. Right so over here

88
00:16:11.250 --> 00:16:19.470
Inderjit Dhillon: F. And W. Just refers to this particular loss over that particular training point only not anything else.

89
00:16:25.470 --> 00:16:30.910
Inderjit Dhillon: So now the problem with traditional gradient descent is that.

90
00:16:31.110 --> 00:16:34.870
Inderjit Dhillon: and could be, you know, millions or even billions.

91
00:16:34.920 --> 00:16:50.840
Inderjit Dhillon: capital. And And so the overall gradient of the function. We'll have a huge sum which will have the number of terms will be the size of the training data, right, which can, like, I said, be very, very big.

92
00:16:51.260 --> 00:17:00.360
Inderjit Dhillon: So each gradient competition of F actually does need to go through all the training samples, and it will be very slow

93
00:17:00.640 --> 00:17:05.940
Inderjit Dhillon: when we have millions and millions of sample. So an actual question, then, is.

94
00:17:06.140 --> 00:17:06.940
Inderjit Dhillon: well.

95
00:17:07.069 --> 00:17:13.810
Inderjit Dhillon: Is there a faster way which is not linear in the size of the training data.

96
00:17:13.880 --> 00:17:17.890
Inderjit Dhillon: faster way to kind of approximate this particular gradient.

97
00:17:19.460 --> 00:17:25.480
Inderjit Dhillon: and that is, then you natural thing to do is think about stochastic sampling.

98
00:17:26.020 --> 00:17:38.670
Inderjit Dhillon: which means that instead of looking at all the data points. all the end data points. I look only at a small subset B right? And B will actually stand for batch.

99
00:17:39.190 --> 00:17:41.380
Inderjit Dhillon: So you look at a small batch.

100
00:17:41.680 --> 00:17:48.060
Inderjit Dhillon: and you just look at the gradient restricted to this particular batch.

101
00:17:48.270 --> 00:17:56.060
Inderjit Dhillon: so you can approximate the gradient by just looking at a subset of the terms that are over here.

102
00:17:56.480 --> 00:18:03.660
Inderjit Dhillon: And so this is now the average gradient over the particular sample, or over the particular batch

103
00:18:03.980 --> 00:18:09.740
Inderjit Dhillon: right, and the size of the batch is given by modulus of the

104
00:18:12.050 --> 00:18:15.860
Inderjit Dhillon: So the stochastic gradient descent algorithm

105
00:18:16.130 --> 00:18:19.570
Inderjit Dhillon: is the following. So you.

106
00:18:20.150 --> 00:18:26.460
Inderjit Dhillon: The training data is given you initialize W many times to just a 0. Vector

107
00:18:26.580 --> 00:18:29.520
Inderjit Dhillon: it's very simple, right? You basically sample a batch.

108
00:18:29.840 --> 00:18:33.470
Inderjit Dhillon: Update the parameters using a particular step size.

109
00:18:33.750 --> 00:18:41.190
Inderjit Dhillon: Note that this is Eta. It's not Ada to the poverty, but this is Ada at the time. Step tick.

110
00:18:42.060 --> 00:18:44.800
Inderjit Dhillon: Okay, and you just keep on repeating it.

111
00:18:47.620 --> 00:18:55.520
Inderjit Dhillon: Now, there's a little bit of a misnomer over here when you look at classical gradient descent.

112
00:18:57.900 --> 00:19:09.430
Inderjit Dhillon: Okay, when you look at classical, gradient descent. Let me see where. So if I look at this classical, gradient descent. you can actually show that this is a descent to method.

113
00:19:09.440 --> 00:19:21.230
Inderjit Dhillon: Okay, so the descent part is that there is a guarantee that it every after taking every step. the objective will actually decrease.

114
00:19:22.140 --> 00:19:22.910
Inderjit Dhillon: Okay.

115
00:19:23.330 --> 00:19:28.070
Inderjit Dhillon: when you start taking an approximate gradient that guarantee actually goes away.

116
00:19:28.670 --> 00:19:32.140
Inderjit Dhillon: So I know some people who kind of our

117
00:19:32.160 --> 00:19:43.950
Inderjit Dhillon: against this particular name descent, because it's really not a monotonically decreasing method where the loss function monotonically decreases. Unlike gradient descent.

118
00:19:44.040 --> 00:19:48.360
Inderjit Dhillon: So some some people will say we should call it stochastic, gradient method.

119
00:19:48.540 --> 00:19:52.210
Inderjit Dhillon: But of course the name has stuck. Okay, but remember

120
00:19:52.240 --> 00:19:55.290
Inderjit Dhillon: that the stochastic, gradient descent method

121
00:19:55.510 --> 00:19:58.170
Inderjit Dhillon: does not actually guarantee that

122
00:19:58.400 --> 00:20:01.440
Inderjit Dhillon: the f value

123
00:20:01.680 --> 00:20:06.570
Inderjit Dhillon: we'll actually keep on decreasing from one step to the other.

124
00:20:06.650 --> 00:20:17.950
Inderjit Dhillon: But eventually you know, the the results are that with in the proper setting that the loss function will go down.

125
00:20:18.640 --> 00:20:20.710
Inderjit Dhillon: Now, you might wonder what

126
00:20:22.360 --> 00:20:23.890
Inderjit Dhillon: B is.

127
00:20:26.250 --> 00:20:32.110
Inderjit Dhillon: and in the extreme case the size of B is just one. So you actually only take

128
00:20:32.140 --> 00:20:33.740
Inderjit Dhillon: one training. Exam.

129
00:20:36.570 --> 00:20:42.310
Inderjit Dhillon: So this is not even a summation, but it is just one example. So that's the extreme. You can take

130
00:20:42.750 --> 00:20:50.890
Inderjit Dhillon: one end, and of course, on the other end the size of B will be capital M, which is the entire data.

131
00:20:51.220 --> 00:20:55.740
Inderjit Dhillon: And practically what ends up happening is that we use something in between.

132
00:20:59.040 --> 00:21:07.880
Inderjit Dhillon: So it's not just deep learning that can be done by Std. You know you can look at logistic regression. You can look at the

133
00:21:08.030 --> 00:21:23.070
Inderjit Dhillon: log loss. Remember that we've kind of covered it in class. We took the gradient of this. and if I wanted to look at Std for logistic regression, then again.

134
00:21:23.550 --> 00:21:26.910
Inderjit Dhillon: pretty much identical to what we have over here

135
00:21:27.960 --> 00:21:34.390
Inderjit Dhillon: all right. We just need to figure out what this is. Okay. And in this case, if you look at it.

136
00:21:36.910 --> 00:21:39.880
Inderjit Dhillon: then my

137
00:21:40.210 --> 00:21:44.420
Inderjit Dhillon: gradient of this function, if I look at the gradient

138
00:21:44.860 --> 00:21:46.890
Inderjit Dhillon: of Fnw.

139
00:21:47.720 --> 00:21:57.700
Inderjit Dhillon: Right? That'll be. I have a log. It'll be one divided by one plus e to the minus y and w transpose

140
00:21:58.570 --> 00:22:04.960
Inderjit Dhillon: X and and then i'll have minus Y. M. Times accent.

141
00:22:07.060 --> 00:22:09.580
Inderjit Dhillon: Okay. And this is what you get over here.

142
00:22:12.470 --> 00:22:17.460
Inderjit Dhillon: This is negative of the gradient. It's just step size.

143
00:22:19.400 --> 00:22:25.180
Inderjit Dhillon: negative of the gradient step size. And this is the gradient. Okay from here.

144
00:22:26.930 --> 00:22:32.050
Inderjit Dhillon: So that's what Std. For logistic regression would look like.

145
00:22:33.700 --> 00:22:37.080
Inderjit Dhillon: Now, Why does the Std work?

146
00:22:37.170 --> 00:22:40.950
Inderjit Dhillon: Okay. So one can show

147
00:22:41.530 --> 00:22:47.020
Inderjit Dhillon: that the expectation of the estimator. So this is the gradient estimator.

148
00:22:47.640 --> 00:22:51.180
Inderjit Dhillon: Right is the actual gradient itself

149
00:22:52.410 --> 00:22:56.450
Inderjit Dhillon: over here, so which means that this estimator

150
00:22:56.610 --> 00:22:58.070
Inderjit Dhillon: is not biased.

151
00:22:59.390 --> 00:23:02.650
Inderjit Dhillon: So you can

152
00:23:02.720 --> 00:23:07.190
Inderjit Dhillon: think of each step as

153
00:23:07.680 --> 00:23:13.450
Inderjit Dhillon: that you are iterating by the gradient, plus a 0 mean noise.

154
00:23:13.680 --> 00:23:16.790
Inderjit Dhillon: And that is what the estimator gives you.

155
00:23:17.070 --> 00:23:19.280
Inderjit Dhillon: Okay, and as a result.

156
00:23:19.530 --> 00:23:26.250
Inderjit Dhillon: this stochastic, gradient method you can start seeing that it actually has some reason to to work.

157
00:23:27.310 --> 00:23:28.370
Inderjit Dhillon: Now.

158
00:23:28.520 --> 00:23:34.410
Inderjit Dhillon: one of the very important prop aspects of Std is

159
00:23:34.500 --> 00:23:44.660
Inderjit Dhillon: what should be the step size that is chosen. Okay? And one of the questions you can ask is. can you actually use a fixed step size? Remember, I had

160
00:23:44.730 --> 00:23:47.270
Inderjit Dhillon: a T where

161
00:23:47.640 --> 00:23:51.640
Inderjit Dhillon: the step size varies with

162
00:23:54.210 --> 00:23:55.930
Inderjit Dhillon: with the

163
00:23:57.830 --> 00:23:58.720
Inderjit Dhillon: step.

164
00:23:58.790 --> 00:24:02.440
Inderjit Dhillon: step, size, do you? The steps are a step number

165
00:24:02.710 --> 00:24:05.140
Inderjit Dhillon: iteration number, and the question is.

166
00:24:05.250 --> 00:24:13.080
Inderjit Dhillon: how should this vary? And one question to ask is, Can I just keep it constant. Can I just keep it to be some value 8.

167
00:24:13.720 --> 00:24:18.290
Inderjit Dhillon: Okay. And what we will say realizes that. Actually that doesn't work.

168
00:24:19.850 --> 00:24:25.080
Inderjit Dhillon: Okay? And there's a pretty simple kind of reason why it cannot work.

169
00:24:26.160 --> 00:24:37.950
Inderjit Dhillon: Suppose W. Star kind of is an optimizer, is the minimizer right? So suppose among all W's. W. Star

170
00:24:37.970 --> 00:24:45.640
Inderjit Dhillon: is argument of F. Ofw. Well, in that case the gradient at this optimizer.

171
00:24:45.990 --> 00:24:50.750
Inderjit Dhillon: right at this point should actually be equal to 0.

172
00:24:52.870 --> 00:25:04.260
Inderjit Dhillon: Right? So suppose it is converging to a suppose there is a local minimum or a global minimum, right? So, for example, in logistic regression, there is a global minimum, because the function that you are minimizing is complex.

173
00:25:04.790 --> 00:25:08.410
Inderjit Dhillon: right in that case the gradient equal to 0.

174
00:25:09.720 --> 00:25:10.840
Inderjit Dhillon: But

175
00:25:12.200 --> 00:25:14.510
Inderjit Dhillon: it's only the gradient that is 0,

176
00:25:14.610 --> 00:25:16.570
Inderjit Dhillon: right? The

177
00:25:17.590 --> 00:25:25.560
Inderjit Dhillon: estimator. which is just looking at a subset of the points. will actually not be 0 if B is a substant.

178
00:25:26.350 --> 00:25:32.880
Inderjit Dhillon: So even if we are at the minimizer, even if we were at the minimizer, right. W. Star.

179
00:25:33.180 --> 00:25:40.210
Inderjit Dhillon: And when we take a step step an Std step. which is this.

180
00:25:42.030 --> 00:25:52.250
Inderjit Dhillon: Okay, this is for logistic regression, we'd actually end up moving away from it because this is not going to be 0, whereas when you're doing gradient descent.

181
00:25:55.580 --> 00:26:07.690
Inderjit Dhillon: what happens is at the optimizer. at the global minimum or at the look, even a local minimum. This is going to be 0. So there would actually be no update done. Okay.

182
00:26:07.840 --> 00:26:09.490
Inderjit Dhillon: So as a result.

183
00:26:10.110 --> 00:26:14.350
Inderjit Dhillon: you can't actually have a constant step size in Std.

184
00:26:25.320 --> 00:26:30.740
Inderjit Dhillon: Okay. So to make a Std. Converge.

185
00:26:31.060 --> 00:26:34.940
Inderjit Dhillon: the step size should actually decrease to 0.

186
00:26:34.970 --> 00:26:49.310
Inderjit Dhillon: Remember, there are 2 parts to the actual step. One of them is Ada, and the other is the gradient estimator. Like we said that the gradient estimator may not go to 0. So then the other term should go to 0, which means that

187
00:26:49.340 --> 00:26:54.610
Inderjit Dhillon: Ada at iteration t S. T. Becomes larger

188
00:26:54.850 --> 00:27:01.490
Inderjit Dhillon: right it should go to infinitely so as t goes to infinity.

189
00:27:01.680 --> 00:27:04.860
Inderjit Dhillon: then it should go to 0.

190
00:27:07.390 --> 00:27:14.540
Inderjit Dhillon: So sometimes people will have. Now you know what rate should it decline at sometimes people will do, let's say, a polynomial rate.

191
00:27:15.030 --> 00:27:18.460
Inderjit Dhillon: which means that you know eta t

192
00:27:18.640 --> 00:27:23.480
Inderjit Dhillon: is t to the power minus a with some constant a.

193
00:27:23.830 --> 00:27:28.470
Inderjit Dhillon: So in the beginning it might be G to the minus one.

194
00:27:29.480 --> 00:27:33.410
Inderjit Dhillon: Or maybe that's too yeah, minus

195
00:27:33.450 --> 00:27:40.250
Inderjit Dhillon: or sorry one to the power minus a 2 to one, minus a 3, to one, minus a 4 per minus a, and so on.

196
00:27:40.570 --> 00:27:45.170
Inderjit Dhillon: so it will decrease as your increasingly.

197
00:27:45.700 --> 00:27:53.770
Inderjit Dhillon: There could be other ways. You can do it. You don't have to go to something like this, but you can try to do step Dk: which means that

198
00:27:53.900 --> 00:27:58.830
Inderjit Dhillon: for many epochs. You keep the learning rate to be the same.

199
00:27:59.270 --> 00:28:02.570
Inderjit Dhillon: and then you make a step change.

200
00:28:03.270 --> 00:28:05.720
Inderjit Dhillon: So I think, in this case

201
00:28:06.590 --> 00:28:14.500
Inderjit Dhillon: you know it's not quite every 25 epochs. I think the changes may be made. It's being made. Every, I think it's around 20 epochs

202
00:28:14.740 --> 00:28:22.840
Inderjit Dhillon: Again, You can see that the learning rate starts at about 14 minus one. and by iteration 200.

203
00:28:23.070 --> 00:28:27.190
Inderjit Dhillon: It is closer to tenth or minus 4,

204
00:28:31.910 --> 00:28:43.760
Inderjit Dhillon: and then people can try other learning rates. so there's no reason why the learning rate has to monotonically kind of decrease. and then people can try something which is cyclic

205
00:28:44.760 --> 00:28:57.220
Inderjit Dhillon: right? And then, instead of going all the way to the top you could go of me. and then halfway, and so on. and then.

206
00:28:58.350 --> 00:29:00.140
So

207
00:29:00.200 --> 00:29:01.640
Inderjit Dhillon: So when you

208
00:29:02.710 --> 00:29:13.760
Inderjit Dhillon: start experimenting with the optimizer. You'll realize that these learning grade schedules can actually be very important for getting your Sgd method to convert, especially

209
00:29:13.800 --> 00:29:18.550
Inderjit Dhillon: if you are dealing with a deep neural network architectures.

210
00:29:21.450 --> 00:29:25.260
Inderjit Dhillon: So we've talked about stochastic, gradient descent.

211
00:29:27.010 --> 00:29:32.660
Inderjit Dhillon: The clear advantages are that there's a cheaper competition, per iteration.

212
00:29:34.040 --> 00:29:49.430
Inderjit Dhillon: and it ends up, seeing what we end up. Seeing is that there is typically faster convergence in the beginning. You're not, you know you're not taking as many. You don't have to spend so much time in each it rate right. Remember that the gradient

213
00:29:49.540 --> 00:29:53.690
Inderjit Dhillon: is actually an average over all the training points.

214
00:29:54.400 --> 00:30:00.600
Inderjit Dhillon: Right so over here you'll see typically that there is faster convergence in the beginning.

215
00:30:01.210 --> 00:30:07.400
Inderjit Dhillon: whereas in gradient descent the convergence is slower.

216
00:30:07.780 --> 00:30:14.940
Inderjit Dhillon: Okay. and obviously, like we said, there there is cheaper competition, per iteration. The cons is.

217
00:30:15.500 --> 00:30:21.790
Inderjit Dhillon: and you'll see over here right? So this is the trajectory. So this is a what's called a level set.

218
00:30:23.080 --> 00:30:33.780
Inderjit Dhillon: Okay. So if you remember the definition of a level set. This is some curve. some surface which is

219
00:30:33.940 --> 00:30:36.460
Inderjit Dhillon: F of X equals Alpha.

220
00:30:37.770 --> 00:30:50.460
Inderjit Dhillon: Okay. Well, this is some surface which is F of x equals. So let me call this Alpha one. This is Alpha 2. This is F. Of X is equal to

221
00:30:50.660 --> 00:31:03.570
Inderjit Dhillon: I for 3, and here Alpha one is bigger than Alpha 2 is bigger than Alpha 3. So this is the minimizer over here.

222
00:31:05.070 --> 00:31:18.210
Inderjit Dhillon: Okay. So this is the minimizer. right? And so you can think of it as like a valley over here. right? And this is the

223
00:31:18.870 --> 00:31:34.680
Inderjit Dhillon: level set, which means that all all these points that i'm drawing that i'm tracing out the function is constant. and if I move inside again, the function is constant at the point that i'm showing, except it's at a lower value.

224
00:31:35.440 --> 00:31:38.610
Inderjit Dhillon: So the it more inside you get

225
00:31:38.970 --> 00:31:42.420
Inderjit Dhillon: the that corresponds to

226
00:31:42.590 --> 00:31:54.030
Inderjit Dhillon: the function having a lower value. Okay. And so what we want to do is we want to be able to start at any point and using the particular method, be able to convert over here.

227
00:31:55.020 --> 00:31:57.850
Inderjit Dhillon: Okay. So for this particular case.

228
00:31:58.420 --> 00:32:03.980
Inderjit Dhillon: you can see that gradient descent this is, let me see. Just make sure that this curve matches up.

229
00:32:04.500 --> 00:32:07.640
Inderjit Dhillon: See what this is. This one is

230
00:32:07.860 --> 00:32:12.780
Inderjit Dhillon: batch, gradient descent. So you can see that this curve is actually pretty smooth, right

231
00:32:13.030 --> 00:32:17.870
Inderjit Dhillon: the way it's moving over here. whereas if I do

232
00:32:19.460 --> 00:32:21.450
Inderjit Dhillon: stochastic, gradient descent.

233
00:32:21.760 --> 00:32:23.800
Inderjit Dhillon: I think this is for

234
00:32:24.380 --> 00:32:28.010
Inderjit Dhillon: where you are basically looking at a bad size of only one.

235
00:32:30.650 --> 00:32:39.680
Inderjit Dhillon: But as if I look at kind of mini-batch gradient, descent. then the curve is slightly better, but still not fully smooth.

236
00:32:40.120 --> 00:32:52.240
Inderjit Dhillon: So the cons is that it's a little bit less table. and then, as it gets closer to convergence, the to to the minimizer. The convergence can actually be somewhat slow.

237
00:32:53.700 --> 00:33:05.870
Inderjit Dhillon: Okay. But as the gradient descent you can see, and there are other ways to accelerate gradient descent By the way, there are kind of Newton methods where you don't have to just take the gradient descent step.

238
00:33:05.980 --> 00:33:09.700
Inderjit Dhillon: but you can actually precondition it by.

239
00:33:11.720 --> 00:33:15.080
Inderjit Dhillon: You know in particular, the inverse of the Hisian.

240
00:33:15.290 --> 00:33:26.350
Inderjit Dhillon: So you can essentially get Newton's method that can really accelerate convergence. Of course those methods are very expensive to apply especially for deep learning

241
00:33:27.790 --> 00:33:40.030
Inderjit Dhillon: any questions about this figure or so, or and then, you know, like we've already talked a little bit about this, and it can be quite hard to sometimes tune the step size for Spd

242
00:33:45.190 --> 00:33:46.680
Inderjit Dhillon: any questions so far.

243
00:33:51.640 --> 00:33:59.520
Inderjit Dhillon: Okay. So that is kind of like a high level overview of stochastic really understand the pros and cons.

244
00:34:01.050 --> 00:34:04.690
Inderjit Dhillon: Now one of the things that you might ask is.

245
00:34:08.110 --> 00:34:13.670
Inderjit Dhillon: i'm especially think of the case where you have

246
00:34:14.610 --> 00:34:16.980
Inderjit Dhillon: just 1 point in each Mini bench.

247
00:34:18.260 --> 00:34:22.650
Inderjit Dhillon: but you're actually only evaluating the gradient over that point

248
00:34:23.810 --> 00:34:31.510
Inderjit Dhillon: right? And then at the next step you look at the gradient over another point, and on the next step you look at the gradient over another point.

249
00:34:31.920 --> 00:34:41.690
Inderjit Dhillon: But if these were all kind of estimators of the overall gradient. you could ask, Why can't I use the actual previous value?

250
00:34:42.909 --> 00:34:46.400
Inderjit Dhillon: Why can't I use the previous value to somehow

251
00:34:46.690 --> 00:34:50.780
Inderjit Dhillon: make the gradient to be a better estimator?

252
00:34:52.530 --> 00:35:02.720
Inderjit Dhillon: Okay, maybe there's going to be a lot of variance when I suddenly change the estimate estimator to be over one batch of points

253
00:35:02.850 --> 00:35:10.790
Inderjit Dhillon: to be over a new badge of points. Okay. So that is captured by kind of the notion of momentum.

254
00:35:12.920 --> 00:35:16.610
Inderjit Dhillon: And this is valid even for gradient descent. Right?

255
00:35:17.020 --> 00:35:18.900
Inderjit Dhillon: So

256
00:35:19.920 --> 00:35:27.800
Inderjit Dhillon: gradient descent is only using the current gradient. What momentum does is it will actually use previous gradient information.

257
00:35:29.520 --> 00:35:42.700
Inderjit Dhillon: So let's look at the momentum update rule. Okay. So now let me just kind of write it down so earlier we were saying that Wt. Plus one is equal to Wt.

258
00:35:42.790 --> 00:35:47.890
Inderjit Dhillon: Minus. I think the notation we were using was a t

259
00:35:49.000 --> 00:35:52.730
Inderjit Dhillon: right times gradient of F of

260
00:35:52.850 --> 00:35:54.510
Inderjit Dhillon: Wt.

261
00:35:56.810 --> 00:36:08.800
Inderjit Dhillon: So ignore this part, for now the step size right. We've just kind of kept it constant for now. But look at this part. This is now being replaced with this.

262
00:36:10.040 --> 00:36:20.530
Inderjit Dhillon: And what is this? Well. first look at the case where Beta, equal to 0. So when Beta equal to 0, this will be.

263
00:36:22.460 --> 00:36:32.110
Inderjit Dhillon: and Beta equals 0. This will not be there. This will be one. and this will be gradient of F: Oh, so when beta equal to 0.

264
00:36:33.470 --> 00:36:34.320
Inderjit Dhillon: Okay.

265
00:36:34.880 --> 00:36:37.850
Inderjit Dhillon: this is the case where Beta equals 0.

266
00:36:41.880 --> 00:36:45.150
Inderjit Dhillon: Okay. And just to make it clear, i'll change this to.

267
00:36:48.450 --> 00:36:53.330
Inderjit Dhillon: So when beta equal to 0, it just becomes as you d.

268
00:36:55.060 --> 00:37:03.340
Inderjit Dhillon: But when beta is slightly different. slightly smaller. then one. Then you can see

269
00:37:03.360 --> 00:37:10.330
Inderjit Dhillon: Well, what is it doing, then? Right? So let's kind of expand out what vt is.

270
00:37:13.100 --> 00:37:16.140
Inderjit Dhillon: Vt is? I'm. Just copying

271
00:37:17.270 --> 00:37:18.890
Inderjit Dhillon: this over here.

272
00:37:20.510 --> 00:37:26.860
Inderjit Dhillon: Vt. Is one minus beta times gradient of F and Wt.

273
00:37:27.600 --> 00:37:30.810
Inderjit Dhillon: plus beta Vt: minus one.

274
00:37:32.630 --> 00:37:44.810
Inderjit Dhillon: Okay. So this is actually a recursion, right? It's expressing Vt. In terms of Vt. Minus one. Okay. so vt minus one is going to be

275
00:37:45.070 --> 00:37:47.010
Inderjit Dhillon: one minus beta

276
00:37:47.780 --> 00:37:52.000
Inderjit Dhillon: gradient of F and W. T. Minus one

277
00:37:52.780 --> 00:37:54.220
Inderjit Dhillon: plus beta

278
00:37:54.260 --> 00:37:56.410
Inderjit Dhillon: vt minus 2.

279
00:37:58.000 --> 00:38:01.580
Inderjit Dhillon: So if I substitute this Vt. Minus one

280
00:38:02.610 --> 00:38:03.940
Inderjit Dhillon: over here.

281
00:38:05.660 --> 00:38:07.350
Inderjit Dhillon: Then I would get

282
00:38:08.410 --> 00:38:10.260
Inderjit Dhillon: one minus beta

283
00:38:11.300 --> 00:38:16.530
Inderjit Dhillon: gradient of F of Wt. Plus beta.

284
00:38:16.930 --> 00:38:19.110
Inderjit Dhillon: and let me substitute that I get

285
00:38:19.650 --> 00:38:23.400
Inderjit Dhillon: one minus beta times gradient of F

286
00:38:24.210 --> 00:38:27.500
Inderjit Dhillon: Wt. Minus one lost

287
00:38:28.040 --> 00:38:30.560
Inderjit Dhillon: Beta Vt. Minus 2.

288
00:38:34.170 --> 00:38:37.940
Inderjit Dhillon: Okay. So let me kind of keep this separate

289
00:38:39.250 --> 00:38:47.370
Inderjit Dhillon: and then expand this. I get one minus beta times gradient of F of Wt.

290
00:38:48.670 --> 00:38:50.340
Inderjit Dhillon: That's just this part.

291
00:38:51.210 --> 00:38:57.170
Inderjit Dhillon: And then I need to take this so it will become beta times one to minus beta

292
00:38:57.440 --> 00:39:06.660
Inderjit Dhillon: times the gradient of F at the previous interest Wt minus one. So i'll get

293
00:39:07.350 --> 00:39:08.990
Inderjit Dhillon: thus beta

294
00:39:09.110 --> 00:39:20.140
Inderjit Dhillon: times one minus beta gradient of f at wt minus one. plus. I get beta square. We

295
00:39:20.790 --> 00:39:23.840
Inderjit Dhillon: Oh, sorry. I know that might happen.

296
00:39:24.190 --> 00:39:27.810
Inderjit Dhillon: Plus beta square. Let me just write it a new line

297
00:39:30.340 --> 00:39:33.860
Inderjit Dhillon: plus beta square times. B T. Minus 2.

298
00:39:35.280 --> 00:39:39.090
Inderjit Dhillon: Okay, and then I can do again right. Vt: minus 2

299
00:39:39.600 --> 00:39:45.740
Inderjit Dhillon: is one minus beta times gradient of F at W. T minus 2

300
00:39:47.530 --> 00:39:50.710
Inderjit Dhillon: plus beta vt. Minus 3.

301
00:39:51.890 --> 00:40:01.000
Inderjit Dhillon: So if I substitute that, let me just do one more step. Okay, I have one minus beta

302
00:40:01.400 --> 00:40:13.120
Inderjit Dhillon: times gradient of F. W. T. Plus. Let me write the one minus beta part first times beta gradient of F

303
00:40:14.000 --> 00:40:16.150
Inderjit Dhillon: Wt. Minus one.

304
00:40:18.580 --> 00:40:21.800
Inderjit Dhillon: and then I have plus beta square

305
00:40:22.190 --> 00:40:24.750
Inderjit Dhillon: Vt. Minus 2, which is from here.

306
00:40:26.240 --> 00:40:27.560
Inderjit Dhillon: It is.

307
00:40:28.430 --> 00:40:32.020
Inderjit Dhillon: Let me see beta square

308
00:40:32.330 --> 00:40:40.280
Inderjit Dhillon: times, one minus beta times gradient of f Wt. Minus 2

309
00:40:41.090 --> 00:40:44.580
Inderjit Dhillon: plus beta vt. Minus 3.

310
00:40:45.190 --> 00:40:56.150
Inderjit Dhillon: So what is the pattern that you see? Right? You see that you have one minus beta gradient of plus one minus beta

311
00:40:57.860 --> 00:41:02.570
Inderjit Dhillon: times beta gradient of F. W. T. Minus one

312
00:41:03.200 --> 00:41:06.200
Inderjit Dhillon: plus one minus beta

313
00:41:06.590 --> 00:41:16.150
Inderjit Dhillon: beta square Times, the gradient of Fw. T. Minus 2, also on Okay, and actually, that's what's written over here.

314
00:41:19.680 --> 00:41:26.100
Inderjit Dhillon: If you notice this, and if you compare with what I've written. then you'll see that

315
00:41:26.300 --> 00:41:30.220
Inderjit Dhillon: you have one minus beta gradient of f one minus

316
00:41:30.290 --> 00:41:36.960
Inderjit Dhillon: greater than the F plus one minus beta times beta gradient at Wt. Minus one

317
00:41:38.120 --> 00:41:41.610
Inderjit Dhillon: one minus beta plus beta square

318
00:41:42.080 --> 00:41:44.730
Inderjit Dhillon: gradient of F at Wt. Minus 2.

319
00:41:45.870 --> 00:41:46.650
Inderjit Dhillon: Okay.

320
00:41:46.940 --> 00:41:50.510
Inderjit Dhillon: So I can actually write this.

321
00:41:52.140 --> 00:41:53.490
Inderjit Dhillon: As

322
00:41:55.550 --> 00:42:00.140
Inderjit Dhillon: you know, I can take one minus beta out common

323
00:42:00.900 --> 00:42:03.150
Inderjit Dhillon: right, and this becomes

324
00:42:03.160 --> 00:42:04.810
Inderjit Dhillon: gradient of F.

325
00:42:05.820 --> 00:42:10.310
Inderjit Dhillon: Times W. T minus one plus beta

326
00:42:10.690 --> 00:42:19.340
Inderjit Dhillon: times gradient of F. W. T. Minus 2 plus beta square gradient of Fw. T minus 3

327
00:42:19.590 --> 00:42:27.120
Inderjit Dhillon: plus beta 4 gradient beta, 3 sorry range of Fw. T. Minus 4, and so on.

328
00:42:30.750 --> 00:42:34.150
Inderjit Dhillon: Okay, so that is what's called

329
00:42:34.850 --> 00:42:46.450
Inderjit Dhillon: moving average of the gradient. So this is a moving average where the coefficient is Beta. So think of Beta as well. If it is something which is

330
00:42:46.600 --> 00:42:50.150
Inderjit Dhillon: oh. let's say, close to 0.

331
00:42:52.130 --> 00:42:54.130
Inderjit Dhillon: Okay, then it can

332
00:42:54.220 --> 00:42:56.180
Inderjit Dhillon: goes to 0 geometrically

333
00:42:57.240 --> 00:43:00.470
Inderjit Dhillon: right. Beta Beta square beta cube.

334
00:43:01.090 --> 00:43:04.990
Inderjit Dhillon: But if Beta is large.

335
00:43:06.190 --> 00:43:07.100
Inderjit Dhillon: Okay.

336
00:43:07.220 --> 00:43:12.090
Inderjit Dhillon: So so at the extreme end it could be that Beta is nearly equal to one.

337
00:43:12.310 --> 00:43:19.340
Inderjit Dhillon: which means that you are taking a lot of the previous things. If Beta is clearly if beta is close to one.

338
00:43:19.640 --> 00:43:31.420
Inderjit Dhillon: then you'll see that this will decrease very slowly. So that means you're basically taking the previous gradients giving a lot of importance to them and using them in the moment.

339
00:43:32.630 --> 00:43:35.620
Inderjit Dhillon: Okay, so this is the kind of

340
00:43:35.830 --> 00:43:37.760
Inderjit Dhillon: momentum, gradient descent

341
00:43:39.500 --> 00:43:41.950
Inderjit Dhillon: which is that I

342
00:43:43.590 --> 00:43:47.500
Inderjit Dhillon: again same kind of things as above. And now the

343
00:43:48.290 --> 00:43:55.180
Inderjit Dhillon: update is Vt. Where vt includes gradient.

344
00:43:56.250 --> 00:43:59.870
Inderjit Dhillon: What is the moving average of the previous.

345
00:44:00.290 --> 00:44:03.480
Inderjit Dhillon: as the gradients at the previous steps?

346
00:44:04.170 --> 00:44:15.080
Inderjit Dhillon: Okay, and sometimes this term beta. it's called the discount fact. And obviously we've seen that if Beta equal to 0, so beta equal to 0 means that

347
00:44:15.540 --> 00:44:18.990
Inderjit Dhillon: you don't actually take any moment. This kind of thing goes away.

348
00:44:24.240 --> 00:44:26.190
Inderjit Dhillon: Okay, so any question on that.

349
00:44:30.940 --> 00:44:33.770
Inderjit Dhillon: And I don't know how many of you have trained.

350
00:44:33.860 --> 00:44:38.300
Inderjit Dhillon: You know. You've trained neural networks. You're training them actually, for the

351
00:44:38.420 --> 00:44:40.290
Inderjit Dhillon: some of the homework problems.

352
00:44:41.730 --> 00:44:54.810
Inderjit Dhillon: and you'll see that you have a choice of the optimizer, and we'll come to one of the optimizers which called Adam optimizer, and you know, sometimes you choose it, and that atom optimizer does have

353
00:44:55.010 --> 00:45:01.860
Inderjit Dhillon: kind of momentum in it. So right now we are talking about things that are definitely, very much kind of used

354
00:45:01.970 --> 00:45:05.020
Inderjit Dhillon: in the way current.

355
00:45:05.160 --> 00:45:07.500
Inderjit Dhillon: Deep learning methods are trained.

356
00:45:09.970 --> 00:45:10.740
Inderjit Dhillon: Okay.

357
00:45:13.300 --> 00:45:18.210
Inderjit Dhillon: Now, so far, we've kind of you know, you could actually have done this even with gradient descent.

358
00:45:19.290 --> 00:45:21.120
Inderjit Dhillon: And actually, that's what these

359
00:45:21.260 --> 00:45:32.790
Inderjit Dhillon: this refers to his momentum. Gradient descent is just saying that i'm going to take gradient and not just the current point. But i'll also look at it at previous points, right. But now

360
00:45:33.030 --> 00:45:45.280
Inderjit Dhillon: I need to adopt it to kind of the Std framework. So remember. the F is an average of the losses overall the plans.

361
00:45:45.640 --> 00:45:57.800
Inderjit Dhillon: and you can basically do the same thing, which is, as in Std. The sample, and I, which would be sample a single point. right? And this is where the gradient comes from.

362
00:46:01.430 --> 00:46:06.500
Inderjit Dhillon: Okay, and like, I said, we'll look at the details of a particular method

363
00:46:06.680 --> 00:46:07.690
Inderjit Dhillon: soon.

364
00:46:07.940 --> 00:46:11.000
Inderjit Dhillon: and see how it is.

365
00:46:13.420 --> 00:46:17.990
Inderjit Dhillon: You know what is the like? What is the atom method that is used on

366
00:46:18.140 --> 00:46:24.420
Inderjit Dhillon: all Martin? Deep learning methods. There's also another

367
00:46:24.430 --> 00:46:28.650
Inderjit Dhillon: quantity or another way of doing things which is called

368
00:46:29.350 --> 00:46:42.780
Inderjit Dhillon: an accelerated gradient method. Okay. And it has a name nested off. Who's very well known Optimization Guru. So in nest of accelerated gradient

369
00:46:44.850 --> 00:46:48.040
Inderjit Dhillon: note over here that this is slightly different.

370
00:46:49.300 --> 00:46:50.200
Inderjit Dhillon: Okay?

371
00:46:51.280 --> 00:46:58.100
Inderjit Dhillon: Well, maybe it is similar to what we had before, you know. Maybe there's no alpha over here, but there's an alpha here.

372
00:46:58.320 --> 00:47:11.520
Inderjit Dhillon: But look at the the big differences over here, actually. that you have wt plus one is Wt: and this is the update. but the update in the previous case with momentum.

373
00:47:11.860 --> 00:47:15.710
Inderjit Dhillon: We have seen that this was the gradient of F at Wt.

374
00:47:17.350 --> 00:47:27.160
Inderjit Dhillon: But here. instead of gradient of F at Wt. you actually have a gradient of F at

375
00:47:28.200 --> 00:47:31.150
Inderjit Dhillon: Oh. Vt. Minus one

376
00:47:32.290 --> 00:47:44.870
Inderjit Dhillon: at Wt. Minus beta vt. Minus one, so it's not at. but it has a slightly different point. and that different point is now. you know again.

377
00:47:45.240 --> 00:47:48.870
Inderjit Dhillon: like some oh.

378
00:47:49.610 --> 00:47:54.110
Inderjit Dhillon: moving average, but it's not quite a moving average of

379
00:47:54.410 --> 00:47:58.300
Inderjit Dhillon: gradients at the particular previous iterates.

380
00:47:58.510 --> 00:48:03.930
Inderjit Dhillon: These gradients are actually at points which are not necessarily iterates.

381
00:48:06.590 --> 00:48:09.990
Inderjit Dhillon: Okay, so here. It is kind of illustrated over here.

382
00:48:10.420 --> 00:48:12.730
Inderjit Dhillon: but if you look at a momentum, update.

383
00:48:13.600 --> 00:48:17.620
Inderjit Dhillon: then a typical momentum. Update. As you take a gradient step.

384
00:48:19.980 --> 00:48:22.700
Inderjit Dhillon: you take a momentum step.

385
00:48:24.100 --> 00:48:28.960
Inderjit Dhillon: and then you kind of add them up together. and then you actually get the actual step.

386
00:48:30.080 --> 00:48:34.360
Inderjit Dhillon: Okay, so just to kind of make it clear.

387
00:48:38.980 --> 00:48:42.580
Inderjit Dhillon: You take a gradient step.

388
00:48:45.140 --> 00:48:47.290
Inderjit Dhillon: You take a momentum step.

389
00:48:48.320 --> 00:48:50.410
Inderjit Dhillon: and then you basically add them together.

390
00:48:52.360 --> 00:48:56.130
Inderjit Dhillon: and that's ends up being your overall step size.

391
00:48:57.700 --> 00:48:59.660
Inderjit Dhillon: whereas momentum.

392
00:49:01.720 --> 00:49:05.600
Inderjit Dhillon: what it does is it takes a momentum step.

393
00:49:08.480 --> 00:49:15.320
Inderjit Dhillon: and that momentum step can be described as Wt. Minus beta vt. Minus one.

394
00:49:16.980 --> 00:49:19.550
Inderjit Dhillon: and then it takes the gradient at that step.

395
00:49:22.560 --> 00:49:28.230
Inderjit Dhillon: Okay, and then it takes so it's not, you know, very important, for you know.

396
00:49:28.530 --> 00:49:30.850
Inderjit Dhillon: super critical to know this. But

397
00:49:30.960 --> 00:49:37.450
Inderjit Dhillon: you know nestle of actually gradient actually ends up giving theoretically very

398
00:49:38.420 --> 00:49:39.040
oh.

399
00:49:39.070 --> 00:49:40.990
Inderjit Dhillon: better convergence.

400
00:49:42.070 --> 00:49:49.100
Inderjit Dhillon: But somehow, in deep learning, it hasn't been as useful as the as in

401
00:49:49.390 --> 00:49:51.810
Inderjit Dhillon: as in theory, actually okay.

402
00:49:51.880 --> 00:49:54.100
Inderjit Dhillon: So so

403
00:49:54.510 --> 00:50:02.140
Inderjit Dhillon: typically we end up using this kind of momentum stochastic, gradient descent.

404
00:50:03.000 --> 00:50:08.270
Inderjit Dhillon: Okay, and we'll come to the As I said, we'll come to the Adam Method a little bit later.

405
00:50:10.600 --> 00:50:24.060
Inderjit Dhillon: So now the question is, why the momentum works, and I think I kind of gave you a little reason for why it works better right for Std. Because

406
00:50:24.570 --> 00:50:30.360
Inderjit Dhillon: if you just look at one single point, let's take the extreme end where it be. Path, size is equal to one.

407
00:50:31.010 --> 00:50:33.240
Inderjit Dhillon: You look at just one single point.

408
00:50:37.220 --> 00:50:39.290
Inderjit Dhillon: and then you basically get

409
00:50:39.940 --> 00:50:46.820
Inderjit Dhillon: the gradient estimator to be potentially varying quite a bit, even though it is unbiased in the sense that

410
00:50:46.900 --> 00:50:56.600
Inderjit Dhillon: the mean of this gradient estimator will be the actual gradient itself. That's what it means to be an unbiased estimator. It can vary quite a bit.

411
00:50:56.940 --> 00:50:59.060
Inderjit Dhillon: Okay, what that means is

412
00:50:59.230 --> 00:51:02.210
Inderjit Dhillon: that the variance off

413
00:51:02.860 --> 00:51:06.970
Inderjit Dhillon: gradient estimation the it? An estimator for Std

414
00:51:07.070 --> 00:51:08.930
Inderjit Dhillon: could actually be quite large.

415
00:51:09.840 --> 00:51:21.150
Inderjit Dhillon: Okay, you know, you increase the bad size. Maybe it will be smaller. Okay. But the reason why momentum works is because it reduces the variance

416
00:51:21.390 --> 00:51:22.900
Inderjit Dhillon: of the gradient estimator.

417
00:51:23.910 --> 00:51:36.440
Inderjit Dhillon: and the way it does it is because it just doesn't discard the previous gradient values. but actually keep them around. As a result, the variance decreases. Okay, and you can see kind of some plots over here.

418
00:51:37.340 --> 00:51:42.420
Inderjit Dhillon: This is std without momentum. This is std with momentum.

419
00:51:42.510 --> 00:51:49.250
Inderjit Dhillon: Remember, these are the level sets. So this is F. Of x equal to Alpha one.

420
00:51:49.470 --> 00:52:01.340
Inderjit Dhillon: This is F. Of x, equal to alpha 2, where alpha 2 is less than alpha one. So we want to basically land up over here. If you look at this Std, then

421
00:52:01.630 --> 00:52:05.430
Inderjit Dhillon: you know, it bounces around. It is moving in the right direction.

422
00:52:05.760 --> 00:52:14.420
Inderjit Dhillon: But when we use std with momentum. then it converges a little bit faster. then std without a moment

423
00:52:17.540 --> 00:52:19.240
Inderjit Dhillon: any questions. So far

424
00:52:31.080 --> 00:52:36.070
Inderjit Dhillon: the all the question is whether you know how much can we improve

425
00:52:38.310 --> 00:52:39.710
Inderjit Dhillon: this kind of method?

426
00:52:41.560 --> 00:52:46.700
Inderjit Dhillon: Okay. what is better than Std better than gradient descent?

427
00:52:49.680 --> 00:52:58.440
Inderjit Dhillon: Well, if you look at traditional optimization. Std. Or sorry Gradient descent is what's called the first auto optimization method.

428
00:53:00.310 --> 00:53:11.610
Inderjit Dhillon: Then there are second order, optimization methods that actually converge much quickly. But you need to do many to do a lot more work at every iteration.

429
00:53:17.230 --> 00:53:18.790
Inderjit Dhillon: Okay, so

430
00:53:19.100 --> 00:53:23.760
Inderjit Dhillon: anybody remember or know what the Newton method is.

431
00:53:30.410 --> 00:53:32.620
Inderjit Dhillon: Let's look at the gradient descent method.

432
00:53:43.550 --> 00:53:46.150
Inderjit Dhillon: Okay. So gradient descent is given over here.

433
00:53:48.840 --> 00:53:49.460
Inderjit Dhillon: Okay.

434
00:53:50.860 --> 00:53:56.160
Inderjit Dhillon: So what you can do in Newton method is

435
00:53:57.150 --> 00:54:02.600
Inderjit Dhillon: so. So what what people observe is that if the level sets are

436
00:54:03.100 --> 00:54:04.750
Inderjit Dhillon: very elliptical.

437
00:54:09.880 --> 00:54:14.130
Inderjit Dhillon: okay, then gradient. The same can actually bounce around quite a bit.

438
00:54:17.160 --> 00:54:21.230
Inderjit Dhillon: Okay. So what then one can do is over here.

439
00:54:22.020 --> 00:54:29.180
Inderjit Dhillon: you can actually do something called H. Inverse. Okay, it stands for

440
00:54:30.210 --> 00:54:31.800
Inderjit Dhillon: the Hessian.

441
00:54:32.900 --> 00:54:40.040
Inderjit Dhillon: Okay, and that is the second. Derivative All F. With respect to W.

442
00:54:40.560 --> 00:54:43.340
Inderjit Dhillon: Now, remember, W. Is a. Vector

443
00:54:44.680 --> 00:54:53.940
Inderjit Dhillon: The gradient is a vector too. Okay. So I guess you know that I should have made clear right W. W. The vector

444
00:54:55.090 --> 00:55:04.870
Inderjit Dhillon: so the gradient is also a vector of the same dimensions. The Hessian or the second derivative is a matrix.

445
00:55:11.590 --> 00:55:14.970
Inderjit Dhillon: Okay, and it's the matrix of the different

446
00:55:16.470 --> 00:55:18.010
Inderjit Dhillon: partial rooms.

447
00:55:19.110 --> 00:55:22.910
Inderjit Dhillon: Secondary is right. So this is Oh.

448
00:55:24.220 --> 00:55:30.440
Inderjit Dhillon: this is a matrix. So this with the H inverse is what's called Newton method.

449
00:55:39.720 --> 00:55:43.560
Inderjit Dhillon: Okay, and Newton method has much faster

450
00:55:44.200 --> 00:55:48.060
Inderjit Dhillon: convergence when you start close to the solution.

451
00:55:49.780 --> 00:55:57.130
Inderjit Dhillon: But you can see one obvious drawback which means you actually have to solve a linear system of equations at every step.

452
00:55:57.840 --> 00:56:04.120
Inderjit Dhillon: and that can be quite expensive, right? That can be cubic in the number of dimensions.

453
00:56:05.360 --> 00:56:19.250
Inderjit Dhillon: So then, what one can try to do is that instead of taking this. you can do something like bug diagonal off edge, and

454
00:56:22.670 --> 00:56:28.510
Inderjit Dhillon: that now makes the method again. Still, kind of have linear amount of competition.

455
00:56:29.650 --> 00:56:31.720
Inderjit Dhillon: and of course, convergence suffers

456
00:56:32.810 --> 00:56:35.250
Inderjit Dhillon: so for big problems.

457
00:56:36.110 --> 00:56:41.860
Inderjit Dhillon: Okay, one possibility is to use the following right, which is to take W.

458
00:56:42.350 --> 00:56:48.950
Inderjit Dhillon: W. Minus eta times a diagonal matrix times gradient of F of double.

459
00:56:50.770 --> 00:56:54.560
Inderjit Dhillon: And here I've given motivation for why

460
00:56:55.360 --> 00:57:04.760
Inderjit Dhillon: the diagonal could be this diagonal matrix could be the the matrix of second order partial derivatives. Right?

461
00:57:09.950 --> 00:57:24.170
Inderjit Dhillon: So that is kind of the next step. which means that think of W as having many parameters. you know neural networks when you hear things like Chat Gpt or

462
00:57:24.490 --> 00:57:35.520
Inderjit Dhillon: see Gp. 4, they have not disclosed how many parameters it uses. But Gpd 3 was 175 billionparameters. which means that this vector

463
00:57:35.560 --> 00:57:38.240
Inderjit Dhillon: has size 175 billion

464
00:57:39.460 --> 00:57:42.490
Inderjit Dhillon: You know 175 billion

465
00:57:45.990 --> 00:57:48.350
Inderjit Dhillon: Okay, so this is a very very long. Vector

466
00:57:48.920 --> 00:57:55.560
Inderjit Dhillon: and obviously, you know, you don't want to form a matrix which is 175 billionby 175,

467
00:57:55.800 --> 00:57:56.630
Inderjit Dhillon: believe.

468
00:57:58.800 --> 00:58:07.140
Inderjit Dhillon: Okay, but you can still try and use diagonal. Now. what this would mean if you have something diagonal

469
00:58:09.400 --> 00:58:18.920
Inderjit Dhillon: is that every if you look at this right. The step size is either gradient is given by a certain value. Right

470
00:58:21.540 --> 00:58:31.000
Inderjit Dhillon: then, this is basically saying that for every diagonal for every parameter there is a specialized waiting step, or you can actually even think of a learning step.

471
00:58:31.920 --> 00:58:44.900
Inderjit Dhillon: right? So that could be the advantage of doing diagonal waiting. And so now we are kind of looking at the next step from Std. Which is

472
00:58:45.240 --> 00:58:54.170
Inderjit Dhillon: Std. That incorporates such diagonal scaling. Okay. So one way to do it is through a method called.

473
00:58:58.090 --> 00:59:02.930
Inderjit Dhillon: So the Scd update has the same step size for all variables

474
00:59:03.930 --> 00:59:15.420
Inderjit Dhillon: or all the parameters. And then you can have adaptive algorithms. So Newton method is kind of one extreme example of an adoptive method. Each dimension can have a different step size.

475
00:59:16.400 --> 00:59:19.660
Inderjit Dhillon: Okay, so think about the diagonal scaling that I talked about.

476
00:59:20.280 --> 00:59:25.810
Inderjit Dhillon: Right? So let's look at the there's a particular algorithm called undergrad, and let's look at it's

477
00:59:27.580 --> 00:59:31.930
Inderjit Dhillon: what it does. Okay. let's go straight to the update.

478
00:59:32.420 --> 00:59:37.600
Inderjit Dhillon: So I have Wt: plus. One is Wt minus era

479
00:59:37.650 --> 00:59:38.730
Inderjit Dhillon: times

480
00:59:38.990 --> 00:59:41.270
Inderjit Dhillon: gradient. This is the same as before.

481
00:59:46.460 --> 00:59:47.280
Inderjit Dhillon: Okay.

482
00:59:47.830 --> 00:59:51.480
Inderjit Dhillon: except now, there is a scaling factor over here.

483
00:59:52.710 --> 01:00:03.100
Inderjit Dhillon: and that is something in the denominator. And if you think about D or d inverse well, that's kind of what it is hinting at.

484
01:00:04.280 --> 01:00:06.350
Inderjit Dhillon: Now, where does this come from?

485
01:00:07.640 --> 01:00:11.880
Inderjit Dhillon: Well. what you can do is you can look at

486
01:00:12.950 --> 01:00:15.490
Inderjit Dhillon: all the

487
01:00:15.900 --> 01:00:17.640
Inderjit Dhillon: in that you get

488
01:00:19.850 --> 01:00:20.710
Inderjit Dhillon: okay.

489
01:00:21.520 --> 01:00:26.370
Inderjit Dhillon: and you can keep on updating this value G

490
01:00:27.310 --> 01:00:30.150
Inderjit Dhillon: from the gradients at all the previous steps.

491
01:00:34.030 --> 01:00:35.980
Inderjit Dhillon: So Gt: minus one.

492
01:00:36.540 --> 01:00:39.690
Inderjit Dhillon: So here i'll basically get the gradients for

493
01:00:39.840 --> 01:00:43.750
Inderjit Dhillon: the previous steps. And J is the

494
01:00:43.990 --> 01:00:47.200
Inderjit Dhillon: Jeth element in this particular. Vector

495
01:00:48.980 --> 01:00:53.360
Inderjit Dhillon: Okay? And so when I'm updating the it's value.

496
01:00:54.560 --> 01:00:59.890
Inderjit Dhillon: then I'm: scaling it by square root times, this where this is the square.

497
01:01:00.920 --> 01:01:16.500
Inderjit Dhillon: Yeah, so you can think of this as kind of computing the non-centered variance. And then I divide by the square root of the non-centered kind of variance of that particular coordinator, that particular parameter.

498
01:01:18.200 --> 01:01:19.000
Inderjit Dhillon: Okay?

499
01:01:19.160 --> 01:01:24.670
Inderjit Dhillon: And then, of course, and then there is a you know, if this ends up happening to be 0,

500
01:01:26.010 --> 01:01:36.940
Inderjit Dhillon: which can happen because of the you know some of the architectures that are being used these days for, especially the forty-point computations that are being used.

501
01:01:37.110 --> 01:01:52.390
Inderjit Dhillon: You hear about, you know, not only just single position, but, you know either float 16 or B 4, 16, and even something smaller. Right? There is a easier for complications to actually overflow and underflow.

502
01:01:52.560 --> 01:01:59.050
Inderjit Dhillon: So that's actually another reason why this Epstein is actually quite important. It guards against kind of divide by 0.

503
01:02:01.400 --> 01:02:05.440
Inderjit Dhillon: Okay, so this is what undergrad does

504
01:02:06.500 --> 01:02:07.970
Inderjit Dhillon: for each dimension.

505
01:02:08.200 --> 01:02:12.120
Inderjit Dhillon: I right. I have this big

506
01:02:12.630 --> 01:02:20.630
Inderjit Dhillon: for each parameter. You can think about it right. Many, many many parameters or dimensions for each dimension.

507
01:02:21.080 --> 01:02:24.260
Inderjit Dhillon: You observed capital d samples.

508
01:02:24.270 --> 01:02:26.600
Inderjit Dhillon: you and I, she

509
01:02:29.780 --> 01:02:43.620
Inderjit Dhillon: g one I Gt: I Okay. And I can basically take the non-centered kind of standard deviation. some of squares divided by

510
01:02:43.650 --> 01:02:47.170
Inderjit Dhillon: the right where we've observed T. Samples.

511
01:02:48.500 --> 01:02:49.200
Inderjit Dhillon: Okay.

512
01:02:50.670 --> 01:02:56.110
And then, if I am decreasing the step size as kind of square root. T.

513
01:02:57.620 --> 01:03:01.250
Inderjit Dhillon: Then this is one way to think about it. Right that i'm

514
01:03:01.320 --> 01:03:11.280
Inderjit Dhillon: taking this step. Size the standard deviation is, and i'm dividing by the standard deviation these 2 cancel out, and it gives me this.

515
01:03:14.880 --> 01:03:21.700
Inderjit Dhillon: Okay. so kind of that's the next evolution of

516
01:03:23.520 --> 01:03:25.750
Inderjit Dhillon: deep learning optimizers.

517
01:03:26.040 --> 01:03:35.150
Inderjit Dhillon: And now we come to, you know, Adam, which I've been talking a little bit about before I mentioned before, and that's what it is. It's basically

518
01:03:36.470 --> 01:03:38.210
Inderjit Dhillon: adaptive momentum.

519
01:03:40.340 --> 01:03:51.200
Inderjit Dhillon: Okay, adaptive momentum. So it's all you can think of it as momentum plus adaptive updates. So right now, just think about

520
01:03:52.830 --> 01:04:05.460
Inderjit Dhillon: sampling. You know one data point at a time. You look at the gradient, this could easily be a meaning patch. You look at the gradient that comes from this.

521
01:04:06.370 --> 01:04:14.910
Inderjit Dhillon: You look at m for momentum. So remember this is that Beta that we use is now beta one over here.

522
01:04:16.960 --> 01:04:20.930
Inderjit Dhillon: Okay, there's the gradient There's the previous values.

523
01:04:22.160 --> 01:04:25.390
Inderjit Dhillon: And now we also look at the

524
01:04:26.200 --> 01:04:28.580
Inderjit Dhillon: second moment, or the square.

525
01:04:29.910 --> 01:04:39.800
Inderjit Dhillon: because we are looking at to compute the variance of standard deviation right? And we do that also in this

526
01:04:42.720 --> 01:04:46.590
Inderjit Dhillon: running some kind of way. What did we call it? Let's see

527
01:04:47.280 --> 01:04:49.010
Inderjit Dhillon: moving average, right?

528
01:04:49.070 --> 01:04:52.270
Inderjit Dhillon: So again, we are looking at the moving averages.

529
01:04:54.980 --> 01:04:58.020
Inderjit Dhillon: So we have this quantity. Okay.

530
01:04:59.090 --> 01:05:10.150
Inderjit Dhillon: Let's ignore these 2 for now, and we'll come back to that later. But this right now assume that empty hat is empty. Vt. Head is vt.

531
01:05:10.680 --> 01:05:12.650
Inderjit Dhillon: And now the update is

532
01:05:13.000 --> 01:05:17.920
Inderjit Dhillon: Wt. Is Wt. Minus one minus particular step size

533
01:05:18.990 --> 01:05:19.960
Inderjit Dhillon: alpha

534
01:05:20.850 --> 01:05:25.020
Inderjit Dhillon: empty, divided by square root of Vt. Hand, so that's

535
01:05:25.180 --> 01:05:29.720
Inderjit Dhillon: like an undergrad step. except now it is with the

536
01:05:30.030 --> 01:05:38.530
Inderjit Dhillon: the the moving average of the square gradient values. Okay. And this is the momentum

537
01:05:39.030 --> 01:05:42.320
Inderjit Dhillon: gradient. This is moving out of the

538
01:05:43.100 --> 01:05:47.640
Inderjit Dhillon: Okay, and this is kind of the well known Adam optimizer.

539
01:05:47.940 --> 01:05:54.940
Inderjit Dhillon: And so, for example, when you look at training deeper networks.

540
01:05:55.090 --> 01:05:57.160
Inderjit Dhillon: then you will many times see

541
01:05:57.820 --> 01:06:10.510
Inderjit Dhillon: you'll be able to choose your optimizer many times the choices that Adam Optimizer, and then sometimes, you know, you'll have to choose Beta one and Beta 2 and many times when you read

542
01:06:10.880 --> 01:06:19.700
Inderjit Dhillon: papers or deep learning. they will say, what optimizer that you use right. For example, they'll say we used.

543
01:06:21.670 --> 01:06:29.820
Inderjit Dhillon: We used beta, you know, Adam Optimizer with Beta one equal to this value, and Beta to equal to certain such value.

544
01:06:31.670 --> 01:06:34.330
Inderjit Dhillon: Now I ignored this part.

545
01:06:36.450 --> 01:06:42.890
Inderjit Dhillon: This part is actually beta one to the poverty. Beta 2 to the poverty

546
01:06:42.900 --> 01:06:45.590
Inderjit Dhillon: right? So what ends up happening is

547
01:06:45.860 --> 01:06:52.920
Inderjit Dhillon: that this is a little bit of like bias correction, because in the beginning.

548
01:06:53.220 --> 01:06:59.950
Inderjit Dhillon: when t is one, t is 2. We really don't have that many terms in the moving average.

549
01:07:00.280 --> 01:07:02.500
Inderjit Dhillon: and so it kind of correct for that

550
01:07:03.770 --> 01:07:05.170
Inderjit Dhillon: during that time period.

551
01:07:06.270 --> 01:07:07.030
Inderjit Dhillon: Okay.

552
01:07:08.430 --> 01:07:22.440
Inderjit Dhillon: So one question that you could ask as well, what should the bad size? B: we've talked about batch equal to one obviously bad size equal to N is 2

553
01:07:22.870 --> 01:07:24.980
Inderjit Dhillon: to large.

554
01:07:26.660 --> 01:07:42.340
Inderjit Dhillon: and this has kind of, you know, by the way, a lot of things that we now are going to talk about in terms of deep learning and so on, are can be evolving right? So you know, 4 or 5 years from now the answers were different

555
01:07:42.900 --> 01:07:47.920
Inderjit Dhillon: right now. The answers are somewhat different. Right People have learned how to

556
01:07:48.040 --> 01:07:56.640
Inderjit Dhillon: use Gpus more effectively, especially multiple gpus more effectively. And so

557
01:07:58.320 --> 01:08:02.560
Inderjit Dhillon: the larger the bad size. There is more competition per update.

558
01:08:03.220 --> 01:08:06.500
Inderjit Dhillon: But when you have a lot of gpus

559
01:08:06.640 --> 01:08:11.080
Inderjit Dhillon: you can actually use them more effectively by having to very large pad sizes.

560
01:08:11.630 --> 01:08:13.400
Inderjit Dhillon: Okay, so usually

561
01:08:13.450 --> 01:08:23.950
Inderjit Dhillon: want to choose the bad side that fits into the Gpu memory and utilize a competition resource. whereas in the early days of deep learning

562
01:08:24.620 --> 01:08:31.899
Inderjit Dhillon: when multiple gpus were not used. Typically the bad sizes were much smaller. Okay. So, for example.

563
01:08:32.979 --> 01:08:42.600
Inderjit Dhillon: for image net training on a standard Gpu, you can use a 512 patch size, but as you train on bigger data sets one bigger

564
01:08:43.069 --> 01:08:48.229
Inderjit Dhillon: gpus in terms of multiple gpus, you can actually increase the spent size.

565
01:08:50.260 --> 01:08:51.830
Inderjit Dhillon: Okay.

566
01:08:53.220 --> 01:09:05.710
Inderjit Dhillon: So that's what I got. We we are saying over here that what? If you want to train a model with hundreds of thousands of gpus right and again, like if you think about

567
01:09:05.720 --> 01:09:11.210
Inderjit Dhillon: what's happening around you these days with the Gpd 3 chat, Gpt.

568
01:09:11.479 --> 01:09:14.310
Inderjit Dhillon: You know

569
01:09:14.620 --> 01:09:19.830
Inderjit Dhillon: the palm models, you know the models from Meta, and so on.

570
01:09:20.340 --> 01:09:24.640
Inderjit Dhillon: They are using hundreds and thousands of gpus or cpus.

571
01:09:26.370 --> 01:09:39.439
Inderjit Dhillon: So you can actually use data parallel distributed computing, which means that you have many kind of workers or gpus that are available to work. But you somehow need to synchronize the competition

572
01:09:40.090 --> 01:09:48.390
Inderjit Dhillon: right. After all. you are doing something like this right? Wd. This one is

573
01:09:48.430 --> 01:09:52.710
Inderjit Dhillon: wt minus Ada times.

574
01:09:53.569 --> 01:10:05.800
Inderjit Dhillon: you know Mt. Minus one divided by something square root V T. Minus one plus epsilon right? This is kind of the general form of the atom optimize

575
01:10:11.970 --> 01:10:16.790
Inderjit Dhillon: so clearly the competition is needs to be synchronized right, because

576
01:10:17.170 --> 01:10:24.920
Inderjit Dhillon: whenever t changes in the scenes. So whenever you go from one Mini batch to another venue match. right, you're coordinating.

577
01:10:26.680 --> 01:10:31.790
Inderjit Dhillon: But what you can do is you can say that. Okay, my batch

578
01:10:31.800 --> 01:10:43.900
Inderjit Dhillon: is actually now split across these multiple work workers. and then these batches will all compute their own kind of gradient. Since things are additive.

579
01:10:44.130 --> 01:10:49.500
Inderjit Dhillon: you can actually assemble the gradient. You can send it back to a master computer

580
01:10:50.060 --> 01:10:53.700
Inderjit Dhillon: which can actually do the update this update

581
01:10:54.010 --> 01:10:59.770
Inderjit Dhillon: and then send the W. Back. and then these workers can go ahead and do their competition.

582
01:11:01.320 --> 01:11:08.070
Inderjit Dhillon: Okay, so that's typically how kind of large batch training is done or can be done.

583
01:11:08.150 --> 01:11:20.430
Inderjit Dhillon: But then there are some perils to it. Right? Which is that what people found is that when you try very, very large pad sizes.

584
01:11:20.480 --> 01:11:27.260
Inderjit Dhillon: then these models end up converging to lower test accuracy. Okay.

585
01:11:27.390 --> 01:11:39.180
Inderjit Dhillon: And you know, i'm not going to go into too much detail over here. But many times people will say that you know some good methods would converse to flat minima.

586
01:11:39.290 --> 01:11:43.980
Inderjit Dhillon: while methods which are not very good might converge to

587
01:11:44.170 --> 01:11:49.070
Inderjit Dhillon: a short minima. And this is more desirable than this.

588
01:11:49.700 --> 01:11:53.050
Inderjit Dhillon: right? And what happens over here when you

589
01:11:53.440 --> 01:12:03.420
Inderjit Dhillon: increase the bad sizes that the variance goes down? Okay. But as the bad size goes up the learning rate might go up.

590
01:12:05.410 --> 01:12:11.570
Inderjit Dhillon: And so you wanted to know kind of control your learning rate, like we said before.

591
01:12:11.700 --> 01:12:16.280
Inderjit Dhillon: you cannot keep on increasing the learning rate without

592
01:12:16.480 --> 01:12:17.440
Inderjit Dhillon: limit.

593
01:12:17.760 --> 01:12:24.840
Inderjit Dhillon: and so sometimes people will say, Look at the different layers of your neural network. So this is.

594
01:12:26.780 --> 01:12:39.300
Inderjit Dhillon: This is one of the large convolutional neural networks. I believe it's a resnet. and you can see that the W. Values. So now, if you think about all my parameter values right

595
01:12:40.190 --> 01:12:42.660
Inderjit Dhillon: when you have a very large neural network.

596
01:12:45.490 --> 01:12:57.510
Inderjit Dhillon: right? And we'll talk about different neural network architecture soon. But you know this: the weight matrix here, the weight matrix here, weight matrix here, weight matrix Here

597
01:12:57.560 --> 01:12:59.910
Inderjit Dhillon: he's basically cultivating towards this W.

598
01:13:00.900 --> 01:13:04.940
Inderjit Dhillon: But layer my layer, what could be happening is that the

599
01:13:05.010 --> 01:13:11.160
Inderjit Dhillon: the the magnitudes of these weight vectors might be very different from each other. Right?

600
01:13:11.300 --> 01:13:15.360
Inderjit Dhillon: So this is an example where, you know, in certain cases

601
01:13:15.610 --> 01:13:26.790
Inderjit Dhillon: the weights actually have much larger magnitude compared to their gradients. And if you think about the updates. the updates are somewhat proportional to the gradients

602
01:13:26.960 --> 01:13:36.790
Inderjit Dhillon: right. So if I have, W. Is W. Minus era times gradient. right of F. And W.

603
01:13:38.450 --> 01:13:40.370
Inderjit Dhillon: Okay, then.

604
01:13:43.260 --> 01:13:57.940
Inderjit Dhillon: then, just think about it right. If this is a very large number, and this is very small. That means you're hardly making any change. but this is very small. then it is going to be over it. And why it this, if it is very large.

605
01:13:58.980 --> 01:14:08.740
Inderjit Dhillon: right? So, as a result, when people use way, deep learning networks. they end up actually adapting the learning rate.

606
01:14:09.860 --> 01:14:11.760
Inderjit Dhillon: you have different scales.

607
01:14:13.250 --> 01:14:16.670
Inderjit Dhillon: Okay. So if I is the I, its layer

608
01:14:18.070 --> 01:14:19.760
Inderjit Dhillon: of the neural network.

609
01:14:20.270 --> 01:14:25.390
Inderjit Dhillon: then you can actually take G divided by normal G,

610
01:14:26.090 --> 01:14:30.890
Inderjit Dhillon: so that the gradient vector is basically unit norm.

611
01:14:32.460 --> 01:14:34.240
Inderjit Dhillon: So it's kind of normalized.

612
01:14:35.400 --> 01:14:40.130
Inderjit Dhillon: These values have magnitude, you know, Norm of W.

613
01:14:41.070 --> 01:14:54.280
Inderjit Dhillon: So you put norm of W over this. So now this and this are on the same scale. So if these values are 10 or minus 3, these values will also be 10, point 3, and then

614
01:14:54.380 --> 01:15:04.760
Inderjit Dhillon: use it to kind of control the probably. So it's. It's for their investigations and solutions like this

615
01:15:05.000 --> 01:15:06.800
Inderjit Dhillon: that end up.

616
01:15:07.240 --> 01:15:14.930
Inderjit Dhillon: you know, earlier, you know, we'll talk about like the bird model right, and

617
01:15:16.470 --> 01:15:25.270
Inderjit Dhillon: the board model is can be quite expensive. But you know there was a a paper in Icl. 20, which ended up using large batches

618
01:15:25.470 --> 01:15:27.550
Inderjit Dhillon: to do the board modeling

619
01:15:27.660 --> 01:15:37.230
Inderjit Dhillon: Okay, and was able to do it only in 76min. Of course, in 2020 this was a large model. Now the models have become much, much, much, much larger.

620
01:15:37.400 --> 01:15:38.720
Inderjit Dhillon: But really

621
01:15:39.780 --> 01:15:42.230
Inderjit Dhillon: the takeaway for you should be

622
01:15:42.330 --> 01:15:50.730
Inderjit Dhillon: that even though there is a current set of solutions right now. There is actually a lot more that can be done to kind of make the methods

623
01:15:50.750 --> 01:15:51.630
Inderjit Dhillon: better.

624
01:15:52.930 --> 01:16:02.720
Inderjit Dhillon: So this is kind of just a a quick tour. giving you the different methods like Std. And in particular, Adam.

625
01:16:03.140 --> 01:16:07.620
Inderjit Dhillon: that are now used in training

626
01:16:07.640 --> 01:16:10.800
Inderjit Dhillon: the state of the deep learning architectures.

627
01:16:11.810 --> 01:16:19.530
Inderjit Dhillon: So that kind of comes toward to the end of the lecture today. Any questions.

628
01:16:25.940 --> 01:16:27.340
Zengqi Liu: Hello, Professor

629
01:16:28.680 --> 01:16:33.300
Zengqi Liu: I'm. A small point. Is that

630
01:16:33.580 --> 01:16:34.570
Zengqi Liu: so?

631
01:16:34.830 --> 01:16:38.470
Zengqi Liu: It's a only one

632
01:16:39.960 --> 01:16:46.560
Zengqi Liu: first question will be chosen randomly or or choose from some rules.

633
01:16:48.200 --> 01:16:54.590
Inderjit Dhillon: Yeah. So you know, I don't think really it is chosen randomly in practice.

634
01:16:54.990 --> 01:17:01.180
Inderjit Dhillon: So when people talk about convergence of these algorithms. They talk about

635
01:17:01.420 --> 01:17:04.520
Inderjit Dhillon: choosing them randomly, but

636
01:17:06.880 --> 01:17:14.880
Inderjit Dhillon: in practice, so I think what you can do is you can shuffle the points in the beginning, and then you can

637
01:17:15.150 --> 01:17:23.620
Inderjit Dhillon: use them in a prescribed order. and then it depends whether you shuffle them at the beginning of every epoch.

638
01:17:23.900 --> 01:17:34.230
Inderjit Dhillon: So so the term epoch is used when all the training points are looked at at least one, and it looked at exactly once.

639
01:17:35.180 --> 01:17:41.590
Zengqi Liu: So put totally. Every each bachelor will be used in that training at least once.

640
01:17:42.660 --> 01:17:45.090
Inderjit Dhillon: if that's the way you chose to do training.

641
01:17:45.290 --> 01:17:51.160
Inderjit Dhillon: Oh, okay, right. I mean, look, a lot of this is depends upon your application.

642
01:17:52.420 --> 01:17:56.040
Zengqi Liu: Okay, right? And it also depends on, like.

643
01:17:56.190 --> 01:17:57.610
Inderjit Dhillon: you know, some

644
01:17:59.750 --> 01:18:05.450
Inderjit Dhillon: just, you know, in terms of getting efficiency right? So, for example, all this large batch training

645
01:18:05.850 --> 01:18:16.080
Inderjit Dhillon: is a way to try and get efficiency. because, you know, these deep learning models, especially the large ones. can be so expensive.

646
01:18:17.330 --> 01:18:22.000
Inderjit Dhillon: You want to use methods which are pretty

647
01:18:23.260 --> 01:18:24.310
Inderjit Dhillon: efficient.

648
01:18:25.430 --> 01:18:28.580
Inderjit Dhillon: So I don't know any place. Do you have anything to add with that

649
01:18:29.950 --> 01:18:36.980
Inderjit Dhillon: like? When you train your methods, you. What order do you train them in when you have it?

650
01:18:37.940 --> 01:18:42.730
Nilesh Gupta: Yeah. Usually like you shuffle your whole training data.

651
01:18:43.870 --> 01:18:46.550
Nilesh Gupta: You go what that suffer order?

652
01:18:46.770 --> 01:18:48.490
Nilesh Gupta: That's good.

653
01:18:50.760 --> 01:19:09.390
Zengqi Liu: Did that answer your question? Thank you. Yeah. I this just focused. The first question is Sol, and then another question is as the 90 rate is, you risk the it's full of the rules. So I think of. Are there a lot of training that we will be up

654
01:19:09.390 --> 01:19:17.470
Zengqi Liu: much, much, much larger, so how to for the way. Make sure that this process is stable.

655
01:19:18.350 --> 01:19:24.150
Zengqi Liu: Which one are we talking about this particular learning rate?

656
01:19:24.540 --> 01:19:28.800
Inderjit Dhillon: No; but the were you talking about the polynomial thing that we talked about, or

657
01:19:28.830 --> 01:19:30.210
Zengqi Liu: yeah to the

658
01:19:36.400 --> 01:19:51.860
Zengqi Liu: Yes here. Okay. So as I, I think the is the by, it's pulling the rules. So I think that for up there a lot lot larger as it is the Region Times the then it will be much larger.

659
01:19:51.860 --> 01:19:56.820
Zengqi Liu: I are you saying that. Are you reading this as Ada to the poverty?

660
01:19:57.050 --> 01:19:57.790
Zengqi Liu: Yes.

661
01:19:58.040 --> 01:20:02.830
Inderjit Dhillon: no, no, no, okay. So I should have made it clear that this is not.

662
01:20:03.120 --> 01:20:04.460
Inderjit Dhillon: This is a dot.

663
01:20:05.260 --> 01:20:16.390
Inderjit Dhillon: Oh, okay, it's just a simple that it's it's time I use different data.

664
01:20:18.010 --> 01:20:21.600
Inderjit Dhillon: We I talked a little bit about. Where is it like over here.

665
01:20:21.880 --> 01:20:23.020
Zengqi Liu: Yeah.

666
01:20:23.970 --> 01:20:26.510
Inderjit Dhillon: right? So this is eta

667
01:20:26.740 --> 01:20:33.760
Inderjit Dhillon: at the is t minus a. It has to decrease

668
01:20:34.230 --> 01:20:46.850
Zengqi Liu: so we can use a a a function to measure it and T, and make it a decrease at least.

669
01:20:47.610 --> 01:20:55.430
Zengqi Liu: and the the polynomial rate is your If you're you're ready, it's the use the most of the time.

670
01:20:58.770 --> 01:21:01.320
Zengqi Liu: So most of the time. I use only polynomials.

671
01:21:02.140 --> 01:21:10.440
Inderjit Dhillon: I don't know if I would say most of the time. But again, Nilesh, do you want to chime in with that. Or.

672
01:21:13.060 --> 01:21:17.120
Nilesh Gupta: yeah, a pretty common learning like to do is like

673
01:21:17.720 --> 01:21:18.480
Nilesh Gupta: you

674
01:21:19.900 --> 01:21:25.070
Nilesh Gupta: kind of linearly increase the learning rate until a point, and then you'd linearly decay.

675
01:21:25.110 --> 01:21:30.460
Nilesh Gupta: So that's also a polynomial. Another one is the one which is shown like you.

676
01:21:30.510 --> 01:21:35.290
Nilesh Gupta: Like you takes after a certain number of steps. You kind of decrease it.

677
01:21:35.680 --> 01:21:36.930
Nilesh Gupta: the piece of learning.

678
01:21:37.540 --> 01:21:41.160
Nilesh Gupta: So these are, you will see, like many more.

679
01:21:41.390 --> 01:21:52.630
Nilesh Gupta: But these are kind of the standard ones that you can use. So typically these days. I think you know my understanding is that when you look at paper they like to describe these things like when they, for example.

680
01:21:52.740 --> 01:22:03.520
Inderjit Dhillon: they'll describe the hyper parameters they'd include like, oh, you know, this is the optimizer I used. Add them with beta, one equal to Beta 2, and then they've described what the learning rate schedule. Also, they use this.

681
01:22:04.730 --> 01:22:05.550
Zengqi Liu: Okay.

682
01:22:06.530 --> 01:22:14.980
Inderjit Dhillon: But typically you need to have the learning schedule that eventually kind of the learning rate decays to 0.

683
01:22:15.650 --> 01:22:16.360
Zengqi Liu: Yeah.

684
01:22:17.650 --> 01:22:18.870
Zengqi Liu: Okay. Thank you very much.

685
01:22:19.590 --> 01:22:24.210
Inderjit Dhillon: Okay. Okay. Thank you. Any other questions.

686
01:22:28.810 --> 01:22:40.240
Inderjit Dhillon: Okay, If not, thank you for listening, and we will now meet on. We will me on Wednesday.

687
01:22:41.400 --> 01:22:42.580
Inderjit Dhillon: Okay, bye, bye.

688
01:22:48.770 --> 01:22:49.770
Ashray Desai: Thank you.

WEBVTT

1
00:02:36.500 --> 00:02:37.260
Nilesh Gupta: Okay.

2
00:02:38.390 --> 00:02:39.360
Nilesh Gupta: hello, everyone.

3
00:02:39.370 --> 00:02:44.620
Nilesh Gupta: So today i'll be taking a lecture. And today's lecture is going to be

4
00:02:44.730 --> 00:02:47.420
Nilesh Gupta: on popular neural network architectures

5
00:02:47.460 --> 00:02:57.330
Nilesh Gupta: and mainly, specifically. We'll be looking into Cnn's and Rn. With Stanford Convolutional Newer Neural network and recorded neural network

6
00:02:59.080 --> 00:03:00.750
Nilesh Gupta: before we begin.

7
00:03:00.760 --> 00:03:08.190
Nilesh Gupta: Just some general announcements that the for the project. Some people have asked for additional compute resources.

8
00:03:08.460 --> 00:03:15.710
Nilesh Gupta: so i'm trying to figure out how to add you guys to some of the talk resources that we have.

9
00:03:16.090 --> 00:03:22.040
Nilesh Gupta: So whoever in like is looking for additional resources to run their projects on.

10
00:03:22.080 --> 00:03:29.340
Nilesh Gupta: Can you please send me an email and just to provide me the

11
00:03:29.530 --> 00:03:41.380
Nilesh Gupta: at Cs Dot, you Texas, dot a do emails for whoever wants access to those resources, and I will hopefully figure out. I figure this out by tomorrow and add you guys there.

12
00:03:42.040 --> 00:03:44.520
Nilesh Gupta: So yeah.

13
00:03:47.150 --> 00:03:48.430
Nilesh Gupta: And

14
00:03:48.550 --> 00:03:56.510
Nilesh Gupta: if there are any questions regarding project or anything. maybe we can take some time to answer that. Otherwise we can start.

15
00:04:02.950 --> 00:04:14.510
Nilesh Gupta: Okay. So before the begin. Oh. the discussion on Cnn's and ordinance. I just quickly wanted to

16
00:04:15.720 --> 00:04:23.020
Nilesh Gupta: talk about neural network initializations. We have seen this

17
00:04:23.430 --> 00:04:35.720
Nilesh Gupta: and the homework, too, I believe. and the last part. When you were trying to implement your new network in numpy. you were asked to like, try different initializations.

18
00:04:35.960 --> 00:04:36.980
Nilesh Gupta: and.

19
00:04:37.320 --> 00:04:42.420
Nilesh Gupta: as you would have seen, like some initializations, were giving good results. Some were not

20
00:04:42.640 --> 00:04:43.530
Nilesh Gupta: so.

21
00:04:44.050 --> 00:04:48.960
Nilesh Gupta: I wanted to just briefly go over that in the lecture today.

22
00:04:49.150 --> 00:04:51.280
Nilesh Gupta: So So

23
00:04:51.300 --> 00:04:59.020
Nilesh Gupta: this is a very nice blog, which kind of explains it in a very nice way. So i'll just go over it.

24
00:04:59.120 --> 00:05:00.060
Nilesh Gupta: So

25
00:05:01.150 --> 00:05:05.530
Nilesh Gupta: they? Yeah, that how you initialize your neural network is kind of

26
00:05:05.660 --> 00:05:12.070
Nilesh Gupta: very important, especially when you go deeper and deeper, start like playing with deeper architectures.

27
00:05:12.180 --> 00:05:23.360
Nilesh Gupta: So to illustrate this example, like the pro. the problems that you get when you like, choose a 0 initialization

28
00:05:23.490 --> 00:05:34.690
Nilesh Gupta: like in this particular example, the data set looks like this, which is a kind of a circular data set. The red classes are in a inner circle, and the blue classes are outside the circle.

29
00:05:34.770 --> 00:05:35.520
Nilesh Gupta: This.

30
00:05:35.970 --> 00:05:45.210
Nilesh Gupta: and if you choose a 0 initializations, what happens is like. Everything looks symmetric, like all the neurons that you get are going to symmetric and the backward.

31
00:05:45.280 --> 00:05:50.830
Nilesh Gupta: The gradients that they will get is also going to be symmetric. So, essentially like

32
00:05:51.260 --> 00:05:56.490
Nilesh Gupta: It's it's it's like that. You only have one neuron instead of multiple neurons in the network.

33
00:05:56.570 --> 00:06:01.740
Nilesh Gupta: and there is nothing to break the symmetry. So that's why you never want to initialize things to 0.

34
00:06:02.000 --> 00:06:08.580
Nilesh Gupta: And the problem that happens when you like. Choose a very small initialization.

35
00:06:08.690 --> 00:06:09.660
Nilesh Gupta: Is that

36
00:06:10.960 --> 00:06:16.640
Nilesh Gupta: the the backward gradient that you get are going to be very small.

37
00:06:17.080 --> 00:06:17.990
Nilesh Gupta: And

38
00:06:18.020 --> 00:06:26.150
Nilesh Gupta: so this leads to very slow convergence, like as you can see now, like now, the network finally has started to

39
00:06:26.260 --> 00:06:36.400
Nilesh Gupta: and learn something that this is the output of the network. So it has, starting to learn something decision boundary which actually separates those 2,

40
00:06:36.430 --> 00:06:44.710
Nilesh Gupta: but, like it took a lot of time to figure it out. So you don't want to initialize your network too small by you with 2 small values.

41
00:06:45.010 --> 00:06:48.620
Nilesh Gupta: So if you initialize it with the appropriate values

42
00:06:48.700 --> 00:06:57.110
Nilesh Gupta: like things will converge much quicker, and you'll get very healthy gradients and the weights that you'll get, but also the

43
00:06:57.810 --> 00:06:59.370
Nilesh Gupta: nice. So

44
00:06:59.570 --> 00:07:10.270
Nilesh Gupta: here, like we are just saying appropriate values, but like just in a bit we'll look at what those appropriate values can look like. And how do we define that? What is the appropriate value?

45
00:07:10.530 --> 00:07:11.560
Nilesh Gupta: So

46
00:07:11.620 --> 00:07:21.880
Nilesh Gupta: yeah, and the other. So if you have a appropriate initialization, things will be nice and like you will converge much faster. and if you have 2 large

47
00:07:21.890 --> 00:07:23.820
Nilesh Gupta: initializations.

48
00:07:23.860 --> 00:07:29.800
Nilesh Gupta: then like things can easily saturate out, and your training will be very unstable like

49
00:07:29.990 --> 00:07:31.300
Nilesh Gupta: they'll be only

50
00:07:31.370 --> 00:07:33.470
Nilesh Gupta: and you

51
00:07:33.620 --> 00:07:46.200
Nilesh Gupta: whole optimization will go towards a local minima and like it will just get stuck there, and or that you will be oscillating between different in the in the law lost landscape.

52
00:07:46.510 --> 00:07:47.480
Nilesh Gupta: So

53
00:07:47.760 --> 00:07:50.950
Nilesh Gupta: yeah, as we can see through this the one station that

54
00:07:50.980 --> 00:07:59.080
Nilesh Gupta: you don't want either 0 to small or 2 large initializations. You want kind of appropriate initializations of your neural network.

55
00:07:59.230 --> 00:08:00.260
Nilesh Gupta: And

56
00:08:00.760 --> 00:08:04.110
Nilesh Gupta: now we will just take some time to describe

57
00:08:05.740 --> 00:08:08.620
Nilesh Gupta: what what can be some

58
00:08:08.970 --> 00:08:12.280
Nilesh Gupta: options for appropriate initializations. So

59
00:08:12.580 --> 00:08:25.940
Nilesh Gupta: you can take a look at this blog later. But i'll just go over some of the main points. The first main point is like, how do we define what is appropriate? So what do we want is, basically

60
00:08:26.150 --> 00:08:32.309
Nilesh Gupta: though mean of the activation that we should be getting. They should be around centered around 0,

61
00:08:32.650 --> 00:08:37.390
Nilesh Gupta: and the variance of the activation should stay at the same across the same layer.

62
00:08:37.600 --> 00:08:38.809
Nilesh Gupta: So basically, like.

63
00:08:39.320 --> 00:08:52.130
Nilesh Gupta: let's say, like there was some particular variance for the input. Then, after applying our neural network layer, we want that like it the kind of the variance that remains the same

64
00:08:52.670 --> 00:08:56.850
Nilesh Gupta: and like it doesn't explode, or it doesn't vanish.

65
00:08:57.500 --> 00:09:01.320
Nilesh Gupta: So why do we want this, because, like, let's say, like.

66
00:09:01.450 --> 00:09:06.390
Nilesh Gupta: even if we have our neural network clear.

67
00:09:06.440 --> 00:09:17.830
Nilesh Gupta: like, even if it multiplies the variance by 1.5 times, then, if you have 10 layers, then, like everything, will get multiplied by 1.5 to the power of 10, which will just make things chaotic.

68
00:09:18.040 --> 00:09:22.850
Nilesh Gupta: So what we want is that the variance of the activation should stay nearly the same.

69
00:09:23.330 --> 00:09:26.430
Nilesh Gupta: And intuitively, if you think like.

70
00:09:26.770 --> 00:09:35.910
Nilesh Gupta: if your input is of dimensions, N. And you are multiplying it by your matrix. So essentially what you are

71
00:09:37.040 --> 00:09:40.820
Nilesh Gupta: doing is you are taking some over n quantities.

72
00:09:42.130 --> 00:09:43.380
Nilesh Gupta: and

73
00:09:44.920 --> 00:09:45.720
Nilesh Gupta: so

74
00:09:48.510 --> 00:09:50.260
Nilesh Gupta: well, if you

75
00:09:52.830 --> 00:09:56.670
Nilesh Gupta: like, multiplying these quantities and adding them up.

76
00:09:56.780 --> 00:10:01.640
Nilesh Gupta: What you would essentially want is that to be variance? To remain the same.

77
00:10:01.830 --> 00:10:08.330
Nilesh Gupta: like the values inside your weight matrix should nearly be about one over root of dimensions.

78
00:10:08.440 --> 00:10:15.290
Nilesh Gupta: so that, like when you are multiplying all the elements and summing them up like the variance, remains to be one so.

79
00:10:15.820 --> 00:10:23.390
Nilesh Gupta: and even if you do the math like it will like it, will turn out to be some constant multiplied by square root of N.

80
00:10:23.500 --> 00:10:26.320
Nilesh Gupta: Like. This is what something that you want to multiply it.

81
00:10:26.610 --> 00:10:31.070
Nilesh Gupta: So that's what is. So what people typically do.

82
00:10:31.160 --> 00:10:39.140
Nilesh Gupta: which is, they initialize the weight matrix by normal distribution which has centered around 0. But, like the Sigma

83
00:10:39.270 --> 00:10:41.760
Nilesh Gupta: Square, is about like

84
00:10:41.850 --> 00:10:45.690
Nilesh Gupta: one by N. And here N. Is the dimension size

85
00:10:45.710 --> 00:10:46.780
Nilesh Gupta: of your input

86
00:10:47.050 --> 00:10:51.630
Nilesh Gupta: or whatever input. The this Mlp layer is saying

87
00:10:52.100 --> 00:10:55.970
Nilesh Gupta: so. This is called the initialization.

88
00:10:56.910 --> 00:10:59.580
Nilesh Gupta: This is the one of the

89
00:10:59.770 --> 00:11:11.790
Nilesh Gupta: popular any such that people typically use when they are initializing the neural network. and as shown in the demonstration here. Oh. yeah.

90
00:11:13.980 --> 00:11:19.250
Nilesh Gupta: So if you had just a like plan uniformly initialized, your weight matrices

91
00:11:20.410 --> 00:11:29.800
Nilesh Gupta: in your. This is in this example. I think that you yeah, they are using a 5 layer, one that code. and like the every layer is followed by a tanage clear.

92
00:11:29.830 --> 00:11:36.330
Nilesh Gupta: So here, like when I' hit the run button like it will try to visualize the

93
00:11:36.390 --> 00:11:42.190
Nilesh Gupta: the distribution of the activations. We will see that like If, when I have initialized

94
00:11:42.350 --> 00:11:46.000
Nilesh Gupta: uniformly the weight matrices uniformly

95
00:11:46.070 --> 00:11:50.070
Nilesh Gupta: like it's very easy for the activations to just like at the end.

96
00:11:50.120 --> 00:11:57.160
Nilesh Gupta: just saturate here like right, it becomes very peaky. It doesn't the variance doesn't remain constant across the layers.

97
00:11:57.390 --> 00:12:00.440
Nilesh Gupta: but like when when we initialize it, using Xavier

98
00:12:00.670 --> 00:12:03.220
Nilesh Gupta: like it almost remains kind of the same

99
00:12:03.300 --> 00:12:04.820
Nilesh Gupta: throughout the Yes.

100
00:12:05.940 --> 00:12:11.520
Nilesh Gupta: so. And this is what we want like. We want the variance of the activations to remain same

101
00:12:11.610 --> 00:12:19.540
Nilesh Gupta: or similar across the layers, so that, like you don't get exploding or dimension in gradients. I see.

102
00:12:26.460 --> 00:12:27.230
Nilesh Gupta: Oh.

103
00:12:28.740 --> 00:12:32.400
Nilesh Gupta: are you guys not seeing the page that i'm sharing?

104
00:12:36.870 --> 00:12:39.430
Yuhan Gao: No, we only see the Powerpoint.

105
00:12:39.480 --> 00:12:41.360
Nilesh Gupta: Oh. sure.

106
00:12:44.100 --> 00:12:47.430
Nilesh Gupta: let me share my screen.

107
00:12:48.110 --> 00:12:49.120
Nilesh Gupta: What about now?

108
00:12:51.660 --> 00:12:53.060
Yuhan Gao: Yeah.

109
00:12:53.570 --> 00:12:55.740
Nilesh Gupta: Okay. So

110
00:12:55.920 --> 00:12:57.080
Nilesh Gupta: sorry.

111
00:12:58.640 --> 00:13:09.130
Nilesh Gupta: I think. Then i'll go over again. This thing. This was the page that I was sure trying to show. I think it was only the slide was shading.

112
00:13:09.280 --> 00:13:14.530
Nilesh Gupta: So there was a demonstration. You can just play with this. and

113
00:13:14.540 --> 00:13:19.310
Nilesh Gupta: basically here it just takes the sample data set.

114
00:13:19.490 --> 00:13:29.100
Nilesh Gupta: and it like initializes the weight matrices of this small neural network, and it has 4 options. One of the option is 0, the other option is too small.

115
00:13:29.130 --> 00:13:39.510
Nilesh Gupta: and the option is appropriate. And then there is 2 large. So when we have 0. Initialization, like everything, is symmetric. our loss, and this is the loss of the loss it doesn't go down.

116
00:13:39.670 --> 00:13:46.480
Nilesh Gupta: and the decision boundary is also like there's nothing much in distinguishes blue and red points.

117
00:13:46.820 --> 00:13:49.270
Nilesh Gupta: and we have 2 small initializations.

118
00:13:49.390 --> 00:13:54.010
Nilesh Gupta: Initially, we don't make any progress eventually like. We will

119
00:13:54.190 --> 00:14:05.630
Nilesh Gupta: make progress after some time. So So that's what we suffer when we have 2 small installation that we get convergence after a very long time. So

120
00:14:07.930 --> 00:14:20.910
Nilesh Gupta: is really right now, like there are some gradients which start to appear, and the loss is like it's starting to go down, and even the difference is that you it it starts appearing something which is sensible.

121
00:14:21.050 --> 00:14:24.400
Nilesh Gupta: like it is separating the red and blue points. So

122
00:14:24.600 --> 00:14:29.620
Nilesh Gupta: that's the problem we have when we have 2 small of initializations.

123
00:14:29.680 --> 00:14:33.850
Nilesh Gupta: and when we have appropriate initialization. like we see that.

124
00:14:34.010 --> 00:14:36.830
Nilesh Gupta: like, we make quite quick progress

125
00:14:37.190 --> 00:14:39.320
Nilesh Gupta: and think. Start

126
00:14:39.470 --> 00:14:45.130
Nilesh Gupta: to like the network starts learning pretty quickly, and it starts learning the right things.

127
00:14:45.380 --> 00:14:46.180
Nilesh Gupta: So you

128
00:14:47.090 --> 00:15:00.420
Nilesh Gupta: that's what you want. You want an appropriate initialization, and then we have 2 large of initialization. Then, like, yeah, things are chaotic like it's just oscillating, and the training doesn't never converges.

129
00:15:00.760 --> 00:15:08.930
Nilesh Gupta: So you would. So you don't want to large to small or 0 institutions, want to appropriate initializations, and for appropriate, like one of the

130
00:15:10.970 --> 00:15:28.920
Nilesh Gupta: method which is very commonly used as a initialization which initializes from a normal distribution which has the mean 0, and the standard deviation is one over square root of N. Or the square of standard deviation is one over N. Here, N. Is the dimension size of your input.

131
00:15:29.700 --> 00:15:40.610
Nilesh Gupta: So this is their initialization. And once, when you have 0 initialization. what you see is like when you run the training, the distribution of activations across the layers

132
00:15:40.630 --> 00:15:41.400
Nilesh Gupta: is

133
00:15:41.460 --> 00:15:43.940
Nilesh Gupta: stumbled, and like it's

134
00:15:43.970 --> 00:15:46.020
Nilesh Gupta: That's it. It doesn't explode.

135
00:15:46.880 --> 00:15:50.570
Nilesh Gupta: and in contrast like, if you had initialized uniformly

136
00:15:50.900 --> 00:15:55.150
Nilesh Gupta: like initially, the variances

137
00:15:55.220 --> 00:16:01.120
Nilesh Gupta: like it's it's normal. But then, like it starts speaking up as you go across the layers.

138
00:16:01.220 --> 00:16:13.760
Nilesh Gupta: So that's that's you. You we don't want this because, like this will lead to exploding or patching gradients and saturation of your activation function.

139
00:16:13.930 --> 00:16:20.050
Nilesh Gupta: So that's why, like typically people initialize it, using Zv. At initialization

140
00:16:20.600 --> 00:16:21.220
so

141
00:16:21.980 --> 00:16:23.520
Nilesh Gupta: hopefully, like

142
00:16:23.930 --> 00:16:32.360
Nilesh Gupta: Even after this lecture you guys can play around with this blog and the visualizations it has. It's not just visual. It's actually training

143
00:16:32.590 --> 00:16:37.610
Nilesh Gupta: your network so like we can take a look, and it has like other descriptions, too.

144
00:16:37.680 --> 00:16:41.150
Nilesh Gupta: which i'm not going over in this lecture.

145
00:16:41.330 --> 00:16:50.670
Nilesh Gupta: But yeah, we can take a look later. So this was. That's all I wanted to discuss on.

146
00:16:50.820 --> 00:16:55.910
Nilesh Gupta: If there's any question you guys can ask on this, otherwise we can

147
00:16:55.950 --> 00:16:58.950
Nilesh Gupta: go forward and discuss.

148
00:17:02.620 --> 00:17:17.460
Zengqi Liu: Yes, I have questions, though, about that. How how to say it's a too large. It's. It's missed that every video in the a later and the past matrix is

149
00:17:17.550 --> 00:17:23.089
Zengqi Liu: to large, or there's a one since the 1 1

150
00:17:23.349 --> 00:17:28.630
Zengqi Liu: I always i'm the to to the one that that schedule over here that is too large.

151
00:17:31.210 --> 00:17:37.560
Nilesh Gupta: So, as I said, like how we define appropriate initialization is

152
00:17:37.570 --> 00:17:39.820
Nilesh Gupta: like you when you're

153
00:17:40.990 --> 00:17:47.900
Nilesh Gupta: kind of the distribution of your input. after after like processing it through your neural network, it remains the same.

154
00:17:47.970 --> 00:17:49.450
Nilesh Gupta: or so similar.

155
00:17:49.630 --> 00:17:53.660
Nilesh Gupta: so like. If it remains similar, then like, then we define that

156
00:17:54.090 --> 00:18:11.940
Nilesh Gupta: that's an appropriate initialization. So if it's increasing the distribution, or it is increasing the range of the values of the outputs it is taking. Then, like we start to say that, like we are this: Wait: Matrices are like 2 large, these values inside it

157
00:18:11.950 --> 00:18:15.350
Nilesh Gupta: that like increasing the range of the output.

158
00:18:16.590 --> 00:18:19.080
Nilesh Gupta: Okay, so does that make sense? Okay, yeah.

159
00:18:20.990 --> 00:18:21.670
Nilesh Gupta: Okay.

160
00:18:24.790 --> 00:18:26.930
Nilesh Gupta: Yes, so let's go ahead.

161
00:18:27.000 --> 00:18:29.530
Nilesh Gupta: And so

162
00:18:29.730 --> 00:18:32.440
Nilesh Gupta: just I'll briefly talk about

163
00:18:32.720 --> 00:18:40.400
Nilesh Gupta: image classification without Cnn. Before Cnn, so that in the homework, too, we have looked at

164
00:18:40.450 --> 00:18:47.200
Nilesh Gupta: one of the way to do it like okay, Which was a very basic way that we just flattened out the image.

165
00:18:47.250 --> 00:18:50.750
Nilesh Gupta: and we passed it through a fully connected layer.

166
00:18:51.000 --> 00:19:02.500
Nilesh Gupta: and we calculated our loss. That's one way to do it. But like as we'll see that that's not the either the most efficient way, or the

167
00:19:02.560 --> 00:19:03.980
Nilesh Gupta: most

168
00:19:04.830 --> 00:19:11.920
Nilesh Gupta: accurately of doing this, because, like images are special, they are not just the random values inside of it.

169
00:19:12.050 --> 00:19:15.080
Nilesh Gupta: They like, have local connectivity.

170
00:19:15.260 --> 00:19:17.040
Nilesh Gupta: and the

171
00:19:17.590 --> 00:19:22.230
Nilesh Gupta: the features that you can calculate at one local region

172
00:19:22.560 --> 00:19:27.010
Nilesh Gupta: the same features might be helpful at other regions as well.

173
00:19:27.030 --> 00:19:27.990
Nilesh Gupta: So

174
00:19:28.100 --> 00:19:37.040
Nilesh Gupta: yeah, that's one way of doing it. But people typically will before Cnn. They used to be a lot of and

175
00:19:37.190 --> 00:19:42.030
Nilesh Gupta: see like featured in the

176
00:19:42.110 --> 00:19:52.300
Nilesh Gupta: like lot of human effort needs to go in there to define what are the good set of input features, and then they use to learn some machine learning model which can

177
00:19:53.500 --> 00:19:58.880
Nilesh Gupta: classes like to take those features and make predictions. So on those features.

178
00:19:59.080 --> 00:20:04.570
Nilesh Gupta: So people typically you you the typical pipeline, used to involve

179
00:20:04.580 --> 00:20:15.740
Nilesh Gupta: like corner detection or interesting point detection. And then, on those interesting points people used to calculate some descriptors called shift descriptors.

180
00:20:15.890 --> 00:20:19.120
Nilesh Gupta: and then they use to construct a visual vocabulary, using them.

181
00:20:19.770 --> 00:20:24.860
Nilesh Gupta: and then, like using that, they transform the image into a feature, vector

182
00:20:25.090 --> 00:20:33.130
Nilesh Gupta: and then they run their classification. We'll call them, which, like, for the state of that cases like they'll use our Svm classification

183
00:20:33.380 --> 00:20:34.720
Nilesh Gupta: from.

184
00:20:34.880 --> 00:20:40.010
Nilesh Gupta: So that's how people typically use to do it. But after the

185
00:20:40.100 --> 00:20:43.930
Nilesh Gupta: breakthroughs during 2,012 or something.

186
00:20:44.210 --> 00:20:54.040
Nilesh Gupta: they started more using, more to like going more towards deep learning approaches which, like, in which, like you, only give your image.

187
00:20:54.250 --> 00:20:57.650
Nilesh Gupta: and you define your neural network in a smart way.

188
00:20:57.800 --> 00:20:59.910
Nilesh Gupta: and then, like cool.

189
00:21:00.390 --> 00:21:09.360
Nilesh Gupta: all this feature processing, the neural network can learn like when you train it on a good amount of data.

190
00:21:09.420 --> 00:21:13.160
Nilesh Gupta: So that's the kind of a

191
00:21:13.380 --> 00:21:21.750
Nilesh Gupta: how people you typically used to do it. And with Cnn's this

192
00:21:21.920 --> 00:21:26.940
Nilesh Gupta: and engineering got removed. So let's

193
00:21:27.010 --> 00:21:33.510
Nilesh Gupta: hmm. Look at the few of the data sets which kind of enabled the

194
00:21:33.740 --> 00:21:37.750
Nilesh Gupta: the research in these directions, like one very popular data set.

195
00:21:37.800 --> 00:21:40.250
Nilesh Gupta: is them in a state as it. It's

196
00:21:40.690 --> 00:21:47.190
Nilesh Gupta: kind of handwritten digit recognition. So there are total, 60,000 samples and 10

197
00:21:47.260 --> 00:21:50.950
Nilesh Gupta: classes Each of the 10 classes are belonging to

198
00:21:51.440 --> 00:21:52.580
Nilesh Gupta: 10 digits.

199
00:21:52.820 --> 00:21:55.290
Nilesh Gupta: And yeah, it's a 10min

200
00:21:55.300 --> 00:22:01.220
Nilesh Gupta: classification problem. It's a very classical data set. I don't think anybody uses it

201
00:22:01.350 --> 00:22:06.700
Nilesh Gupta: today. But till it was one of the

202
00:22:06.850 --> 00:22:13.530
Nilesh Gupta: earlier classification data sets which enable you to research. Yeah. I in this field.

203
00:22:14.060 --> 00:22:26.300
Nilesh Gupta: And here is some history of a classification accuracy on Einstein linear Classifiers used to get a tested out of 12. Then Svm's improve them, go by quite a lot.

204
00:22:26.490 --> 00:22:35.560
Nilesh Gupta: Initially, Neural network, like tuli or neural networks. We're working decent, but like they are one, not beating the

205
00:22:35.700 --> 00:22:38.540
Nilesh Gupta: but during 2,011 and 2,012

206
00:22:39.370 --> 00:22:47.070
Nilesh Gupta: larger Cnn. Popped up, and they kind of improved over even the best Svm results.

207
00:22:47.600 --> 00:22:48.420
Nilesh Gupta: So

208
00:22:49.500 --> 00:22:53.800
Nilesh Gupta: another data set which is very popular even

209
00:22:53.960 --> 00:22:58.150
Nilesh Gupta: today, I would say. and is the kind of

210
00:22:58.410 --> 00:23:00.810
Nilesh Gupta: the data set which here

211
00:23:01.050 --> 00:23:09.290
Nilesh Gupta: enabled, like large scale. Deep learning training of well on vision data sets is this image net data

212
00:23:09.370 --> 00:23:16.500
Nilesh Gupta: which consisted of 1,000 classes and about 1.2 million images. And

213
00:23:16.720 --> 00:23:26.390
Nilesh Gupta: this is kind of the standard data that I'm: sure. Like many of you might be already familiar with this. But yeah, I just wanted to briefly

214
00:23:28.520 --> 00:23:30.930
Nilesh Gupta: tell about this so.

215
00:23:32.210 --> 00:23:42.290
Nilesh Gupta: and the the results. History on this one is initially like the shallow network and classical machine learning approaches which were used, and.

216
00:23:42.350 --> 00:23:49.300
Nilesh Gupta: as you can see, the top, 5 errors were not great, like it was about 25% or something.

217
00:23:49.760 --> 00:23:52.340
Nilesh Gupta: but in 2,012. Alex net came.

218
00:23:52.530 --> 00:23:55.250
Nilesh Gupta: Alex Newton was

219
00:23:55.260 --> 00:23:56.180
Nilesh Gupta: kind of

220
00:23:56.980 --> 00:24:06.720
Nilesh Gupta: like eightly, a deep convolutional neural network which improved over the shallow networks by quite a lot at almost by 10% margin.

221
00:24:06.740 --> 00:24:12.340
Nilesh Gupta: and then improvements over Alex. That further reduce the

222
00:24:12.870 --> 00:24:13.840
That's Ted.

223
00:24:14.060 --> 00:24:16.320
Nilesh Gupta: And I think

224
00:24:17.960 --> 00:24:18.990
Nilesh Gupta: you

225
00:24:19.360 --> 00:24:28.100
Nilesh Gupta: in in today's world, I think this top 5 accuracy should be below point 5%. Not sure. But yeah.

226
00:24:28.470 --> 00:24:29.380
Nilesh Gupta: Oh.

227
00:24:29.500 --> 00:24:36.820
Nilesh Gupta: quite a lot of progress have been made, and we'll briefly look at the Alex. Night, vg. And resnet in this lecture.

228
00:24:37.060 --> 00:24:39.550
Nilesh Gupta: So no, I.

229
00:24:41.010 --> 00:24:55.150
Nilesh Gupta: Yeah, this is a yeah. This was the brief history on the image that. So let's talk about convolutional neural network. And So, as we have seen previously, like neural networks, are

230
00:24:55.340 --> 00:24:59.700
Nilesh Gupta: all the neural network that we have seen are densely connected. That is the day.

231
00:24:59.780 --> 00:25:13.260
Nilesh Gupta: like every new neuron in a layer like sees all the inputs in the previous layer. So these fully connected networks they empirically doesn't work well in computer vision applications.

232
00:25:13.980 --> 00:25:18.810
Nilesh Gupta: And Oh. no! So

233
00:25:19.010 --> 00:25:32.330
Nilesh Gupta: so! And all people often have like attributed this to the, to the special nature of vision applications that, like images, have a special

234
00:25:32.620 --> 00:25:34.960
Nilesh Gupta: structure in their input.

235
00:25:35.040 --> 00:25:40.420
Nilesh Gupta: and so like, they are fully connected. Neural networks are not the way

236
00:25:40.630 --> 00:25:42.700
Nilesh Gupta: to process them, so

237
00:25:43.160 --> 00:25:48.010
Nilesh Gupta: that's why. I think and around 1,998, or something

238
00:25:48.060 --> 00:25:50.070
Nilesh Gupta: in

239
00:25:50.730 --> 00:25:51.510
Nilesh Gupta: go

240
00:25:52.160 --> 00:25:57.350
Nilesh Gupta: no like the popular form in which we see Cnn's work kind of first proposed.

241
00:25:57.510 --> 00:26:01.540
Nilesh Gupta: And Cns mainly consist of 2 important layers.

242
00:26:01.690 --> 00:26:14.150
Nilesh Gupta: One is the convolution layer, and the other is the pooling layer. So they are kind of just stacking of these layers on top of each other. This is the structure of a Vg. Network, which is

243
00:26:14.660 --> 00:26:25.820
Nilesh Gupta: how popular Cnn Network will take a look later. But right now, like what I want to convey with this picture is like. It is just a stack of convolutional Max Pooling

244
00:26:25.920 --> 00:26:30.110
Nilesh Gupta: and I some activation functions and some fully connected layers.

245
00:26:31.230 --> 00:26:35.540
Nilesh Gupta: So let's they

246
00:26:36.100 --> 00:26:45.050
Nilesh Gupta: take a look like, try to motivate like what what the convolutional layer is, and take a deeper look. What this layer does.

247
00:26:45.760 --> 00:26:51.470
Nilesh Gupta: So, as I said, fully connected Layers have to first of all have too many parameters.

248
00:26:51.540 --> 00:26:58.450
Nilesh Gupta: and that leads to poor performance. For example, if you look at the Vg's first layer.

249
00:26:58.500 --> 00:27:03.350
Nilesh Gupta: if if there is a input image which is of size 224,

250
00:27:03.390 --> 00:27:06.820
Nilesh Gupta: and there are 3 channels which are the RGB channels.

251
00:27:06.940 --> 00:27:19.810
Nilesh Gupta: and they match. and if you want to produce a for every data location you want to produce, like 64 channels like 2, 24, plus 2, 24 plus 64 outputs.

252
00:27:20.660 --> 00:27:25.030
Nilesh Gupta: So like, if you had to take a dense neural network.

253
00:27:25.180 --> 00:27:31.380
Nilesh Gupta: your number of parameters will be very large, like it will be about 500 billion This thing

254
00:27:31.390 --> 00:27:36.330
2, 24 cost to 24, plus 3 cost to 24 plus 2 24 plus 64

255
00:27:36.760 --> 00:27:37.610
Nilesh Gupta: so

256
00:27:37.770 --> 00:27:40.960
Nilesh Gupta: it's it's it's just too many parameters

257
00:27:41.050 --> 00:27:48.260
Nilesh Gupta: So convolation layer with convolutional layer will still be like producing this output

258
00:27:48.270 --> 00:27:51.730
Nilesh Gupta: 2, 24 plus 204, plus 64, but will be

259
00:27:51.810 --> 00:28:03.390
Nilesh Gupta: producing them in a very efficient way, and will be also like. Try to capture the local connectivities inside the image, and we'll be sharing parameters

260
00:28:03.460 --> 00:28:06.330
Nilesh Gupta: inside the neural network to compute this.

261
00:28:07.590 --> 00:28:11.090
Nilesh Gupta: So so. so.

262
00:28:13.520 --> 00:28:17.880
Nilesh Gupta: so by local connectivity, like what first I want.

263
00:28:19.970 --> 00:28:21.840
Nilesh Gupta: What I wanted to say was that

264
00:28:22.070 --> 00:28:25.110
Nilesh Gupta: in an image, basically

265
00:28:25.130 --> 00:28:26.550
Nilesh Gupta: what we want is

266
00:28:26.940 --> 00:28:31.060
Nilesh Gupta: each hidden unit to be connected to only a sub region of input

267
00:28:31.350 --> 00:28:32.860
Nilesh Gupta: like. We don't want

268
00:28:33.270 --> 00:28:42.830
Nilesh Gupta: that that. But the hidden neuron is looking at all of the image at once, like we wanted to just look at maybe a sub region of

269
00:28:42.880 --> 00:28:43.730
Nilesh Gupta: the input

270
00:28:43.950 --> 00:28:46.830
Nilesh Gupta: and usually

271
00:28:47.230 --> 00:28:54.810
Nilesh Gupta: like. We can also treat each of the RGB channels independently. but usually like our Gp chance, or also

272
00:28:54.930 --> 00:28:56.460
Nilesh Gupta: like

273
00:28:56.730 --> 00:29:02.640
Nilesh Gupta: we are the when we are defining the sub region of Input we are only defining spatially like we are.

274
00:29:02.810 --> 00:29:06.840
Nilesh Gupta: We are looking at RGB. Channels together. So

275
00:29:07.550 --> 00:29:14.460
Nilesh Gupta: this is the kind of one demonstration where, like we are seeing this patch inside the input

276
00:29:14.660 --> 00:29:19.370
Nilesh Gupta: at, and you are seeing it across the all of the 3 channels

277
00:29:19.560 --> 00:29:30.680
Nilesh Gupta: and computing this one convolution. And we are. I computing the activations of that hidden neuron

278
00:29:30.730 --> 00:29:37.040
Nilesh Gupta: on this particular patch. So the difference between a fully connected and

279
00:29:37.170 --> 00:29:43.060
Nilesh Gupta: the locally connected header neuron is kind of defected in this picture.

280
00:29:43.110 --> 00:29:44.040
Nilesh Gupta: Yeah.

281
00:29:44.410 --> 00:29:47.950
Nilesh Gupta: like, instead of looking at the whole image like we are looking at

282
00:29:48.090 --> 00:29:50.620
Nilesh Gupta: just some separate sub regions.

283
00:29:50.810 --> 00:29:51.650
Nilesh Gupta: So

284
00:29:54.500 --> 00:29:58.990
Nilesh Gupta: so how do we do this like we before I

285
00:30:00.430 --> 00:30:01.310
Nilesh Gupta: do. You

286
00:30:03.350 --> 00:30:04.110
Nilesh Gupta: let me see.

287
00:30:05.240 --> 00:30:07.140
Nilesh Gupta: Yeah. So

288
00:30:07.610 --> 00:30:16.580
Nilesh Gupta: the idea is that we want to make some assumption that is, if one feature is useful to compute at some spatial position. Xy.

289
00:30:16.840 --> 00:30:22.990
Nilesh Gupta: Then it should also be useful to compute at a different position. X 2 comma by 2, and

290
00:30:25.540 --> 00:30:30.820
Nilesh Gupta: this is because, like in images like, let's say, like the feature that you are calculating is

291
00:30:30.870 --> 00:30:33.120
Nilesh Gupta: some edge. Then.

292
00:30:34.470 --> 00:30:38.840
Nilesh Gupta: like defining the edge at one location like you, you can like

293
00:30:39.170 --> 00:30:40.530
Nilesh Gupta: just have

294
00:30:41.120 --> 00:30:44.990
Nilesh Gupta: same set of weights which can detect edges at

295
00:30:45.500 --> 00:30:48.740
Nilesh Gupta: right different locations inside the image.

296
00:30:48.900 --> 00:30:53.980
Nilesh Gupta: So this is done using the convolution operator.

297
00:30:54.450 --> 00:30:55.740
Nilesh Gupta: And

298
00:30:55.950 --> 00:30:56.660
hmm

299
00:30:56.790 --> 00:31:03.090
Nilesh Gupta: like. i'll define the convolution operation here. So so

300
00:31:03.470 --> 00:31:10.010
Nilesh Gupta: the convolution of an image X with a kernel K is computed as this Here, like

301
00:31:10.040 --> 00:31:12.610
Nilesh Gupta: the weight, my my matrix that we see

302
00:31:14.070 --> 00:31:16.610
Nilesh Gupta: we call it the kernel K.

303
00:31:16.740 --> 00:31:22.350
Nilesh Gupta: It is different than the kernel that we saw in a second like it's not

304
00:31:22.650 --> 00:31:25.140
Nilesh Gupta: out connected to that. But like

305
00:31:25.700 --> 00:31:31.980
Nilesh Gupta: well, let's say that this was the image of 3 Cross 3

306
00:31:32.080 --> 00:31:32.890
Nilesh Gupta: great.

307
00:31:33.020 --> 00:31:39.160
Nilesh Gupta: and these were the values, and this was my kernel, which is a 2 cross, 2 matrix

308
00:31:41.190 --> 00:31:45.190
Nilesh Gupta: having these values, 1.5 point, 2, 5, and 0.

309
00:31:45.680 --> 00:31:51.960
Nilesh Gupta: So what we'll do is well like. Place the kernel on top of the first

310
00:31:52.100 --> 00:31:56.120
Nilesh Gupta: to cross, to sub region on site, the gray input image

311
00:31:56.220 --> 00:32:02.420
Nilesh Gupta: and we'll compute the element, wise product between the kernel weights and the input, image

312
00:32:02.570 --> 00:32:06.070
Nilesh Gupta: values, and then sum them all up.

313
00:32:06.300 --> 00:32:09.670
Nilesh Gupta: So here in this case, like we'll multiply one times one

314
00:32:09.850 --> 00:32:16.610
Nilesh Gupta: and point 2 times, point 5, point 2 times, point 2 5 0 times 0, and some mall summit all up.

315
00:32:16.850 --> 00:32:18.910
Nilesh Gupta: so we'll get 1.1 5,

316
00:32:19.340 --> 00:32:22.070
Nilesh Gupta: Then we'll just slide it across

317
00:32:23.580 --> 00:32:33.900
Nilesh Gupta: by one step. and well do the same computation again. and then calculate the value in this case like this way, this will turn out to be 4. Point 5.

318
00:32:34.160 --> 00:32:37.050
Nilesh Gupta: Next will go down like the.

319
00:32:37.540 --> 00:32:39.660
Nilesh Gupta: and we'll do the same computation.

320
00:32:39.880 --> 00:32:44.110
Nilesh Gupta: and this will give us a value point 2, 5,

321
00:32:44.490 --> 00:32:51.320
Nilesh Gupta: and in the next case, like we do the same operation again. Just we slide it one step to the right.

322
00:32:51.650 --> 00:32:57.910
Nilesh Gupta: and perform the same operation and compute this value. So

323
00:32:59.250 --> 00:33:03.750
Nilesh Gupta: any questions to this point, though.

324
00:33:04.910 --> 00:33:09.210
Nilesh Gupta: How about conversations or anything that we have discussed so far?

325
00:33:27.200 --> 00:33:27.960
Nilesh Gupta: Okay.

326
00:33:28.040 --> 00:33:35.550
Nilesh Gupta: So notice that, like we are using the same set of weights, just that we are applying it different regions inside our

327
00:33:36.010 --> 00:33:49.010
Nilesh Gupta: image or input grid. and we are computing different values. So this is the main operation behind convolutional networks that we have this

328
00:33:49.380 --> 00:33:53.070
Nilesh Gupta: idea of kernel matrices where we take the kernel

329
00:33:53.160 --> 00:33:57.870
Nilesh Gupta: and move it across the image and compute the output values.

330
00:33:58.330 --> 00:33:59.130
Nilesh Gupta: So

331
00:34:01.040 --> 00:34:01.680
set.

332
00:34:02.700 --> 00:34:06.190
Nilesh Gupta: Yeah. So if you take a look at the

333
00:34:06.780 --> 00:34:13.250
Nilesh Gupta: slightly bigger example that if we had this kind of image where we have white values

334
00:34:13.449 --> 00:34:17.400
Nilesh Gupta: on these diagonal and these

335
00:34:17.610 --> 00:34:28.520
Nilesh Gupta: great points and 0 values everywhere else. And we take this kernel. This kernel kind of looks like it is time to detect diagonal edge.

336
00:34:29.090 --> 00:34:29.929
Nilesh Gupta: So

337
00:34:31.320 --> 00:34:34.040
Nilesh Gupta: if we, if we apply this kernel to

338
00:34:34.150 --> 00:34:38.730
Nilesh Gupta: our input image, like it will give us output which will look something like this.

339
00:34:40.980 --> 00:34:47.830
Nilesh Gupta: And, as you can see, like where the data. Points belong to some

340
00:34:50.290 --> 00:34:55.870
Nilesh Gupta: kind of belong to on a are part of a diagnos there, like

341
00:34:55.920 --> 00:34:58.450
Nilesh Gupta: the kernel, will produce a high value.

342
00:34:58.520 --> 00:35:05.730
Nilesh Gupta: and otherwise like it would produce a low value. So it kind of what intuitively, what we can

343
00:35:05.810 --> 00:35:11.940
Nilesh Gupta: think of it, that this kernel is trying to detect detect a diagonal edge inside the image.

344
00:35:16.030 --> 00:35:18.040
Nilesh Gupta: Okay. So

345
00:35:20.480 --> 00:35:23.570
Nilesh Gupta: usually when we take convolutions

346
00:35:23.780 --> 00:35:29.230
Nilesh Gupta: after taking convolutions, we also like, pass it through activation function

347
00:35:29.300 --> 00:35:37.220
Nilesh Gupta: most of the time. It's really in this example. I think it's a sigmoid function, but

348
00:35:38.330 --> 00:35:45.190
Nilesh Gupta: so, but it's nothing more like as a similar to standard neural networks.

349
00:35:45.250 --> 00:35:47.920
Nilesh Gupta: Whenever we are passing through

350
00:35:47.990 --> 00:35:49.320
Nilesh Gupta: like the

351
00:35:49.390 --> 00:35:54.760
Nilesh Gupta: M. Lt. Lay like we had one activation so similarly like. Here

352
00:35:54.850 --> 00:35:58.810
Nilesh Gupta: the take the convolution

353
00:35:58.940 --> 00:36:04.410
Nilesh Gupta: and then pass it through element, element, wise activation function.

354
00:36:04.840 --> 00:36:05.680
Nilesh Gupta: So

355
00:36:07.270 --> 00:36:15.410
Nilesh Gupta: now here it's. This is just kind of a visualization of some of the kernel that

356
00:36:15.470 --> 00:36:19.350
Nilesh Gupta: learned by the Alex net model. Much.

357
00:36:20.710 --> 00:36:23.310
Nilesh Gupta: Oh, rich as it is. as I

358
00:36:23.360 --> 00:36:26.180
Nilesh Gupta: showed earlier, was one of the

359
00:36:26.500 --> 00:36:31.900
Nilesh Gupta: first, the Cnn based neural networks, which kind of outperform.

360
00:36:31.930 --> 00:36:32.560
Hmm.

361
00:36:32.600 --> 00:36:41.590
Nilesh Gupta: The standard methods by quite a lot of margin. So yeah, in their paper they had visualized like what kind of kernel set is learning.

362
00:36:41.760 --> 00:36:48.360
Nilesh Gupta: And as you can see that these are somewhat meaningful kernels, at least on a

363
00:36:48.540 --> 00:36:55.060
Nilesh Gupta: like a low, level features. These seem to represent that some are trying to learn edges

364
00:36:55.250 --> 00:36:58.560
Nilesh Gupta: at some particular places; some are trying to learn

365
00:36:58.610 --> 00:37:04.230
Nilesh Gupta: some color separation between the bottom half and the top half.

366
00:37:04.490 --> 00:37:05.470
Nilesh Gupta: So.

367
00:37:06.640 --> 00:37:07.930
Nilesh Gupta: yeah, as you can.

368
00:37:10.350 --> 00:37:13.470
Nilesh Gupta: so like kernels can have

369
00:37:13.560 --> 00:37:16.480
Nilesh Gupta: like particular meanings if you try to visualize it

370
00:37:16.490 --> 00:37:25.660
Nilesh Gupta: most of the time like, since at least in these days, like, we use so many colors that it's hard to kind of pinpoint. What exactly

371
00:37:25.740 --> 00:37:30.960
Nilesh Gupta: that kernel is doing learning, but like if you have few more kernels, some meaningful law

372
00:37:31.140 --> 00:37:34.870
Nilesh Gupta: patents can emerge. So

373
00:37:35.970 --> 00:37:38.890
Nilesh Gupta: another thing is the padding.

374
00:37:38.970 --> 00:37:43.810
Nilesh Gupta: so as you, as you would have seen in this example, we started with a five-

375
00:37:43.890 --> 00:37:46.170
Nilesh Gupta: and our kernel was the 2 cost 2 thing.

376
00:37:46.290 --> 00:37:53.020
Nilesh Gupta: So when you slide it across the 5 first 5 grid like it will produce output which will be 4 cost, for

377
00:37:53.040 --> 00:38:06.280
Nilesh Gupta: so it is kind of shrinking Your output like it does not allow your output to be the same size with your input. So to overcome that people typically what they do is like, they 0 pad

378
00:38:06.420 --> 00:38:07.280
Nilesh Gupta: the

379
00:38:07.290 --> 00:38:10.940
Nilesh Gupta: image with 0 values, and then apply the kernels

380
00:38:11.010 --> 00:38:21.300
Nilesh Gupta: so like, Suppose like, if this was the 5 plus 5 image. I have so to produce the same 5 plus 5 output. Using that kernel. I got it

381
00:38:21.670 --> 00:38:25.800
Nilesh Gupta: just 0 pad the image and make it a 7 cross 7.

382
00:38:26.020 --> 00:38:27.250
Nilesh Gupta: I it's good.

383
00:38:27.260 --> 00:38:31.100
Nilesh Gupta: and this will now allow me to have

384
00:38:31.360 --> 00:38:32.850
Nilesh Gupta: 5 plus 5 output.

385
00:38:33.270 --> 00:38:39.910
Nilesh Gupta: So it's just a trip with which you can it? It becomes easier to control the size of the output layer.

386
00:38:41.320 --> 00:38:43.380
Nilesh Gupta: So any questions so far.

387
00:38:55.620 --> 00:38:56.310
Nilesh Gupta: Okay.

388
00:38:58.240 --> 00:38:59.090
Nilesh Gupta: So

389
00:39:00.400 --> 00:39:07.990
Nilesh Gupta: another thing which I term that which often comes when discussing convolution, says the stride.

390
00:39:08.260 --> 00:39:12.040
Nilesh Gupta: it's the amount of movement between applications of the filter

391
00:39:12.090 --> 00:39:14.470
Nilesh Gupta: on the input image. So on.

392
00:39:14.860 --> 00:39:19.130
Nilesh Gupta: on all of these examples that I we have been seeing like we were

393
00:39:19.240 --> 00:39:23.770
Nilesh Gupta: moving the kernel just by one unit to the right or to the bottom.

394
00:39:23.860 --> 00:39:29.290
Nilesh Gupta: But you can move the kernel more more than by one unit. So

395
00:39:30.050 --> 00:39:33.820
Nilesh Gupta: if you have a stride of 2 comma 2.

396
00:39:33.920 --> 00:39:36.060
Nilesh Gupta: Then it like your

397
00:39:36.080 --> 00:39:43.030
Nilesh Gupta: at time, like you're moving by 2 steps to the right, and when you are going down also, like you are moving 2 steps to the bottom.

398
00:39:43.480 --> 00:39:47.580
Nilesh Gupta: So so is it. So

399
00:39:47.660 --> 00:39:52.990
Nilesh Gupta: when you have larger strides, the output that you will you will produce is going to be smaller.

400
00:39:53.000 --> 00:39:53.840
Nilesh Gupta: because.

401
00:39:55.300 --> 00:39:56.460
Nilesh Gupta: like, it will be

402
00:39:56.490 --> 00:39:59.450
Nilesh Gupta: covering the image and you less number of steps.

403
00:40:01.010 --> 00:40:12.150
Nilesh Gupta: So this people use strides to kind of down sample the input that they have the like. If they want a lower resolution of the input they.

404
00:40:12.180 --> 00:40:13.960
Nilesh Gupta: we'll use the largest right.

405
00:40:14.250 --> 00:40:18.320
Nilesh Gupta: and it will produce a lower resolution. But

406
00:40:20.420 --> 00:40:21.980
Nilesh Gupta: so okay.

407
00:40:23.110 --> 00:40:28.400
Nilesh Gupta: coming up to the last part inside the conversation neural network, which is the pooling

408
00:40:28.510 --> 00:40:31.030
Nilesh Gupta: pooling, is

409
00:40:31.440 --> 00:40:41.410
Nilesh Gupta: very simple operator, which it takes looks at the but a subgrid of your input. and it

410
00:40:41.880 --> 00:40:46.160
Nilesh Gupta: kind of aggregate the values inside that subgrade.

411
00:40:46.270 --> 00:40:55.050
Nilesh Gupta: So a very common pooling operator is the Max pooling. So what it will do is like. Suppose, if you have a 4 crossed for image Grid. it will.

412
00:40:56.750 --> 00:41:07.900
Nilesh Gupta: it will pick out the and you are pooling with a filter of 2 cross 2. Then it will like, go over this 2 plus 2 grid. and it will pick the maximum value.

413
00:41:08.110 --> 00:41:11.430
Nilesh Gupta: and then, like, if your side is to like. It will move

414
00:41:11.490 --> 00:41:17.000
Nilesh Gupta: 2 steps to the right and at Well, then, look at this green colored subgrade.

415
00:41:17.010 --> 00:41:20.100
Nilesh Gupta: and it will pick out the maximum value. So

416
00:41:22.350 --> 00:41:29.040
Nilesh Gupta: pooling is kind of used to reduce the size of representation similar to the strides

417
00:41:29.060 --> 00:41:30.890
Nilesh Gupta: thing. but it

418
00:41:30.900 --> 00:41:39.570
Nilesh Gupta: it kind of makes the robust the model robust to the exact special location of features that is like.

419
00:41:40.040 --> 00:41:50.670
Nilesh Gupta: because, like images like can be easily be slightly shifted in one or the other direction. So it's kind of when you are Max pooling it right. It

420
00:41:50.680 --> 00:41:51.680
Nilesh Gupta: makes

421
00:41:51.840 --> 00:41:57.120
Nilesh Gupta: the output of your network robust to the small deviations in the values.

422
00:41:59.300 --> 00:42:00.250
Nilesh Gupta: So

423
00:42:00.740 --> 00:42:10.260
Nilesh Gupta: Okay, so these were kind of the main components inside conventional. But now let's see, like how typically they are stacked inside a Cnn.

424
00:42:10.460 --> 00:42:12.970
Nilesh Gupta: And how it is.

425
00:42:13.140 --> 00:42:15.740
Nilesh Gupta: Oh. yeah.

426
00:42:15.980 --> 00:42:19.870
Nilesh Gupta: how it works all together. So this is

427
00:42:20.760 --> 00:42:32.470
Nilesh Gupta: gain. A very nice visualization which lets you visualize a small convolutional neural network on some sample images. So it consists of it

428
00:42:32.790 --> 00:42:46.460
Nilesh Gupta: the like 3 layers of convolutions. actually 4 layers of convolutions. and and then finally, it has a fully connected layer, and then, like for loose by the soft Max.

429
00:42:46.630 --> 00:42:48.810
Nilesh Gupta: So, as you can see, this is

430
00:42:50.480 --> 00:42:57.080
Nilesh Gupta: this convolution. This is, I think, a 3 plus 3 convolution. So what is doing is.

431
00:42:57.170 --> 00:43:05.320
Nilesh Gupta: it will see. I don't know if you guys can see this clearly or not, but what it is doing it like. If you it is producing this output

432
00:43:05.530 --> 00:43:12.610
Nilesh Gupta: by taking convolution over a 3 cross, 3 subgrade and then multiplying it by the kernel matrix.

433
00:43:12.950 --> 00:43:18.320
Nilesh Gupta: And then, like this is the value that you get in this case like this is minus point 4,

434
00:43:18.860 --> 00:43:23.430
Nilesh Gupta: you can play it on. But yeah.

435
00:43:24.090 --> 00:43:24.920
Nilesh Gupta: so

436
00:43:25.880 --> 00:43:35.730
Nilesh Gupta: so it in this visualization it is shown that, like the convolution is happening independently on the RGB. Channels, like the 3 channels that we have here.

437
00:43:35.740 --> 00:43:36.510
Nilesh Gupta: but, like

438
00:43:36.630 --> 00:43:40.910
Nilesh Gupta: it, is later summing them up. So what essentially is happening is like

439
00:43:40.940 --> 00:43:47.780
Nilesh Gupta: it is applying the convolution operator. Oh. on all of the RGB. Channels together.

440
00:43:47.950 --> 00:43:51.980
Nilesh Gupta: But like just for visualization, like they have shown it independently. That

441
00:43:52.210 --> 00:44:01.700
Nilesh Gupta: Thank you. It looking at. So I say, suppose the our channel and then applying convolution. and then it is giving you

442
00:44:01.770 --> 00:44:03.170
Nilesh Gupta: output.

443
00:44:03.500 --> 00:44:05.630
Nilesh Gupta: and then you are doing same thing on the

444
00:44:06.350 --> 00:44:14.630
Nilesh Gupta: the other channels, and then the outputs are. Then you sum it up, and then you add a bias. and this gives you the output

445
00:44:15.090 --> 00:44:18.650
Nilesh Gupta: after the first convolution layer. So.

446
00:44:19.850 --> 00:44:29.400
Nilesh Gupta: going back, as you can see after the convolution, we have this value. So value is just Max of 0 comma the input value that you get. So

447
00:44:29.460 --> 00:44:35.630
Nilesh Gupta: this is the standard value operator. Then you again have this convolution layer

448
00:44:35.940 --> 00:44:39.170
Nilesh Gupta: which is looking at all of the channels of the previous clear.

449
00:44:40.060 --> 00:44:49.960
Nilesh Gupta: and then, like it, is performing 3 plus 3 convolution. and that is giving you the results. And then you are coming up the values from all the chance together.

450
00:44:50.020 --> 00:44:52.390
Nilesh Gupta: and adding the bias. And this gives you

451
00:44:52.920 --> 00:44:58.750
Nilesh Gupta: the output of the this conversation there. So here again, after convolution you have a value.

452
00:44:58.900 --> 00:45:09.750
Nilesh Gupta: And then there is this Max Pooling operator here, like it is a 2 cost to Max Pooling. so like it is looking at a 2 plus 2 mini grid.

453
00:45:09.770 --> 00:45:12.620
Nilesh Gupta: and then picking out the maximum value, and that is

454
00:45:12.920 --> 00:45:15.960
Nilesh Gupta: giving the output of the Max pulling thing

455
00:45:17.160 --> 00:45:22.200
Nilesh Gupta: so similarly like it is followed by convolution there and then

456
00:45:22.750 --> 00:45:28.990
Nilesh Gupta: another convolution layer, followed by Max Pooling, and at the end. Here you have a fully connected layer which is

457
00:45:29.170 --> 00:45:30.580
Nilesh Gupta: connected to

458
00:45:31.160 --> 00:45:38.850
Nilesh Gupta: all of the outputs here, like basically like you are taking all of these outputs, flattening them. And then you have a fully connected layer.

459
00:45:39.570 --> 00:45:49.680
Nilesh Gupta: and at the end, of because, like this is a multi-class license. You are also applying the soft Mac non-linearity to convert them into probabilities.

460
00:45:50.790 --> 00:45:54.560
Nilesh Gupta: So hopefully this visualization make things a bit clear

461
00:45:54.740 --> 00:45:57.110
Nilesh Gupta: how everything is tagged up

462
00:45:57.150 --> 00:46:03.300
Nilesh Gupta: inside the convolutionally new network. So any questions so far.

463
00:46:18.230 --> 00:46:19.050
Nilesh Gupta: alright.

464
00:46:21.280 --> 00:46:22.200
Nilesh Gupta: So

465
00:46:24.900 --> 00:46:28.380
Nilesh Gupta: now we'll just look at like. Oh.

466
00:46:28.660 --> 00:46:32.520
Nilesh Gupta: 3 of the popular convolutional neural networks.

467
00:46:32.830 --> 00:46:34.680
Nilesh Gupta: and like

468
00:46:34.730 --> 00:46:36.620
Nilesh Gupta: see their specifics.

469
00:46:36.810 --> 00:46:38.790
Nilesh Gupta: So this

470
00:46:39.160 --> 00:46:49.170
Nilesh Gupta: Leonette was the, as I said. it was kind of one of the first successful convolutional neural networks which was able to do

471
00:46:49.220 --> 00:46:55.670
Nilesh Gupta: well on the seminar data set. It consisted of 5 layers. The first layer was this.

472
00:46:56.800 --> 00:47:12.120
Nilesh Gupta: it's a convolutional layer. That's 5 cross, 5 filters and right one. And then there was this: the Max Pooling operator, 2 plus 2, Max pulling operator and convolution, and then pooling, and then finally followed by 3 fully connected players.

473
00:47:12.470 --> 00:47:18.620
Nilesh Gupta: So it was further improved by Alex Net. Like it got bigger.

474
00:47:18.940 --> 00:47:26.410
Nilesh Gupta: It it kind of is very similar to Lena, but just that it's a bigger network, as it clears

475
00:47:26.510 --> 00:47:37.950
Nilesh Gupta: total 60 million parameters. And oh. it was the the network which kind of oh! Led to

476
00:47:39.380 --> 00:47:52.010
Nilesh Gupta: like confidence in deep learning that it can actually like. Given an update, it can learn very meaningful things, and or can out perform the the hand engineered features.

477
00:47:52.900 --> 00:47:53.990
Nilesh Gupta: So

478
00:47:54.290 --> 00:48:00.410
Nilesh Gupta: at all Alex net was fall, like Vg. Network, which we saw earlier.

479
00:48:00.640 --> 00:48:10.210
Nilesh Gupta: just made it bigger like it. Time Again, it is very similar to the rest of the convolutional neural networks, such that

480
00:48:10.290 --> 00:48:17.930
Nilesh Gupta: it's the design is slightly different. and it made it bigger and more layers. So

481
00:48:19.700 --> 00:48:24.850
Nilesh Gupta: for the interest of time I won't. Go into these calculations

482
00:48:24.950 --> 00:48:36.450
Nilesh Gupta: of what the weights inside the which is a network. But maybe you guys can take a look later. So i'll briefly talk about like the it

483
00:48:36.550 --> 00:48:43.660
Nilesh Gupta: kind of the receptive field of a neuron. and like what actually like, these filters are trying to learn. So

484
00:48:43.950 --> 00:48:48.580
Nilesh Gupta: basically. The we say that the receptor field

485
00:48:49.420 --> 00:48:59.060
Nilesh Gupta: of a neuron is the input region that can affect the output of its neurons. So if if you look at just the first lay the first layer.

486
00:48:59.540 --> 00:49:03.920
Nilesh Gupta: Yeah. First layer neuron K is only looking like

487
00:49:03.970 --> 00:49:06.210
Nilesh Gupta: at the

488
00:49:06.610 --> 00:49:15.110
Nilesh Gupta: subgroup of the kernel size. So it's a receptive field is only the oh. the

489
00:49:17.270 --> 00:49:24.680
Nilesh Gupta: the size of the kernel that it's using. So it's capturing only very local patterns. But as you go higher and higher.

490
00:49:24.850 --> 00:49:35.140
Nilesh Gupta: so as the like. If you look at this output, this output depends on like, let's say, like we are looking at a 3 plus 3 kernel size convolution.

491
00:49:35.230 --> 00:49:40.580
Nilesh Gupta: then this output is so dependent on

492
00:49:41.650 --> 00:49:47.190
Nilesh Gupta: 3 cross, 3 input, pixels. So if you apply a 3 plus 3 convolution on the

493
00:49:47.480 --> 00:49:49.170
Nilesh Gupta: on the B

494
00:49:50.880 --> 00:49:51.710
Nilesh Gupta: Yeah.

495
00:49:53.070 --> 00:49:54.270
Nilesh Gupta: then, like

496
00:49:54.630 --> 00:50:00.680
Nilesh Gupta: now, you will be looking at a much larger area, because, like since each of the element inside the B tensor

497
00:50:00.730 --> 00:50:07.360
Nilesh Gupta: is looking at a 3 course, 3 input grid of elements of the a 10%. So

498
00:50:07.550 --> 00:50:14.890
Nilesh Gupta: so for higher layer neurons the receptive field becomes larger and larger, and at the end like what we hope is that, like

499
00:50:14.910 --> 00:50:19.250
Nilesh Gupta: it starts to capture the global patterns which exist in the image.

500
00:50:19.970 --> 00:50:22.840
Nilesh Gupta: So, in a sense, like

501
00:50:23.090 --> 00:50:27.960
Nilesh Gupta: Cnn is kind of flow and a very hierarchical pattern. Oh.

502
00:50:28.780 --> 00:50:32.840
Nilesh Gupta: over the input features that they learn. And that's what makes them

503
00:50:33.080 --> 00:50:38.560
Nilesh Gupta: the very effective one on these image problems.

504
00:50:40.110 --> 00:50:45.290
Nilesh Gupta: So typically you will train this using

505
00:50:46.180 --> 00:50:56.130
Nilesh Gupta: our standard S. Gd. Or Adam optimizes. and these back propagations, like all the operations inside convolution layer.

506
00:50:56.300 --> 00:50:57.450
Nilesh Gupta: I don't know.

507
00:50:57.530 --> 00:51:04.600
Nilesh Gupta: like you can pass in to them. so you can apply your typical deep learning pipeline

508
00:51:04.840 --> 00:51:06.980
Nilesh Gupta: to train them. and

509
00:51:08.170 --> 00:51:19.820
Nilesh Gupta: oh. usually like as you as as we saw that, like these networks, like just became bigger and bigger as

510
00:51:21.810 --> 00:51:29.000
Nilesh Gupta: as we went from Linet to Alex to to Vg: so when you have so many parameters, it becomes easy to overset.

511
00:51:29.300 --> 00:51:30.730
Nilesh Gupta: So we

512
00:51:31.420 --> 00:51:37.840
Nilesh Gupta: how there are multiple things that people do to avoid that over fitting.

513
00:51:37.890 --> 00:51:40.450
Nilesh Gupta: and so that let the model generalize as well.

514
00:51:41.050 --> 00:51:44.220
Nilesh Gupta: One of the important thing is drop out.

515
00:51:44.280 --> 00:51:49.530
Nilesh Gupta: so drop out as a very simple concept. But so

516
00:51:50.490 --> 00:51:54.000
Nilesh Gupta: her works very effectively, and like

517
00:51:54.410 --> 00:52:08.100
Nilesh Gupta: It's kind of used in all the networks that you will see in today's era. So what it does is like it's with some probability. What you want to do is, you want to just drop some connections when you are doing your forward pass.

518
00:52:08.380 --> 00:52:13.640
Nilesh Gupta: So like, let's say like this is the standard neural network in which everything is connected to everything.

519
00:52:13.800 --> 00:52:15.860
Nilesh Gupta: So if you

520
00:52:15.980 --> 00:52:27.200
Nilesh Gupta: let's say like you drop out like there is. is the probability of dropping out a hyper of the neurons, and that's your hyper parameter. Drop out probability.

521
00:52:27.420 --> 00:52:28.290
Nilesh Gupta: So

522
00:52:29.260 --> 00:52:44.960
Nilesh Gupta: like, let's say, like your drop of probability is 0 point 5, so like with probability, point 5 will randomly like. Drop the connections between this when you're doing a forward pass. And this is like for every forward pass and every input, like this, you

523
00:52:47.140 --> 00:52:49.390
Nilesh Gupta: you will randomly do this.

524
00:52:49.490 --> 00:52:55.330
Nilesh Gupta: So what it forces your network to do is like it forces your network to not rely on

525
00:52:55.390 --> 00:53:07.070
Nilesh Gupta: just few of the features like, and try to learn more robust features inside the network. so that, like even if some features are. the network, can still make good predictions.

526
00:53:07.250 --> 00:53:15.570
Nilesh Gupta: So it's a very simple but very effective technique to avoid overfitting. And

527
00:53:15.630 --> 00:53:21.050
Nilesh Gupta: there are a lot of other things that people do like people apply data augmentation.

528
00:53:21.070 --> 00:53:23.090
Nilesh Gupta: And L: 2:

529
00:53:24.270 --> 00:53:26.840
Nilesh Gupta: Yeah, I think, yeah, the homework 3

530
00:53:26.880 --> 00:53:34.680
Nilesh Gupta: ask you to try way, decay weight, because just some another form of L 2 regularization.

531
00:53:35.640 --> 00:53:40.470
Nilesh Gupta: And there is also like in the last class we talked about

532
00:53:41.950 --> 00:53:49.230
Nilesh Gupta: learning rate scheduling. So typically what people do is they decrease the learning rate by a factor of 10

533
00:53:49.440 --> 00:53:54.670
Nilesh Gupta: when, like the validation accuracy, it reaches a stable value.

534
00:53:54.920 --> 00:53:57.650
Nilesh Gupta: So yeah, so

535
00:53:57.800 --> 00:54:04.760
Nilesh Gupta: there's kind of a lot of tricks and art going on in order to

536
00:54:05.000 --> 00:54:09.680
Nilesh Gupta: make this neural networks work and these large new networks work on

537
00:54:09.960 --> 00:54:12.390
Nilesh Gupta: these big data sets. So

538
00:54:16.320 --> 00:54:18.080
Nilesh Gupta: yeah, so

539
00:54:18.530 --> 00:54:25.790
Nilesh Gupta: just one last thing that I wanted to discuss on Cnn is the residual connections so

540
00:54:27.000 --> 00:54:30.690
Nilesh Gupta: people to as oh!

541
00:54:31.740 --> 00:54:35.830
Nilesh Gupta: As people started building like more deeper networks.

542
00:54:35.940 --> 00:54:40.380
Nilesh Gupta: what they observed was that deep cornets do not train well

543
00:54:40.550 --> 00:54:46.320
Nilesh Gupta: like they suffer with the problem of vanishing gradient, like even with right initialization

544
00:54:46.460 --> 00:55:04.290
Nilesh Gupta: like these deep conf minutes like when you have a very deep new network, let's say, 50 or 100 layers. like the performance of that bigger network, was much worse than the performance of a smaller shallower.

545
00:55:04.800 --> 00:55:13.250
Nilesh Gupta: the network. So to so mitigate this. and to solve this like

546
00:55:13.780 --> 00:55:19.380
Nilesh Gupta: the like. One of the key idea that was introduced was the idea of residual networks.

547
00:55:19.520 --> 00:55:21.930
Nilesh Gupta: And this is also something which is

548
00:55:22.310 --> 00:55:24.770
Nilesh Gupta: on the surface very simple, but

549
00:55:24.830 --> 00:55:28.870
Nilesh Gupta: yet very effective in training these networks.

550
00:55:29.450 --> 00:55:30.920
Nilesh Gupta: So it.

551
00:55:32.310 --> 00:55:35.750
Nilesh Gupta: The key idea is that you introduce a pass through into each layer

552
00:55:35.870 --> 00:55:37.890
Nilesh Gupta: basically like you.

553
00:55:38.550 --> 00:55:39.250
Nilesh Gupta: Hmm.

554
00:55:39.380 --> 00:55:42.590
Nilesh Gupta: After computing the output of so

555
00:55:43.040 --> 00:55:48.080
Nilesh Gupta: the after processing the input let's say the input is X through you earlier.

556
00:55:48.130 --> 00:55:50.940
Nilesh Gupta: Like you add the input back to the output.

557
00:55:51.090 --> 00:55:51.970
Nilesh Gupta: So

558
00:55:52.020 --> 00:55:56.150
Nilesh Gupta: the output of this layer will become F of X plus x.

559
00:55:56.250 --> 00:56:00.580
Nilesh Gupta: So in a way, F of X is only trying to learn the residual

560
00:56:00.720 --> 00:56:12.810
Nilesh Gupta: that needs that you want to be on so like. And this this allows you to have a direct path from the loss to

561
00:56:12.970 --> 00:56:18.140
Nilesh Gupta: the inputs and at each layer that you want. So

562
00:56:19.770 --> 00:56:24.350
Nilesh Gupta: here I won't. Go into too much details like

563
00:56:24.820 --> 00:56:28.890
Nilesh Gupta: Hmm. What? Oh.

564
00:56:29.430 --> 00:56:35.000
Nilesh Gupta: how how the residual networks allow you to

565
00:56:35.240 --> 00:56:45.170
Nilesh Gupta: be better than the standard at least. But, like I just wanted to introduce this idea that there there is something called residual connections, which is.

566
00:56:45.180 --> 00:56:51.820
Nilesh Gupta: the operation itself is very simple that you are adding the input back to the output.

567
00:56:51.880 --> 00:56:54.070
Nilesh Gupta: but it helps you to

568
00:56:54.260 --> 00:56:59.850
Nilesh Gupta: chain like, have very deep neural network, and still be able to train them

569
00:56:59.950 --> 00:57:01.200
Nilesh Gupta: properly.

570
00:57:02.130 --> 00:57:08.270
Nilesh Gupta: So, just to show you the results like initially. Vg: Well.

571
00:57:08.280 --> 00:57:11.720
Nilesh Gupta: well, I mean like at very pretty good top 5.

572
00:57:11.860 --> 00:57:19.180
Nilesh Gupta: But then, when people started to go deeper, they were not getting those gains, but after resnets like

573
00:57:19.500 --> 00:57:24.890
Nilesh Gupta: the the resnet paper, was was able to train a 1 50 layer neural network

574
00:57:25.230 --> 00:57:32.190
Nilesh Gupta: which which was able to outperform the state of that by quite a significant margin.

575
00:57:32.530 --> 00:57:37.330
Nilesh Gupta: So yeah. just wanted to introduce this

576
00:57:37.790 --> 00:57:40.090
Nilesh Gupta: concept of residual connections.

577
00:57:40.490 --> 00:57:44.170
Nilesh Gupta: So any questions to this point

578
00:57:47.430 --> 00:57:51.440
Nilesh Gupta: at the end of the discussions on convolutional networks. So

579
00:57:52.150 --> 00:57:54.220
Nilesh Gupta: if you have any questions on

580
00:58:18.810 --> 00:58:19.490
Nilesh Gupta: Okay?

581
00:58:20.820 --> 00:58:25.020
Nilesh Gupta: So now. Oh. this

582
00:58:25.600 --> 00:58:28.750
Nilesh Gupta: so far, we we looked at how to

583
00:58:28.920 --> 00:58:31.680
Nilesh Gupta: process image

584
00:58:32.010 --> 00:58:39.160
Nilesh Gupta: using neural networks. Now we'll look at like how to process a sequence, using the new network.

585
00:58:39.400 --> 00:58:43.580
Nilesh Gupta: So the inputs that

586
00:58:43.630 --> 00:58:48.000
Nilesh Gupta: by sequence I mean, the input, is going to be a sequence of

587
00:58:48.450 --> 00:58:56.450
Nilesh Gupta: sequence of X, one x, 2 to till X, T. And each X X I. Is a feature at time. Step T.

588
00:58:56.820 --> 00:58:59.970
Nilesh Gupta: And we want to output another sequence.

589
00:59:00.440 --> 00:59:04.970
Nilesh Gupta: and like the sequence like in this particular example. It is

590
00:59:05.090 --> 00:59:07.670
Nilesh Gupta: of the same length.

591
00:59:07.710 --> 00:59:11.400
Nilesh Gupta: but they, it doesn't need to be of the same one. Like

592
00:59:11.550 --> 00:59:22.730
Nilesh Gupta: it can be different than the input. Sequence length. But so we want to produce this other sequence by one by 2 to it. and

593
00:59:25.490 --> 00:59:38.340
Nilesh Gupta: these each of the why I's can be either a multi class output or a regression output that we want to produce. And this kind of data we call it a time series or a sequence data.

594
00:59:38.650 --> 00:59:39.520
Nilesh Gupta: So

595
00:59:39.570 --> 00:59:45.390
Nilesh Gupta: we'll briefly look at like how we can process the sequential data

596
00:59:45.590 --> 00:59:52.730
Nilesh Gupta: and like, look at the special architecture that being that have been developed for processing these.

597
00:59:53.860 --> 00:59:54.830
Nilesh Gupta: So

598
00:59:55.690 --> 01:00:04.750
Nilesh Gupta: some examples of this kind of sequential data is the climate data here. Your X Ts can be your temperature at time. T.

599
01:00:05.110 --> 01:00:13.970
Nilesh Gupta: And your byt can be that temperature or the temperature change at time t plus one. The other kind of

600
01:00:15.000 --> 01:00:18.750
Nilesh Gupta: example can be the stock price crediting stock price.

601
01:00:21.640 --> 01:00:24.750
Nilesh Gupta: so like this is also sequential in nature.

602
01:00:26.030 --> 01:00:28.570
Nilesh Gupta: So and in

603
01:00:28.970 --> 01:00:32.170
Nilesh Gupta: language modeling, this kind of

604
01:00:32.200 --> 01:00:41.570
Nilesh Gupta: data comes a lot so typically like when you are trying to model the English language, or you are trying to

605
01:00:43.560 --> 01:00:48.220
Nilesh Gupta: learn a model which is which knows about the language that

606
01:00:48.320 --> 01:00:55.510
Nilesh Gupta: the basic task that you want to train it on is try to predict the next and next token that exists in the

607
01:00:56.840 --> 01:01:01.160
Nilesh Gupta: that will come in that sentence. So, for example, like

608
01:01:01.340 --> 01:01:06.830
Nilesh Gupta: we can have this particular phrase, the cat is. and we want to predict.

609
01:01:07.000 --> 01:01:09.890
Nilesh Gupta: like, what's the next token that can come here.

610
01:01:10.160 --> 01:01:11.040
Nilesh Gupta: So

611
01:01:12.300 --> 01:01:13.720
Nilesh Gupta: how we typically

612
01:01:13.950 --> 01:01:18.970
Nilesh Gupta: do is like we'll have a one hot and coding to representing

613
01:01:19.320 --> 01:01:21.970
Nilesh Gupta: each word at step. 2,

614
01:01:22.320 --> 01:01:23.300
Nilesh Gupta: and

615
01:01:23.340 --> 01:01:26.300
Nilesh Gupta: we'll try to predict the next world

616
01:01:26.460 --> 01:01:27.890
Nilesh Gupta: by the byt.

617
01:01:28.100 --> 01:01:30.200
Nilesh Gupta: where it is like

618
01:01:30.690 --> 01:01:34.710
Nilesh Gupta: token, and one to V. There we is the vocabulary size.

619
01:01:36.260 --> 01:01:37.390
Nilesh Gupta: So

620
01:01:39.190 --> 01:01:40.040
Nilesh Gupta: oh.

621
01:01:41.950 --> 01:01:46.310
Nilesh Gupta: yeah. So this is also like we can kind of formulators

622
01:01:47.340 --> 01:01:50.560
Nilesh Gupta: a sequential data. and

623
01:01:50.740 --> 01:01:59.170
Nilesh Gupta: the other application that that often comes in these language. Processing task is the part of speech tagging.

624
01:02:01.120 --> 01:02:04.840
Nilesh Gupta: for by part of speech tagging we mean that.

625
01:02:05.310 --> 01:02:15.540
Nilesh Gupta: like given a sentence, you want to tag each token with the part of speech like whether it is a noun, it is a word adjective.

626
01:02:15.790 --> 01:02:16.710
Nilesh Gupta: So

627
01:02:18.700 --> 01:02:34.150
Nilesh Gupta: yeah, so this is also we can look at that. This is a sequence to sequence problem that, like given the input, sequence, we want to predict this output sequence where each of the output is one of the part of speech, and it represents the part of speech

628
01:02:34.380 --> 01:02:36.000
Nilesh Gupta: of the I. It. Input

629
01:02:37.760 --> 01:02:43.560
Nilesh Gupta: So these are some of the examples of where this sequential data can come. and

630
01:02:43.720 --> 01:02:46.580
Nilesh Gupta: the way we model

631
01:02:48.140 --> 01:02:52.310
Nilesh Gupta: the architecture that you that it was

632
01:02:52.340 --> 01:02:59.810
Nilesh Gupta: like, at least in the past, was very popular on modeling. The sequential data is the recurrent neural networks.

633
01:03:00.130 --> 01:03:04.980
Nilesh Gupta: And it so. Hmm.

634
01:03:05.210 --> 01:03:09.310
Nilesh Gupta: I'll just briefly describe like, what's the typical mechanism that we use

635
01:03:10.950 --> 01:03:18.870
Nilesh Gupta: inside a regular Rnn. So let's say, like like X. T. Is a Df: input And

636
01:03:19.910 --> 01:03:23.870
Nilesh Gupta: so we want to somehow maintain

637
01:03:25.450 --> 01:03:26.270
Nilesh Gupta: the

638
01:03:27.610 --> 01:03:34.330
Nilesh Gupta: context that we have seen so far in the sequence and use that context and the

639
01:03:34.400 --> 01:03:37.300
Nilesh Gupta: the current input to produce our output.

640
01:03:37.720 --> 01:03:38.690
Nilesh Gupta: So

641
01:03:38.850 --> 01:03:44.890
Nilesh Gupta: so this so in our end we we can do that by maintaining a hidden state.

642
01:03:45.220 --> 01:03:49.450
Nilesh Gupta: This hidden state, like, let's say we call it St. Here.

643
01:03:49.490 --> 01:03:59.500
Nilesh Gupta: and it kind of represents the memory of the network that what whatever it has seen so far. So so we compute this S. By

644
01:04:00.860 --> 01:04:02.320
Nilesh Gupta: By by

645
01:04:03.020 --> 01:04:12.030
Nilesh Gupta: summing, by first multiplying the input the current input xt by your matrix, you and then

646
01:04:12.210 --> 01:04:19.610
Nilesh Gupta: we take the previous hidden state, which is St. Minus one, and we multiply it by the matrix. W.

647
01:04:19.840 --> 01:04:20.990
Nilesh Gupta: You sum them up

648
01:04:21.160 --> 01:04:27.680
Nilesh Gupta: and pass it through activation function F like it can be.

649
01:04:27.900 --> 01:04:31.740
Nilesh Gupta: and that gives us the current heading state

650
01:04:31.800 --> 01:04:32.810
Nilesh Gupta: at time. T.

651
01:04:34.310 --> 01:04:36.760
Nilesh Gupta: And Hmm.

652
01:04:36.790 --> 01:04:39.280
Nilesh Gupta: This W. Matrix is

653
01:04:39.440 --> 01:04:42.660
Nilesh Gupta: called the transition matrix.

654
01:04:42.780 --> 01:04:49.150
Nilesh Gupta: And the You matrix, which is applied to the input is called the word emitting matrix. And

655
01:04:50.200 --> 01:04:55.680
Nilesh Gupta: this, like we start with the initial head in state, is really set to be 0

656
01:04:57.130 --> 01:05:01.640
Nilesh Gupta: and like to predict a particular output. We

657
01:05:01.850 --> 01:05:09.640
Nilesh Gupta: at time T. We can like, take this in the current hidden state and multiply it by another main

658
01:05:09.830 --> 01:05:11.950
Nilesh Gupta: matrix. and

659
01:05:12.610 --> 01:05:17.010
Nilesh Gupta: then take the arc, Max, to get the class.

660
01:05:17.080 --> 01:05:25.000
Nilesh Gupta: the which which scores the highest. So here, like we are assuming that the outputs or ot is

661
01:05:25.490 --> 01:05:27.170
Nilesh Gupta: no

662
01:05:27.400 --> 01:05:29.420
Nilesh Gupta: a class

663
01:05:29.510 --> 01:05:32.110
Nilesh Gupta: from a multi-class, the

664
01:05:32.440 --> 01:05:35.440
Nilesh Gupta: from a set of like K classes.

665
01:05:35.490 --> 01:05:36.210
Nilesh Gupta: So

666
01:05:39.320 --> 01:05:40.310
Nilesh Gupta: so

667
01:05:41.210 --> 01:05:44.460
Nilesh Gupta: so does it make sense like so far what I've said

668
01:05:45.870 --> 01:05:46.570
Nilesh Gupta: that

669
01:05:47.850 --> 01:05:53.260
Nilesh Gupta: we want to maintain kind of a context of all the things that we have seen so far.

670
01:05:53.320 --> 01:05:58.170
Nilesh Gupta: and we want to use that and the current input predict our given output.

671
01:06:02.100 --> 01:06:17.090
Nilesh Gupta: And we are doing that in our elements by like a simple operation which is like, we're multiplying the input by a embed matrix, you and multiplying the hidden state by matrix to blue, summing them up and passing it

672
01:06:17.100 --> 01:06:22.880
Nilesh Gupta: through activation function. It will give us the good intended state.

673
01:06:32.080 --> 01:06:35.790
Nilesh Gupta: Okay. So typically.

674
01:06:37.250 --> 01:06:42.170
Nilesh Gupta: these are the parameters you W. And G. Of your neural network.

675
01:06:42.640 --> 01:06:43.700
Nilesh Gupta: And

676
01:06:45.850 --> 01:06:50.210
Nilesh Gupta: now, as you would normally do like you, we

677
01:06:50.290 --> 01:07:00.300
Nilesh Gupta: finally, we During training, we've tried to find you W. And V. By minimizing empirical laws and the loss of a sequence

678
01:07:00.440 --> 01:07:01.950
Nilesh Gupta: we

679
01:07:02.170 --> 01:07:05.280
Nilesh Gupta: basically like compute loss for every output.

680
01:07:05.340 --> 01:07:09.260
Nilesh Gupta: and then sum it up, and that becomes the loss for our sequence.

681
01:07:09.840 --> 01:07:10.780
Nilesh Gupta: And

682
01:07:12.380 --> 01:07:22.570
Nilesh Gupta: as as we always do, we take the address loss over all of the data points inside our data set. and we solve this by Std. And Adam.

683
01:07:25.850 --> 01:07:34.460
Nilesh Gupta: And as you one thing to note here is that, like you are reusing these weight matrices, you W. And B at each time step

684
01:07:34.870 --> 01:07:42.440
Nilesh Gupta: so like these gradients also, like you, are kind of unrolling them and

685
01:07:42.610 --> 01:07:48.520
Nilesh Gupta: back propagating the greetings to like

686
01:07:48.900 --> 01:07:53.050
Nilesh Gupta: you are using, like the output of each of the time. Step to

687
01:07:53.380 --> 01:08:00.660
Nilesh Gupta: update your You W. Envs. but they are shared across all of

688
01:08:01.160 --> 01:08:02.040
Nilesh Gupta: the

689
01:08:03.680 --> 01:08:05.560
Nilesh Gupta: elements inside the sequence.

690
01:08:09.950 --> 01:08:10.640
Nilesh Gupta: Okay.

691
01:08:11.900 --> 01:08:12.800
Nilesh Gupta: So.

692
01:08:13.610 --> 01:08:18.319
Nilesh Gupta: as I said, like the output of

693
01:08:18.460 --> 01:08:20.779
Nilesh Gupta: like, you can use Rn. To do

694
01:08:20.899 --> 01:08:23.899
Nilesh Gupta: some text classification as well like

695
01:08:24.020 --> 01:08:31.840
Nilesh Gupta: you, Don't, necessarily have to like, produce one output at each time. Step. so that you can do by just

696
01:08:32.000 --> 01:08:35.069
Nilesh Gupta: processing your input sequence

697
01:08:35.189 --> 01:08:41.100
Nilesh Gupta: and taking the last hidden state, and applying on neural network on that last heading state.

698
01:08:41.330 --> 01:08:45.439
Nilesh Gupta: to make predictions for the whole sequence.

699
01:08:46.000 --> 01:08:47.010
Nilesh Gupta: So

700
01:08:47.890 --> 01:08:58.040
Nilesh Gupta: this kind of the R. And it's kind of treat the general sequence to sequence problem like it can be a like. You can have a

701
01:08:58.500 --> 01:09:02.979
Nilesh Gupta: like long sequence, and then you are just trying to categorize it

702
01:09:03.330 --> 01:09:10.680
Nilesh Gupta: into like few of the classes. Or you can like be producing like one output for each of the element in the sequence.

703
01:09:10.710 --> 01:09:14.850
Nilesh Gupta: or you can be just computing a completely different sequence

704
01:09:14.950 --> 01:09:16.950
Nilesh Gupta: from the given input sequence.

705
01:09:18.210 --> 01:09:19.200
Nilesh Gupta: So

706
01:09:20.160 --> 01:09:25.180
Nilesh Gupta: they are kind of a general framework to like. Treat these sequences.

707
01:09:25.670 --> 01:09:29.620
Nilesh Gupta: and you can stack our elements on top of each other

708
01:09:29.859 --> 01:09:33.930
Nilesh Gupta: by having a multi layer Rn. So

709
01:09:34.520 --> 01:09:40.029
Nilesh Gupta: so as shown in the figure here like you are just like whatever

710
01:09:41.380 --> 01:09:46.560
Nilesh Gupta: output that you are producing by the first layer Rnn: like you are giving it to.

711
01:09:46.840 --> 01:09:51.399
Nilesh Gupta: As input to the second layer, Rnn. And that is used

712
01:09:51.520 --> 01:09:57.110
Nilesh Gupta: to produce like the final output, so you can stag. These are n on top of each other.

713
01:09:57.260 --> 01:10:01.130
Nilesh Gupta: and this will give you the multi-layer Rnn.

714
01:10:03.510 --> 01:10:04.340
Nilesh Gupta: So

715
01:10:08.220 --> 01:10:15.450
Nilesh Gupta: I won't go into the details of the kind of the later architecture that would

716
01:10:15.560 --> 01:10:16.410
Nilesh Gupta: Oh.

717
01:10:18.830 --> 01:10:19.500
Nilesh Gupta: hmm!

718
01:10:19.710 --> 01:10:23.510
Nilesh Gupta: Published, which improve. These are in instructions.

719
01:10:23.650 --> 01:10:28.670
Nilesh Gupta: But I just wanted to mention that, like this is.

720
01:10:29.160 --> 01:10:33.670
Nilesh Gupta: these are in, and this particular formulation of Rnn.

721
01:10:33.920 --> 01:10:40.390
Nilesh Gupta: Is kind of very simplistic, and it suffers from some challenges that, like it

722
01:10:40.590 --> 01:10:43.420
Nilesh Gupta: is not able to capture long term dependencies.

723
01:10:43.730 --> 01:10:50.930
Nilesh Gupta: and like when you have long sequences like this, also like, has the vanishing gradient problem.

724
01:10:51.350 --> 01:10:53.390
Nilesh Gupta: because, like you are multiplying

725
01:10:53.550 --> 01:10:58.600
Nilesh Gupta: by the weight matrices over a long sequence. Then, like

726
01:10:59.820 --> 01:11:01.510
Nilesh Gupta: it's possible that you'll

727
01:11:01.560 --> 01:11:10.710
Nilesh Gupta: get very less gradients like towards the start of the sequence. So there were multiple solutions that were proposed, and one of the

728
01:11:10.760 --> 01:11:16.730
Nilesh Gupta: popular, most popular solution is the Lsdm. Which is the long, short term memory network.

729
01:11:16.980 --> 01:11:23.120
Nilesh Gupta: So it it kind of works the same that, like it, looks at

730
01:11:23.460 --> 01:11:25.990
Nilesh Gupta: it, tries to maintain our hidden state.

731
01:11:26.300 --> 01:11:30.030
Nilesh Gupta: and which which is supposed to capture

732
01:11:30.110 --> 01:11:35.810
Nilesh Gupta: all the information that you have seen so far and like it takes the input.

733
01:11:35.930 --> 01:11:40.340
Nilesh Gupta: But how it combines the hidden States and the input to produce the output

734
01:11:40.440 --> 01:11:53.050
Nilesh Gupta: is slightly different. But like these were the solutions that were proposed to solve some of the challenges that exist in the standard item in

735
01:11:53.920 --> 01:11:54.910
Nilesh Gupta: So

736
01:11:56.130 --> 01:12:01.650
Nilesh Gupta: Yeah. So that's all. For today. Let me know if you guys have any questions.

737
01:12:05.830 --> 01:12:16.460
Nilesh Gupta: I yeah, I think like today's lecture wasn't a lot to digest. So yeah, please feel free to

738
01:12:17.960 --> 01:12:20.820
Nilesh Gupta: ask questions, even if they are silly.

739
01:12:22.390 --> 01:12:23.120
Nilesh Gupta: But

740
01:12:23.350 --> 01:12:28.490
Nilesh Gupta: sure in the next lecture at yeah, Professor, in the jet will go over

741
01:12:29.050 --> 01:12:29.910
Nilesh Gupta: kind of

742
01:12:30.710 --> 01:12:35.770
Nilesh Gupta: one of the very popular architecture these days, which are called transformers.

743
01:12:36.030 --> 01:12:38.160
Nilesh Gupta: which in a sense.

744
01:12:40.460 --> 01:12:42.730
Nilesh Gupta: are now able to solve

745
01:12:42.870 --> 01:12:44.900
Nilesh Gupta: both of these problems, like

746
01:12:45.160 --> 01:12:47.630
Nilesh Gupta: like they are used in images

747
01:12:47.810 --> 01:12:54.380
Nilesh Gupta: as well as sequence. They were initially proposed for this the sequences, but

748
01:12:54.730 --> 01:12:59.180
Nilesh Gupta: people have made them work for images as well.

749
01:12:59.330 --> 01:13:01.180
Nilesh Gupta: and like that, so

750
01:13:01.930 --> 01:13:10.950
Nilesh Gupta: kind of one of the go to architecture these days, whenever you are encountered any machine learning problem and you have enough data.

751
01:13:11.220 --> 01:13:16.170
Nilesh Gupta: So these architectures are kind of

752
01:13:16.450 --> 01:13:19.660
Nilesh Gupta: not that widely used today.

753
01:13:19.790 --> 01:13:21.050
Nilesh Gupta: but

754
01:13:22.580 --> 01:13:27.220
Nilesh Gupta: it's it's a good to know about them and see how

755
01:13:27.280 --> 01:13:28.130
Nilesh Gupta: it was.

756
01:13:43.500 --> 01:13:47.980
Nilesh Gupta: Yeah. So if there are no questions, then I guess we can

757
01:13:49.980 --> 01:13:53.450
Nilesh Gupta: stop right now. We are here all of a sudden.

758
01:13:53.940 --> 01:13:56.910
Nilesh Gupta: 4, 45, so

759
01:13:58.340 --> 01:14:02.750
Nilesh Gupta: quick it see, you guys. I was could be.

WEBVTT

1
00:01:07.820 --> 00:01:09.870
Inderjit Dhillon: Okay, Good afternoon. Everybody

2
00:01:13.240 --> 00:01:19.210
Inderjit Dhillon: Can somebody quickly verify that. They can see my screen, which has some slides.

3
00:01:21.120 --> 00:01:22.610
Ashray Desai: Yes, we can see your screen.

4
00:01:24.090 --> 00:01:25.510
Inderjit Dhillon: Okay.

5
00:01:27.450 --> 00:01:34.760
Inderjit Dhillon: Okay, any questions before I start, I do have a couple of reminders.

6
00:01:35.190 --> 00:01:48.270
Inderjit Dhillon: So from next week onwards we'll start having presentations by you all from with my students on topics that you know many people have signed up for.

7
00:01:48.520 --> 00:01:57.890
Inderjit Dhillon: but there, I think, still a few people who have not signed up for any presentation. And so remember that every person needs every student

8
00:01:58.050 --> 00:02:00.970
Inderjit Dhillon: needs to be part of a presentation.

9
00:02:02.080 --> 00:02:05.680
Inderjit Dhillon: So let me know if you have. Is there any questions around that.

10
00:02:08.300 --> 00:02:10.750
Inderjit Dhillon: Okay, there is a sign up sheet

11
00:02:11.140 --> 00:02:23.020
Inderjit Dhillon: where you can put your and your partner's name. There are some cases where I think there are, only there's only one person assigned, but you know there are 2 persons for presentation.

12
00:02:23.100 --> 00:02:32.550
Inderjit Dhillon: So if you want to join in with the people who are. you know, right now single in those presentations. So you know, you should connect with them

13
00:02:34.780 --> 00:02:36.500
Inderjit Dhillon: any questions about that.

14
00:02:38.690 --> 00:02:48.980
Inderjit Dhillon: Okay, it's a part that i'm very much kind of looking forward to right, because it'll expose us to a lot of like recent stuff that is going on in deep learning.

15
00:02:51.300 --> 00:02:53.700
Inderjit Dhillon: Okay? And the second

16
00:02:56.200 --> 00:03:06.810
Inderjit Dhillon: announcement, you know that I just wanted to make is that your homework for is coming up. We plan to assign it on Wednesday, April fifth.

17
00:03:07.060 --> 00:03:10.480
Inderjit Dhillon: and it will be due on April

18
00:03:10.830 --> 00:03:14.210
Inderjit Dhillon: seventeenth. So

19
00:03:15.040 --> 00:03:24.860
Inderjit Dhillon: the main reason for doing it on, you know, making it on the seventeenth is, you know. is so that you can have time, for you know your projects, and so on. So.

20
00:03:24.880 --> 00:03:33.600
Inderjit Dhillon: But I don't think it'll be an issue if you need to extend that deadline. But I would just urge you to try not to delay things.

21
00:03:33.790 --> 00:03:37.460
Inderjit Dhillon: because, like the end of the semester, is coming fast.

22
00:03:40.060 --> 00:03:42.520
Inderjit Dhillon: So any questions about

23
00:03:43.780 --> 00:03:47.870
Inderjit Dhillon: anything else you might have wanted to talk about in the beginning.

24
00:03:51.980 --> 00:03:56.130
Inderjit Dhillon: If not, I can start, you know, going through the

25
00:03:57.560 --> 00:04:07.070
Inderjit Dhillon: today's lecture. Okay, so is that kind of no more questions I will talk about

26
00:04:11.970 --> 00:04:15.960
Inderjit Dhillon: something that actually has, you know. Probably

27
00:04:16.010 --> 00:04:29.590
Inderjit Dhillon: you've heard about has a lot of people excited in machine learning. in deep learning, and which is about this neural network architecture of transformers that uses

28
00:04:29.660 --> 00:04:38.820
Inderjit Dhillon: a mechanism called attention. So last week Nilish reviewed. Some talked a little bit about some

29
00:04:38.930 --> 00:04:51.070
Inderjit Dhillon: neural network architectures like, you know. If you know more multi there, Perceptron, you're using it as part of your one of your homeworks. He talked about convolutional neural networks.

30
00:04:51.180 --> 00:04:54.910
Inderjit Dhillon: talked a little bit about Rn. Ends

31
00:04:54.940 --> 00:05:00.540
Inderjit Dhillon: the current neural networks and Lstm. Lsdms. And

32
00:05:00.990 --> 00:05:05.690
Inderjit Dhillon: you know Lsdms for state of the art I want to say about

33
00:05:06.070 --> 00:05:17.780
Inderjit Dhillon: 5 6 years ago used to power machine translation in many user facing applications. But since then

34
00:05:17.850 --> 00:05:26.250
Inderjit Dhillon: what is taken over are these architectures for transformers? And they've really been having a transformative impact

35
00:05:26.350 --> 00:05:28.880
Inderjit Dhillon: on how deep learning is done.

36
00:05:29.490 --> 00:05:38.960
Inderjit Dhillon: Okay. So we'll talk about that today. So we'll put it in the context of natural language processing. Okay, so we'll talk about natural language processing.

37
00:05:39.770 --> 00:05:50.320
Inderjit Dhillon: how the modeling can be done, how it used to be done, and then come to the transformer architecture. So now let's just think, from very basics.

38
00:05:50.540 --> 00:05:57.760
Inderjit Dhillon: Suppose you have a sentence or a document in some natural language. Okay, so English.

39
00:05:57.850 --> 00:06:01.600
Inderjit Dhillon: Spanish, French, Chinese in the

40
00:06:02.360 --> 00:06:07.660
Inderjit Dhillon: how do you represent things? So the simplest thing? And this is being done since the

41
00:06:07.690 --> 00:06:13.580
Inderjit Dhillon: you know 1960 s it's still used this back of words marvel

42
00:06:13.850 --> 00:06:22.980
Inderjit Dhillon: and kind of classical way to represent and Lp. Data, but one which kind of loses the sequential information that is present

43
00:06:23.000 --> 00:06:27.740
Inderjit Dhillon: in natural language sentences. Okay. So the way this is done is

44
00:06:28.150 --> 00:06:42.810
Inderjit Dhillon: that each sentence or document is going to be represented by a very, very, very high, dimensional d dimensional. Vector. Okay, and this d dimensional vector will capture

45
00:06:42.870 --> 00:06:48.910
Inderjit Dhillon: all the words in your vocabulary. in all the documents.

46
00:06:48.990 --> 00:07:01.490
Inderjit Dhillon: Right? So think of all the unique words that are present in a collection of documents. So if you have a 1 million documents, this could easily be in the billions right in terms of the actual words that are used.

47
00:07:01.790 --> 00:07:02.930
Inderjit Dhillon: and

48
00:07:03.470 --> 00:07:10.070
Inderjit Dhillon: what this representation is that X. I. So maybe let me use the course.

49
00:07:10.740 --> 00:07:12.130
Inderjit Dhillon: So x I.

50
00:07:16.230 --> 00:07:20.430
Inderjit Dhillon: So you represent this by X and X. I,

51
00:07:20.510 --> 00:07:26.950
Inderjit Dhillon: is the number of occurrences of word I Okay. So there is some

52
00:07:27.140 --> 00:07:41.460
Inderjit Dhillon: listing of the words in the entire vocabulary. Right? So here is a very toy example of a document that says the International Conference on Machine Learning is the Leaving International Academic Conference in machine learning.

53
00:07:41.710 --> 00:07:49.470
Inderjit Dhillon: So you can see that you look at the different tokens or words International conference, machine

54
00:07:50.060 --> 00:07:57.770
Inderjit Dhillon: train learning, leading total role, right? These might be. This is a total collection of all the words

55
00:07:57.780 --> 00:08:09.180
Inderjit Dhillon: in the collection of buckets. But this document has 2 occurrences international here international here. So this count is 2.

56
00:08:09.540 --> 00:08:16.950
Inderjit Dhillon: Conference appears also twice here, and here, so discount is 2 machine occurs twice

57
00:08:17.140 --> 00:08:28.670
Inderjit Dhillon: right and leading occurs once right. The leading is over here. Okay? And then there are some words that Don't occur at all that are present in the vocabulary or in the collection of documents.

58
00:08:28.700 --> 00:08:35.559
Inderjit Dhillon: But i'm not present in this document. Okay, and most actually of the words will actually have a 0, because

59
00:08:35.570 --> 00:08:50.780
Inderjit Dhillon: you know, typically you have a short document. There are only going to be. For example, this is a title of about, you know 1014 words. They're going to be at least more at least at most 14, if if 14 is the length at most. 14 non zeros.

60
00:08:50.800 --> 00:08:57.330
Inderjit Dhillon: Okay. And but the collection, the dimensionality d can be very high. It could be in the bill.

61
00:08:58.530 --> 00:09:09.050
Inderjit Dhillon: but then you could say, Well, you kind of lost a lot of information over here, right you, for example. Really, it's International Conference. It is not just machine. Separate and learning separate.

62
00:09:09.080 --> 00:09:13.950
Inderjit Dhillon: It's machine learning. That is a concept, so you can find capture and off

63
00:09:14.460 --> 00:09:26.850
Inderjit Dhillon: what are called n gram features where and can vary. So if N. Is 2, then you have diagrams. So, in addition, you could have this unigrams. In addition, you can have diagrams.

64
00:09:26.880 --> 00:09:30.130
Inderjit Dhillon: So you have international conference, machine learning.

65
00:09:30.470 --> 00:09:36.580
Inderjit Dhillon: leading international. So you are basically just taking bygrounds words that are kind of occur

66
00:09:36.620 --> 00:09:48.870
Inderjit Dhillon: together. academic conference, and so on. Okay. and then, obviously, you'll have some diagrams that do not like this particular document. But then you keep the count over here.

67
00:09:49.300 --> 00:09:49.930
Inderjit Dhillon: Okay.

68
00:09:50.680 --> 00:09:56.090
Inderjit Dhillon: The problem, of course, is that your dimensionality is going to explode right? Because if you have.

69
00:09:56.160 --> 00:10:07.230
Inderjit Dhillon: for example, 10,000 words. The number of potential features is going to be quadratic, right? So in the example that I talked about a 1 billionsquare will be much, much, much larger.

70
00:10:08.550 --> 00:10:18.880
Inderjit Dhillon: Okay, so. But that is one way of capturing more context or more information about the document. So in these cases, what you can do is.

71
00:10:19.030 --> 00:10:29.630
Inderjit Dhillon: you can use, construct a data matrix, or you know, a bag of words matrix, right? Or we can talk about it's normalized version. I mean, let's talk about it right now.

72
00:10:29.670 --> 00:10:34.470
Inderjit Dhillon: So one option is just to consider the Tf. Which is the term frequency.

73
00:10:34.710 --> 00:10:35.310
Inderjit Dhillon: Okay.

74
00:10:35.410 --> 00:10:39.150
So, for example, here this example has 3 documents.

75
00:10:39.450 --> 00:10:54.390
Inderjit Dhillon: One, I think, is New York Times. The second is new, your post. and the third is Los Angeles Times. Right? So these are newspaper names.

76
00:10:54.720 --> 00:11:00.670
Inderjit Dhillon: and one is, you can just take the number of times the term appears in a document.

77
00:11:01.180 --> 00:11:13.850
Inderjit Dhillon: or you could say well, the words that appear less commonly and only in that document are somehow more important. So this is the concept of doing an idf or an inverse

78
00:11:14.010 --> 00:11:15.740
Inderjit Dhillon: document frequency.

79
00:11:15.990 --> 00:11:23.100
Inderjit Dhillon: And what you typically do is you multiply Tf: by its I, by the idf of the corresponding word.

80
00:11:24.230 --> 00:11:28.630
Inderjit Dhillon: Okay. So that's why it's called Df: Ids.

81
00:11:29.080 --> 00:11:35.400
Inderjit Dhillon: okay. So and the way it works is, it's actually you're multiplying the term frequency

82
00:11:35.600 --> 00:11:39.610
Inderjit Dhillon: by the Idf, which is one plus log

83
00:11:39.910 --> 00:11:53.570
Inderjit Dhillon: of the number of documents divided by the number of documents with this particular word. So, for example, if this word is a very common word. and it actually appears in all the documents

84
00:11:54.100 --> 00:11:57.010
Inderjit Dhillon: right? Suppose there is a generic word like the.

85
00:11:57.310 --> 00:12:01.580
Inderjit Dhillon: and it appears in all the documents you're actually this is going to be

86
00:12:01.700 --> 00:12:07.280
Inderjit Dhillon: and divided by N. Log of one. That's 0. So the way, it would just be one.

87
00:12:08.290 --> 00:12:13.870
Inderjit Dhillon: Okay, and otherwise it will get a higher weight. So they the extreme, and you could have a very rare word

88
00:12:14.150 --> 00:12:20.640
Inderjit Dhillon: which only appears once, and then this weight will be log of n plus one.

89
00:12:21.320 --> 00:12:24.710
Inderjit Dhillon: so you will give higher weight to words

90
00:12:24.840 --> 00:12:30.160
Inderjit Dhillon: that occur less frequently. So in this example you'll see

91
00:12:30.210 --> 00:12:32.150
Inderjit Dhillon: that you know the word like

92
00:12:32.380 --> 00:12:37.340
Inderjit Dhillon: York Times. New New York Times. Right Los Angeles Times.

93
00:12:37.360 --> 00:12:38.880
Inderjit Dhillon: They occurred twice

94
00:12:38.910 --> 00:12:42.610
Inderjit Dhillon: as a result in this representation

95
00:12:42.670 --> 00:12:50.150
Inderjit Dhillon: so d one. Of course, that contains all the words but Los Angeles the term Los Angeles is given higher weight

96
00:12:50.160 --> 00:13:07.100
Inderjit Dhillon: than the word types, so of course it makes sense in a bigger corpus. But that's one idea of trying to isolate out or upweight words that occur less frequency, and it's a I Df: Df: I Df. Is, you know.

97
00:13:07.230 --> 00:13:10.720
Inderjit Dhillon: essentially the default way to scale

98
00:13:10.880 --> 00:13:20.600
Inderjit Dhillon: the back of words waiting that appear in that particular vector space. Now you can take a back of words, and

99
00:13:21.130 --> 00:13:26.270
Inderjit Dhillon: I want to say, around 15 years ago

100
00:13:26.330 --> 00:13:28.720
Inderjit Dhillon: pretty much all

101
00:13:29.860 --> 00:13:38.180
Inderjit Dhillon: text applications actually use this kind of vectorization. And then, when they wanted to do something like

102
00:13:38.440 --> 00:13:50.520
Inderjit Dhillon: text classification, for example, spam, classification, sentiment, prediction. or you're trying to predict whether you know review score prediction. Then it would be done by.

103
00:13:50.900 --> 00:13:57.140
Inderjit Dhillon: you know, using that as X. So X would be some very, very high, dimensional, sparse. Vector

104
00:13:57.190 --> 00:14:09.480
Inderjit Dhillon: and then W. Would be learned typically by a linear model such as a linear Svm or logistic regression. Remember that this dimensionality would be quite high right, and this would typically be a sparse sector.

105
00:14:10.060 --> 00:14:28.240
Inderjit Dhillon: And then, in some sense, what is learned by the decision boundary is the contribution of each word to the corresponding task. Right? So, for example, you know, for the sentiment, prediction. If the words are, you know, good, joyful, and so on, they would be, you know, more.

106
00:14:28.240 --> 00:14:33.450
Inderjit Dhillon: get higher contribution, but it's like poor bad.

107
00:14:33.620 --> 00:14:38.880
Inderjit Dhillon: so on, would be, you know, indicators of a negative sentiment. And so

108
00:14:39.060 --> 00:14:45.340
Inderjit Dhillon: this particular depend. This particular weight will actually depend upon the tasks that are at hand.

109
00:14:47.300 --> 00:14:49.850
Inderjit Dhillon: Okay. So now

110
00:14:50.030 --> 00:15:02.780
Inderjit Dhillon: you can. Instead of using linear support vector machines or logistic regression, you can think of using deep learning right? And in particular, you can think of about just using

111
00:15:02.960 --> 00:15:07.620
Inderjit Dhillon: Mlp. Or monthly there.

112
00:15:08.410 --> 00:15:09.600
Inderjit Dhillon: So remember.

113
00:15:09.650 --> 00:15:17.010
Inderjit Dhillon: you know, X is our Input let's say a document, and it's representation is a back of words model.

114
00:15:17.440 --> 00:15:20.460
Inderjit Dhillon: right? Not each.

115
00:15:20.720 --> 00:15:31.320
Inderjit Dhillon: I can think of this multi-layer perceptron as this repeated linear transformation w I X. Followed by

116
00:15:31.340 --> 00:15:36.680
Inderjit Dhillon: activation function, which is typically a nonlinearity such as you know, a sigmoid function.

117
00:15:37.130 --> 00:15:37.710
Hmm.

118
00:15:37.920 --> 00:15:44.560
Inderjit Dhillon: So you can. Your F, which is kind of doing this forward pass

119
00:15:44.670 --> 00:15:46.490
Inderjit Dhillon: in this neural network

120
00:15:46.720 --> 00:15:59.410
Inderjit Dhillon: is, let's say, of this form: F of x, right? So w not then a Sigma, then the wi and the sigma, and then there is a finally a linear layer at the end in this particular case.

121
00:15:59.590 --> 00:16:00.360
Inderjit Dhillon: Okay.

122
00:16:00.960 --> 00:16:05.840
Inderjit Dhillon: if you look particularly at W, not X. Think of what it is.

123
00:16:06.430 --> 00:16:13.080
Inderjit Dhillon: If the dimensionality of X is D. Then W. Naught is from D, one by d matrix.

124
00:16:14.470 --> 00:16:28.750
Inderjit Dhillon: Okay. typically D. Is going to be very, very large. If you have a back of 4 small. so D is extremely large. right? So you can think of it, as you know, and w not is

125
00:16:30.630 --> 00:16:31.920
Inderjit Dhillon: like.

126
00:16:32.130 --> 00:16:38.100
Inderjit Dhillon: you know this is d one. This is the like, a very short. that matrix.

127
00:16:38.770 --> 00:16:40.970
Inderjit Dhillon: Okay. So this is W. Lot.

128
00:16:44.040 --> 00:16:53.610
Inderjit Dhillon: And if you think of each column of this matrix. it is W. I. And if you think this is the number of words.

129
00:16:58.280 --> 00:17:00.770
Inderjit Dhillon: Okay. so W. I.

130
00:17:01.240 --> 00:17:09.710
Inderjit Dhillon: Is the one-dimensional representation of this I it. For so you can think of it as a word, a minute.

131
00:17:11.099 --> 00:17:23.060
Inderjit Dhillon: Okay. And you can think of W. Not X, which is the first time over here. right? It is a linear combination off

132
00:17:23.260 --> 00:17:24.250
Inderjit Dhillon: the

133
00:17:25.310 --> 00:17:29.510
Inderjit Dhillon: different d dimensional words, right? So in particular.

134
00:17:29.550 --> 00:17:37.090
Inderjit Dhillon: So just so. You know what is happening right? So if I have. this is W not. And if this is x

135
00:17:38.720 --> 00:17:41.810
Inderjit Dhillon: right, x, one is, remember x is d dimensional.

136
00:17:43.190 --> 00:17:50.000
and if this component is x one, this component is like second proponent of the x, 2 third component of x, 3

137
00:17:51.830 --> 00:17:57.160
Inderjit Dhillon: right. This is equal to this right x, not times

138
00:17:57.750 --> 00:17:59.110
Inderjit Dhillon: W. One

139
00:17:59.760 --> 00:18:06.420
Inderjit Dhillon: x, 2 times w, 2 plus x, 3 times one, B, 3, and so on.

140
00:18:06.660 --> 00:18:11.770
Inderjit Dhillon: So w not x is a linear combination of these vectors.

141
00:18:11.920 --> 00:18:22.200
Inderjit Dhillon: so w not is kind of called the word the embedding matrix, and actually can be very large, right? Because X can very, have very, very large dimensions.

142
00:18:22.440 --> 00:18:30.400
Inderjit Dhillon: And then the final prediction over here, right, which is F. Of X. You can view it as

143
00:18:30.620 --> 00:18:34.340
Inderjit Dhillon: kind of an L minus one layer like this part

144
00:18:35.550 --> 00:18:38.790
Inderjit Dhillon: operating on Sigma times.

145
00:18:40.530 --> 00:18:45.080
Inderjit Dhillon: Okay, which is this linear combination of the world embeddings.

146
00:18:45.550 --> 00:18:53.710
Inderjit Dhillon: So one of the things that you've seen is that you know we are not capturing so far by using a bag of ports model.

147
00:18:53.940 --> 00:18:57.900
Inderjit Dhillon: We're not capturing the sequence of information in in language.

148
00:19:01.670 --> 00:19:15.040
Inderjit Dhillon: So the next step where we come to is okay, how can we capture the sequence? Information. Okay, and rather than going state to transformers, which is the way that it is done right now.

149
00:19:15.070 --> 00:19:26.100
Inderjit Dhillon: let's talk a little. Let's just review a little bit about the current neural networks that Nilish went through. He went through it a little bit quickly, so so i'm going to kind of repeat that right Historically.

150
00:19:27.150 --> 00:19:37.900
Inderjit Dhillon: it's a pretty important, and in some sense a very natural way of trying to capture a sequence to information, whereas transformers are. You know we'll see later. They are.

151
00:19:38.190 --> 00:19:49.200
Inderjit Dhillon: you know, I think, a little bit less natural way. But once you understand what Lsdms are doing or recurrent neural networks are doing, then they make more more sense. So

152
00:19:49.750 --> 00:20:02.920
Inderjit Dhillon: here's the problem: right? Suppose you have a time, series, data or sequence data, so you can think about the input as being, you know. each X T. Think of it as a d dimensional. Vector.

153
00:20:03.320 --> 00:20:06.780
Inderjit Dhillon: So you are getting input as x one x, 2

154
00:20:06.980 --> 00:20:14.650
Inderjit Dhillon: x little t all the way to like capital T: right? So you have capital T. D. Dimensional vectors

155
00:20:15.170 --> 00:20:29.140
Inderjit Dhillon: coming sequentially right. So, for example, you can think of them as the word embeddings that occur for the corresponding words in a particular sentence which is of length, Capital, T:

156
00:20:29.530 --> 00:20:30.300
Inderjit Dhillon: Okay.

157
00:20:31.300 --> 00:20:37.410
Inderjit Dhillon: And then typically you may have some tasks or so associated with it, and you can think about an output.

158
00:20:38.740 --> 00:20:52.310
Inderjit Dhillon: Potentially. It doesn't have to be there. But potentially at every step it could be that we only have an output at the Ts step. Okay. But there could be, for example, in machine translation there could be an output at every step.

159
00:20:53.030 --> 00:21:01.390
Inderjit Dhillon: Okay, and each Y. It is the output at step T. Now this could either be like a multi-class output.

160
00:21:01.770 --> 00:21:11.440
Inderjit Dhillon: for example, what word is present in this sequence? Or it could actually be a regression output. So it could be, you know, one out of these L classes.

161
00:21:11.500 --> 00:21:14.870
Inderjit Dhillon: or it could be a real number in the regression set.

162
00:21:15.400 --> 00:21:22.770
Inderjit Dhillon: So you know, Obviously, time series prediction is very important in many applications. So you can think about. You know

163
00:21:22.790 --> 00:21:40.690
Inderjit Dhillon: climate data. We are measuring temperatures at different times, at times sequentially. And then you want client, maybe predict the time at sorry the temperature at the next time step or in the financial case, you know you are getting you.

164
00:21:40.840 --> 00:21:45.000
Inderjit Dhillon: These xts are stock prices at different time points.

165
00:21:45.030 --> 00:21:51.930
Inderjit Dhillon: and then you want to try and predict the stock price at the next time time. Step. Okay. So that's one

166
00:21:52.010 --> 00:21:58.310
Inderjit Dhillon: class of applications to thinking about when you have sequence information.

167
00:21:58.760 --> 00:22:05.570
Inderjit Dhillon: But today we'll talk more about language modeling. As as I said, right, so you can think about. You know

168
00:22:05.680 --> 00:22:14.240
Inderjit Dhillon: the cat is. and the question is, can you predict the next word and the fact that this is a sequence

169
00:22:14.320 --> 00:22:18.820
Inderjit Dhillon: can actually help us end predicting what the next word is.

170
00:22:20.200 --> 00:22:29.090
Inderjit Dhillon: So how do we model this using deep learning? And we first talk about the kind of neural networks.

171
00:22:29.100 --> 00:22:32.360
Inderjit Dhillon: so you can think of xt as

172
00:22:32.740 --> 00:22:36.640
Inderjit Dhillon: kind of for like a one hot encoding. So by one hard and coding.

173
00:22:36.840 --> 00:22:41.140
Inderjit Dhillon: What I mean is that you know there'll only be one

174
00:22:41.610 --> 00:22:42.760
Inderjit Dhillon: number

175
00:22:43.310 --> 00:22:49.030
Inderjit Dhillon: which is the one in this vector so that's why it's called one hot. It's only caught in this location.

176
00:22:49.560 --> 00:22:53.530
Inderjit Dhillon: Okay, and this location I identify the corresponding word.

177
00:22:53.860 --> 00:22:59.830
Inderjit Dhillon: Okay. So, for example, if this is this location corresponds to the word cat.

178
00:23:00.460 --> 00:23:03.810
Inderjit Dhillon: Okay, then x 2 will be

179
00:23:04.150 --> 00:23:07.020
Inderjit Dhillon: one in this place and zeros everywhere else.

180
00:23:07.780 --> 00:23:12.550
Inderjit Dhillon: and you can think of it as trying to predict the next word.

181
00:23:13.150 --> 00:23:16.120
Inderjit Dhillon: This xt is

182
00:23:16.340 --> 00:23:20.010
Inderjit Dhillon: v. Dimensional, which is the size of the vocabulary.

183
00:23:20.080 --> 00:23:29.800
Inderjit Dhillon: And so similarly, X. T is also trying to predict the next word. It's one word in the vocabulary. So you have to predict, you know, the index of that particular word.

184
00:23:30.420 --> 00:23:42.800
Inderjit Dhillon: So here is the kind of visual. the The corresponds to X. One cat call response to X 2. In this case we are thinking about 4 dimensional

185
00:23:42.970 --> 00:23:54.090
Inderjit Dhillon: embeddings. and then. you know, you could try and predict why one after Hello. you could try and predict why, 2 after the cat

186
00:23:54.340 --> 00:24:03.410
Inderjit Dhillon: you can think about trying to predict why, 3 or after the sequence. The cat is right, and you can see that the

187
00:24:03.440 --> 00:24:09.300
Inderjit Dhillon: more information that you have. the easier it might get to do the production.

188
00:24:10.750 --> 00:24:16.580
Inderjit Dhillon: So there are various kind of examples that you can think about where

189
00:24:16.660 --> 00:24:23.410
Inderjit Dhillon: this is not just the next word in what your you know. It is not just the next word.

190
00:24:23.420 --> 00:24:33.830
Inderjit Dhillon: It could actually be trying to predict the label of this word. So, for example, you might be trying to look at a sentence and label the individual words with their part of speech.

191
00:24:33.880 --> 00:24:38.660
Inderjit Dhillon: whether something is a noun, whether something is a word, whether something is an adjective.

192
00:24:38.680 --> 00:24:46.130
Inderjit Dhillon: and you can see that then it becomes like a multi class classification problem, right? So a different case of it.

193
00:24:46.680 --> 00:24:54.980
Inderjit Dhillon: So now let's look a little bit at the recurrent neural networks that the nil is covered in class on Wednesday

194
00:24:55.800 --> 00:24:58.870
Inderjit Dhillon: and over here again. Same

195
00:25:01.390 --> 00:25:12.560
Inderjit Dhillon: notation xt is the Tf: input So you can represent this very concisely. right? And the dynamics are like this.

196
00:25:13.450 --> 00:25:19.280
Inderjit Dhillon: Okay, so you can represent it very concisely. But if you unroll in time. then

197
00:25:19.460 --> 00:25:23.920
Inderjit Dhillon: you can represent the the recurrent neural network in this way.

198
00:25:27.450 --> 00:25:30.080
Inderjit Dhillon: Okay. So if you look over here.

199
00:25:30.510 --> 00:25:34.220
Inderjit Dhillon: i'm taking St. Minus one

200
00:25:34.790 --> 00:25:36.290
Inderjit Dhillon: as an input

201
00:25:37.550 --> 00:25:48.800
Inderjit Dhillon: that gets multiplied by W. So I have W. Times St. Minus one. That's the state that carries over from

202
00:25:49.090 --> 00:25:56.150
Madhav Tummala: time t minus one.

203
00:26:00.780 --> 00:26:03.940
Inderjit Dhillon: Is that somebody who just needs to be muted? Or is there a question?

204
00:26:06.880 --> 00:26:19.770
Inderjit Dhillon: Okay. So you have W. Times St. Minus one. And this is the previous state that is carrying over. And remember that at every time step you get input. Xd.

205
00:26:19.960 --> 00:26:22.340
Inderjit Dhillon: That gets multiplied by you

206
00:26:23.360 --> 00:26:25.570
Inderjit Dhillon: right, and then finally

207
00:26:25.710 --> 00:26:31.780
Inderjit Dhillon: you apply some kind of nonlinear function over here. and then you evolve.

208
00:26:32.880 --> 00:26:46.070
Inderjit Dhillon: So this is Sd. Is the hidden state at time T. You can think of it as the memory of the network depending upon what it has seen. So far Right W. Is called the transition matrix.

209
00:26:46.660 --> 00:26:51.900
Inderjit Dhillon: right, and like we saw before you as the word embedding matrix.

210
00:26:52.850 --> 00:27:04.750
Inderjit Dhillon: Okay. So the predicted output at time T: so far, I've only talked about the dynamics over here. right? That's St. This is St.

211
00:27:07.300 --> 00:27:22.040
Inderjit Dhillon: But now, if you think about output at every step. right you can take. we and you just take the state at that time sd you multiply by a. V, and that's what you get.

212
00:27:22.090 --> 00:27:23.340
Inderjit Dhillon: This is.

213
00:27:23.760 --> 00:27:34.820
Inderjit Dhillon: And then, if you want to do kind of multi-class classification also, you can. You know you can look at V. Times, St. And then you can take them, or Max

214
00:27:35.040 --> 00:27:38.570
Inderjit Dhillon: of that, and you know you may have a

215
00:27:38.700 --> 00:27:42.350
Inderjit Dhillon: you may have a soft Max on top of that operation.

216
00:27:43.100 --> 00:27:44.850
Inderjit Dhillon: So that's kind of how.

217
00:27:45.330 --> 00:27:49.550
Inderjit Dhillon: And then the dynamics of R and E can be captured.

218
00:27:49.740 --> 00:27:55.520
Inderjit Dhillon: And then the question as well, what are these matrices? What is W. What is the what is you

219
00:27:55.800 --> 00:27:58.930
Inderjit Dhillon: right? So these need to be learned.

220
00:27:59.590 --> 00:28:02.990
Inderjit Dhillon: Okay. So typically the way R. And ends are trained

221
00:28:03.120 --> 00:28:15.140
Inderjit Dhillon: is that you want to find W. UV. So that you minimize this empirical loss. and the loss over the entire sequence which is of length. T

222
00:28:15.480 --> 00:28:20.050
Inderjit Dhillon: would be. Then, you know, some lost basic class that you have

223
00:28:20.280 --> 00:28:24.070
Inderjit Dhillon: of the output and the supervised lab.

224
00:28:25.690 --> 00:28:26.560
Inderjit Dhillon: Okay.

225
00:28:26.970 --> 00:28:33.380
Inderjit Dhillon: And remember that, of course, even St. Is a function of you, W. And B.

226
00:28:33.510 --> 00:28:45.970
Inderjit Dhillon: As going over here. Okay. So that is what recurrent neural networks do so traditionally in a recurrent neural networks.

227
00:28:46.220 --> 00:28:55.090
Inderjit Dhillon: Only look at the current hidden state. and that's what's going to differ when we talk about attention. Okay.

228
00:28:55.680 --> 00:29:02.740
Inderjit Dhillon: And then, you know, Typically, this is a Ls or an ends, or you learn these.

229
00:29:02.810 --> 00:29:14.250
Inderjit Dhillon: my having this kind of optimization function that you are trying to minimize, which is the average loss over all the subsequences, all the sequences. And then you

230
00:29:14.260 --> 00:29:22.700
Inderjit Dhillon: learn these parameters by using Std. Or a method like Adam, which we discussed a couple of classes before.

231
00:29:23.830 --> 00:29:28.100
Inderjit Dhillon: Yeah. So in particular, if you have a text classification.

232
00:29:28.910 --> 00:29:33.680
Inderjit Dhillon: right, you don't need to output, and every step like I said.

233
00:29:34.820 --> 00:29:42.570
Inderjit Dhillon: So what you really want to do is go from a sentence to a category. So, for example, if you're trying to predict whether an email is spam or not.

234
00:29:42.960 --> 00:29:53.660
Inderjit Dhillon: then you want to go from the sentence. Now, what do you mean by go from a sentence? Well. if you're using an lstm or an O. Sorry if you're using a recurrent neural network.

235
00:29:53.720 --> 00:29:57.520
Inderjit Dhillon: Then you look at, for example, a sentence such as

236
00:29:57.580 --> 00:29:59.740
Inderjit Dhillon: the cat is eating

237
00:30:00.180 --> 00:30:02.610
Inderjit Dhillon: right. and then

238
00:30:02.720 --> 00:30:07.640
Inderjit Dhillon: you get the hidden state after you see eating, so that

239
00:30:07.760 --> 00:30:19.230
Inderjit Dhillon: captures the sentence. So sometimes this s 4 you can refer to as a sentence submitting. and then depending upon the sentence, i'm running.

240
00:30:19.300 --> 00:30:24.370
Inderjit Dhillon: You want to have your binary classification task

241
00:30:24.520 --> 00:30:32.340
Inderjit Dhillon: right, for example, whether this is plausible, or whether this is not plausible right? So?

242
00:30:33.320 --> 00:30:36.370
Inderjit Dhillon: Oh. so that is

243
00:30:37.060 --> 00:30:46.790
Inderjit Dhillon: for an ends record on neural networks, and then you can go multi-layer with or it's just like you can go multi-layer with you know obviously a multi layer perception.

244
00:30:47.030 --> 00:30:54.520
Inderjit Dhillon: You can have multiple layers. So this would be like the first layer. This would be the second layer.

245
00:30:54.960 --> 00:30:58.020
Inderjit Dhillon: right? So the inputs would feed over here.

246
00:30:58.510 --> 00:31:03.830
Inderjit Dhillon: and then the output of this would feed I actually as an info to the second layer.

247
00:31:04.250 --> 00:31:06.880
Inderjit Dhillon: So the output of the previous layer

248
00:31:07.050 --> 00:31:10.840
Inderjit Dhillon: feeds into as the input of the next layer.

249
00:31:11.070 --> 00:31:14.420
Inderjit Dhillon: and then you can think about trying to

250
00:31:14.460 --> 00:31:25.340
Inderjit Dhillon: have either one target. Suppose you are doing classification or kind of multiple sequential target if you are trying to do something like machine translation.

251
00:31:25.990 --> 00:31:37.660
Inderjit Dhillon: right? So that's kind of the idea behind R and Ms. But on an ends actually come with almost in unsolvable problems. Right?

252
00:31:39.110 --> 00:31:52.470
Inderjit Dhillon: One problem is how to look at long term dependencies because you are summarizing the State by just one vector by looking at, You know, the previous state and the most recent input

253
00:31:52.790 --> 00:31:58.670
Inderjit Dhillon: But another very big problem is what's called the vanishing gradient problem.

254
00:32:00.020 --> 00:32:02.520
Inderjit Dhillon: And and I,

255
00:32:04.280 --> 00:32:09.530
Inderjit Dhillon: we're not going to too much detail. But what happens is that if this sequence length is very long.

256
00:32:11.220 --> 00:32:13.920
Inderjit Dhillon: think about how

257
00:32:14.000 --> 00:32:23.580
Inderjit Dhillon: back propagation routes. Okay, When you, for example, using Std. Or Adam. you need to propagate gradients from here

258
00:32:24.580 --> 00:32:26.070
Inderjit Dhillon: to all the way

259
00:32:27.140 --> 00:32:28.230
Inderjit Dhillon: to the beginning.

260
00:32:29.530 --> 00:32:32.190
Inderjit Dhillon: 2 s 0 to to the

261
00:32:32.410 --> 00:32:39.030
Inderjit Dhillon: you want to pay propagate gradients so that you can kind of modify the weights

262
00:32:39.330 --> 00:32:43.550
Inderjit Dhillon: and what ends up happening, and what we want. What one can show is

263
00:32:43.590 --> 00:32:51.590
Inderjit Dhillon: that these gradients become proportional to W to the power t where t is the number of steps that you need to go back.

264
00:32:53.500 --> 00:32:58.760
Inderjit Dhillon: and what can happen is that since it's w to the power t

265
00:33:00.340 --> 00:33:02.790
Inderjit Dhillon: so w to the power t

266
00:33:03.000 --> 00:33:10.430
Inderjit Dhillon: right. The norm of this is proportional to the largest singular value

267
00:33:10.530 --> 00:33:11.470
Inderjit Dhillon: of to.

268
00:33:13.270 --> 00:33:16.650
Inderjit Dhillon: and If Sigma, one is less than one.

269
00:33:16.770 --> 00:33:20.450
Inderjit Dhillon: then this quantity signal one to the power. T

270
00:33:20.530 --> 00:33:22.520
Inderjit Dhillon: goes to 0 very quickly.

271
00:33:23.670 --> 00:33:34.090
Inderjit Dhillon: As a result, what can happen is because of the limited precision that we have in a computer right in a floating point arithmetic.

272
00:33:35.660 --> 00:33:44.820
Inderjit Dhillon: The underflow threshold is 10 to the power minus 38, but like in the machines that are being built the gpus that are being built. They're using.

273
00:33:44.960 --> 00:33:48.950
Inderjit Dhillon: you know, something called B float 6 B. Float.

274
00:33:49.210 --> 00:33:52.010
Inderjit Dhillon: 16 or float 16,

275
00:33:53.160 --> 00:34:02.710
Inderjit Dhillon: and there the underflow threshold can be very low. So the chances of under flow. And as a result this is called the vanishing gradient problem

276
00:34:02.730 --> 00:34:04.640
Inderjit Dhillon: can actually be severe

277
00:34:04.740 --> 00:34:09.449
Inderjit Dhillon: enough that you really cannot train on an ends in a stable now.

278
00:34:10.020 --> 00:34:13.920
Inderjit Dhillon: So the solution that to that was

279
00:34:14.139 --> 00:34:24.489
Inderjit Dhillon: and these were actually very successful is to use these Lsdms, or a slight variance of them called grues, which are gated recurrent units.

280
00:34:25.420 --> 00:34:28.389
Inderjit Dhillon: Okay. So that was kind of state of the art

281
00:34:28.710 --> 00:34:40.730
Inderjit Dhillon: I want to say 6 years ago, right where there were very lot of impressive work. By the way, on being able to train Lsdms of

282
00:34:41.130 --> 00:34:46.590
Inderjit Dhillon: you know, 6 layers of stms, many more layers of Lsdm

283
00:34:46.719 --> 00:34:52.679
Inderjit Dhillon: that could do, for example, machine translation, and would get state of the out.

284
00:34:52.980 --> 00:34:59.930
Inderjit Dhillon: And so what Lsdms do? Is they kind of modify the simple architecture of

285
00:35:00.090 --> 00:35:03.320
Inderjit Dhillon: on an ends? Which is this with

286
00:35:03.520 --> 00:35:15.320
Inderjit Dhillon: introducing some additional gates. Okay. i'm. Not going to go into detail, because, as we will soon talk about transformers have really taken over.

287
00:35:15.370 --> 00:35:19.240
Inderjit Dhillon: and Lsdms are actually not now being used for

288
00:35:19.370 --> 00:35:22.910
Inderjit Dhillon: these applications. Okay.

289
00:35:24.650 --> 00:35:32.670
Inderjit Dhillon: So I mentioned your little machine translation a few times or machine translation a few times. So let's talk a little bit about that.

290
00:35:34.380 --> 00:35:40.730
Inderjit Dhillon: So machine translation is a problem that you're given an input sentence in one language.

291
00:35:40.990 --> 00:35:47.630
Inderjit Dhillon: and then you want to to output this translated sentence. Okay. So the training data is

292
00:35:47.800 --> 00:35:55.420
Inderjit Dhillon: kind of a set of input output pairs. So this is very much kind of a supervised setting right that somebody gives you

293
00:35:56.700 --> 00:36:07.080
Inderjit Dhillon: input sentences in one language, and it's translations in a different language. Right? So over here you can see, this is the input sentence.

294
00:36:07.570 --> 00:36:12.800
Inderjit Dhillon: and this is the output in English right? It's awesome sauce.

295
00:36:13.170 --> 00:36:18.520
Inderjit Dhillon: So typically. when we used Lsdms. For this task

296
00:36:18.810 --> 00:36:23.650
Inderjit Dhillon: we use this approach called Encoder Decoder, which is a general

297
00:36:23.760 --> 00:36:25.000
Inderjit Dhillon: approach

298
00:36:25.570 --> 00:36:33.880
Inderjit Dhillon: not necessarily even tied to an Lsd. And we'll see, for example, transform or architectures can also be encoder decoder architectures.

299
00:36:34.050 --> 00:36:41.850
Inderjit Dhillon: So here we can use, let's say, an lstm to encode the inputs

300
00:36:42.060 --> 00:36:45.250
Inderjit Dhillon: sentence input as a late inventor.

301
00:36:45.370 --> 00:36:49.120
Inderjit Dhillon: It's very similar to what we talked about in classification. Right? So

302
00:36:49.150 --> 00:36:52.430
Inderjit Dhillon: we look at these Xi's

303
00:36:52.470 --> 00:37:03.860
Inderjit Dhillon: over here. I think we've used edges instead of S's for the hidden state. Right? We have the dynamics captured by the transition matrix. and at the end of seeing the last token

304
00:37:04.020 --> 00:37:05.230
Inderjit Dhillon: we get a state.

305
00:37:05.960 --> 00:37:08.030
Inderjit Dhillon: Okay. So that is the

306
00:37:08.310 --> 00:37:12.030
Inderjit Dhillon: sentence embedding. So that is the encoding

307
00:37:12.210 --> 00:37:14.640
Inderjit Dhillon: of the input sentence.

308
00:37:15.240 --> 00:37:20.720
Inderjit Dhillon: Then, when we get the encoding, we actually need to do a decoding right. We

309
00:37:20.980 --> 00:37:35.280
Inderjit Dhillon: take the decoding, and then what we want to do is decode this. and do the translation. So use Lsdms again to generate the a a sentence based upon this

310
00:37:35.410 --> 00:37:43.860
Inderjit Dhillon: latent. Vector So that's how kind of Lsdms proceed in order to do machine translation.

311
00:37:44.410 --> 00:37:49.480
Inderjit Dhillon: and, like I said, you know Lsdms were hugely successful. People used.

312
00:37:49.570 --> 00:37:52.160
Inderjit Dhillon: you know, multiple layers.

313
00:37:52.790 --> 00:38:00.950
Inderjit Dhillon: And so you have over Here is an example of economic growth has slowed down in recent years. Very true.

314
00:38:01.270 --> 00:38:06.670
Inderjit Dhillon: right? And it's translation into franchise.

315
00:38:06.800 --> 00:38:16.320
Inderjit Dhillon: Okay. So you have an encoder over here. My, this is the wi I got. This is the hidden state.

316
00:38:17.100 --> 00:38:21.060
Inderjit Dhillon: and it is going over here

317
00:38:21.840 --> 00:38:26.230
Inderjit Dhillon: multiple layers of the encoder. This is the final state

318
00:38:26.440 --> 00:38:30.170
Inderjit Dhillon: final embedding, and this is being then taken as as input

319
00:38:30.310 --> 00:38:33.390
Inderjit Dhillon: and the recording is happening over here. Good.

320
00:38:35.460 --> 00:38:49.410
Inderjit Dhillon: Now. before the transformer we work came along, and if some of you are familiar with the initial transform or paper, it was called attention is all unique.

321
00:38:49.560 --> 00:38:53.280
Inderjit Dhillon: Okay. But this notion of attention

322
00:38:53.540 --> 00:38:59.530
Inderjit Dhillon: already existed right? So attention and it actually existed in

323
00:38:59.540 --> 00:39:01.350
Inderjit Dhillon: Lsdm.

324
00:39:02.050 --> 00:39:03.590
Inderjit Dhillon: Right? So

325
00:39:04.530 --> 00:39:07.440
Inderjit Dhillon: as I and I think I've mentioned you before that

326
00:39:08.380 --> 00:39:16.520
Inderjit Dhillon: we generated all these hidden States. But then we just took the last date as the

327
00:39:16.670 --> 00:39:18.030
Inderjit Dhillon: the

328
00:39:18.060 --> 00:39:20.420
Inderjit Dhillon: output of the encoder.

329
00:39:20.450 --> 00:39:25.690
Inderjit Dhillon: and then we only use the last state to predict the to do the

330
00:39:25.740 --> 00:39:26.640
Inderjit Dhillon: they connect

331
00:39:27.500 --> 00:39:30.110
Inderjit Dhillon: instead. What you can do is

332
00:39:30.790 --> 00:39:33.570
Inderjit Dhillon: so. Each output world

333
00:39:33.620 --> 00:39:38.620
Inderjit Dhillon: usually is only related. So so here's the problem, right, which which

334
00:39:39.110 --> 00:39:40.850
Inderjit Dhillon: captures attention.

335
00:39:41.920 --> 00:39:46.790
Inderjit Dhillon: the notion of attention. So think about a word.

336
00:39:48.610 --> 00:39:51.290
Inderjit Dhillon: Think in in particular machine translation.

337
00:39:52.200 --> 00:39:55.440
Inderjit Dhillon: Think about. Let's say an English language.

338
00:39:55.480 --> 00:39:57.120
Inderjit Dhillon: and it's French translation.

339
00:39:58.490 --> 00:40:17.200
Inderjit Dhillon: Each output were. Now you can't do translation just by looking at each individual word right? You can't. Just do you know. Look at the first English word. Get a translation. Look at the second English word gets its translation, and that composes a valid word, a valid sentence in the target language. That's not how it works, but

340
00:40:17.400 --> 00:40:22.350
Inderjit Dhillon: each output. Word is related typically only to a subset of input words.

341
00:40:22.810 --> 00:40:27.020
Inderjit Dhillon: right? So it's giving attention

342
00:40:27.430 --> 00:40:33.200
Inderjit Dhillon: to a subset of. So each output word can is basically only needs to attend

343
00:40:33.220 --> 00:40:46.930
Inderjit Dhillon: to a subset of input words. But this subset is not just a size one. The subset is larger. So what you can do is you can carry around this thing called attention. Okay, so how is it done so let you be the current.

344
00:40:47.140 --> 00:40:53.040
Inderjit Dhillon: So so suppose you are at the decoding stage, You is the current decoder leading state.

345
00:40:53.460 --> 00:40:54.320
Inderjit Dhillon: and

346
00:40:54.580 --> 00:41:00.980
Inderjit Dhillon: that V. One through. Vm: right. Be the latent state. Sorry for each input word.

347
00:41:02.580 --> 00:41:09.350
Inderjit Dhillon: And then what you do is you compute the weight of each State by taking

348
00:41:09.830 --> 00:41:14.130
Inderjit Dhillon: and inner product with the

349
00:41:14.320 --> 00:41:18.910
Inderjit Dhillon: current latent state and with all the previous

350
00:41:19.170 --> 00:41:28.330
Inderjit Dhillon: input, for latent state vectors for each word. Okay. So then you get a context vector which basically I can assemble.

351
00:41:28.700 --> 00:41:31.330
Inderjit Dhillon: This is

352
00:41:31.900 --> 00:41:35.350
Inderjit Dhillon: V. Times, P. So this is the vector, P.

353
00:41:35.720 --> 00:41:40.610
Inderjit Dhillon: Right? And I can basically take this: this: P. One

354
00:41:40.690 --> 00:41:43.910
Inderjit Dhillon: is soft Max of you transpose V. One

355
00:41:44.490 --> 00:41:49.010
Inderjit Dhillon: right. and so I take the soft Max off.

356
00:41:49.620 --> 00:41:54.990
Inderjit Dhillon: So so remember Softmax is going to be so. This set of numbers, these end numbers will

357
00:41:55.090 --> 00:41:56.480
Inderjit Dhillon: some up to one.

358
00:41:56.760 --> 00:42:05.090
Inderjit Dhillon: so you can think of V. Times. P. As taking a convex combination, so P. One through pn and up to one.

359
00:42:05.260 --> 00:42:14.870
Inderjit Dhillon: And I'm: essentially taking the latent state forward by looking at this attention. Vector

360
00:42:16.180 --> 00:42:22.060
Inderjit Dhillon: Okay. So we will. We will discuss this a little bit more. Okay, so here is kind of how

361
00:42:22.260 --> 00:42:24.910
Inderjit Dhillon: how it works. Right? So you have

362
00:42:25.200 --> 00:42:28.690
Inderjit Dhillon: this encoder. Let's say this is kind of the Lstm.

363
00:42:30.370 --> 00:42:34.300
Inderjit Dhillon: Now earlier we were only taking this over to the decoder.

364
00:42:35.200 --> 00:42:45.550
Inderjit Dhillon: But now we are taking all of them over to this decoder right? And we are going to basically just look at this kind of operation that we will take

365
00:42:45.810 --> 00:42:46.900
Inderjit Dhillon: the

366
00:42:47.080 --> 00:42:56.100
Inderjit Dhillon: inner product with respect to the current latent state of the decoder with each of these in quarters.

367
00:42:56.960 --> 00:43:01.100
Inderjit Dhillon: Okay. and then

368
00:43:01.120 --> 00:43:04.030
Inderjit Dhillon: we will kind of, you know, compute these

369
00:43:04.140 --> 00:43:17.110
Inderjit Dhillon: softmax scores which capture the attention right? And then that's how we will get our context. Vector. Okay. So this context vector. Is

370
00:43:17.310 --> 00:43:20.970
Inderjit Dhillon: this: vector. Over here? V. Times, P.

371
00:43:21.200 --> 00:43:32.770
Inderjit Dhillon: So you know, here we've written it much more concisely. And here you are thinking in terms of like the you know the diagram as to how this all this

372
00:43:32.980 --> 00:43:37.990
Inderjit Dhillon: attention there works. So this is the attention layer that takes

373
00:43:39.490 --> 00:43:53.300
Inderjit Dhillon: V. One through V. N. As inputs. so these are the hidden states of each stage of the and quarter. and they the output of the attention layer is

374
00:43:53.430 --> 00:43:57.040
Inderjit Dhillon: over here, and it's captured by

375
00:43:57.440 --> 00:43:58.580
Inderjit Dhillon: this

376
00:44:00.160 --> 00:44:03.890
Inderjit Dhillon: competition. And this is what's called attention.

377
00:44:05.780 --> 00:44:14.450
Inderjit Dhillon: So this is how kind of things were done. like I said before the transformer architecture came along.

378
00:44:15.950 --> 00:44:20.290
Inderjit Dhillon: and, like I said, the transformer is really like a you know.

379
00:44:20.450 --> 00:44:29.930
Inderjit Dhillon: transformative concept that has appeared in deep learning machine learning to model sequences.

380
00:44:30.490 --> 00:44:42.760
Inderjit Dhillon: and like i'm sure many of you have played around with Chat, Gpd, and so on. So most of these large language models or chat bots, and so on that are there

381
00:44:43.060 --> 00:44:48.910
Inderjit Dhillon: are using at the core these transformer architectures.

382
00:44:49.660 --> 00:44:53.980
Inderjit Dhillon: So let's see how the transformer architecture works.

383
00:44:55.980 --> 00:45:03.250
Inderjit Dhillon: Okay, so the paper title was very catchy. Attention is all you need appeared in 2,017.

384
00:45:03.890 --> 00:45:07.870
Inderjit Dhillon: Why, they regard it as now one of the most influential papers

385
00:45:08.060 --> 00:45:11.220
Inderjit Dhillon: in current day, deep learning.

386
00:45:11.770 --> 00:45:16.450
Inderjit Dhillon: and it's an architecture, as the title says, relies entirely on attention.

387
00:45:17.520 --> 00:45:26.550
Inderjit Dhillon: Previously there was quite a lot of work on using on and and also using Cnn. So we Haven't talked about that. But you know, using Cnn's also for sequence data.

388
00:45:28.250 --> 00:45:35.000
Inderjit Dhillon: And this paper, really, you know. focused its attention on neural machine translation.

389
00:45:36.710 --> 00:45:48.010
Inderjit Dhillon: And there is some talk that you know. The authors of the paper also probably did not realize how important or how many applications this paper.

390
00:45:48.060 --> 00:45:54.600
Inderjit Dhillon: the or this architecture would lead to right, but it's really had like a tremendous tremendous impact.

391
00:45:55.820 --> 00:46:00.780
Inderjit Dhillon: So let's look at it from current, you know, for the machine translation.

392
00:46:01.030 --> 00:46:02.460
Inderjit Dhillon: Oh.

393
00:46:02.530 --> 00:46:03.720
Inderjit Dhillon: application.

394
00:46:04.190 --> 00:46:07.880
Inderjit Dhillon: So the transformer is kind of multiple layers

395
00:46:08.440 --> 00:46:09.930
Inderjit Dhillon: often encoder.

396
00:46:11.840 --> 00:46:19.610
Inderjit Dhillon: And if you just look at the encoder part right, it basically looks like an Mlp. A multi layer perceptron, right? Which is basically chained together.

397
00:46:22.010 --> 00:46:27.990
Inderjit Dhillon: Okay, but these are, do this, this encoding? I mean, there. There are multiple layers of this.

398
00:46:28.010 --> 00:46:32.090
Inderjit Dhillon: and we'll see that instead of there being a very sequential.

399
00:46:32.100 --> 00:46:41.300
Inderjit Dhillon: so you know, in Lsdms it's a very sequential in order. and what we will see is that this transformer encoder

400
00:46:41.340 --> 00:46:45.480
Inderjit Dhillon: is very parallel. You can actually do these competitions in parallel

401
00:46:45.620 --> 00:46:49.330
Inderjit Dhillon: one of the reasons for it success. And then

402
00:46:49.570 --> 00:46:53.320
Inderjit Dhillon: and there is going to be an attention layer over here.

403
00:46:53.400 --> 00:46:55.250
Inderjit Dhillon: and then something after that.

404
00:46:56.020 --> 00:47:01.780
Inderjit Dhillon: and you stack multiple layers of this, and this becomes your overall encoder.

405
00:47:02.010 --> 00:47:03.180
Inderjit Dhillon: and then

406
00:47:03.340 --> 00:47:13.840
Inderjit Dhillon: you have a decoder, and you have kind of multiple layers of the decoder. And then you kind of output the translated sentence.

407
00:47:15.930 --> 00:47:16.960
Inderjit Dhillon: So

408
00:47:17.310 --> 00:47:20.670
Inderjit Dhillon: we talked about encoder and decoder.

409
00:47:21.050 --> 00:47:24.500
Inderjit Dhillon: If you look into detail, you unpack these.

410
00:47:25.950 --> 00:47:30.490
Inderjit Dhillon: There's each and quarter layer is self attention.

411
00:47:31.250 --> 00:47:34.350
Inderjit Dhillon: followed by a feed forward network. And, like I said.

412
00:47:36.180 --> 00:47:43.460
Inderjit Dhillon: this is very much kind of like a parallel kind of operation. So, as a result, both of these operations

413
00:47:43.660 --> 00:47:47.530
Inderjit Dhillon: can be done efficiently on Gpus or Tp.

414
00:47:48.330 --> 00:47:58.160
Inderjit Dhillon: The decoder as self attention. and then encoder decoder attention and a feed forward name.

415
00:47:59.160 --> 00:48:02.660
Inderjit Dhillon: So the decoder is just slightly different than the encoder.

416
00:48:06.520 --> 00:48:11.290
Inderjit Dhillon: so some of the notions hold from what we have

417
00:48:11.500 --> 00:48:15.020
Inderjit Dhillon: learned before. So each word

418
00:48:15.810 --> 00:48:23.530
Inderjit Dhillon: So so right now we are just thinking of transformers as taking input. let's say a sentence in a language.

419
00:48:24.400 --> 00:48:36.750
Inderjit Dhillon: Each of the word in the sentence has a corresponding latent. Vector initially, the world embedding for each word. And then one simple way of thinking of an encoder is the following.

420
00:48:37.540 --> 00:48:39.290
Inderjit Dhillon: It receives a list of

421
00:48:40.580 --> 00:48:44.160
Inderjit Dhillon: it, receives a list of vectors as input

422
00:48:46.010 --> 00:48:53.120
Inderjit Dhillon: right, which is what we've seen. and it outputs a list of vectors. So if there are

423
00:48:54.110 --> 00:48:57.320
Inderjit Dhillon: 10 words which are in the input

424
00:48:59.130 --> 00:49:03.110
Inderjit Dhillon: 10 vectors as input, it outputs.

425
00:49:06.570 --> 00:49:11.090
Inderjit Dhillon: and what happens within the encoder within the encoder.

426
00:49:12.000 --> 00:49:18.810
Inderjit Dhillon: You take these vectors and you've passed through them. Well, we've talked about it right. You pass through them a self attention

427
00:49:18.970 --> 00:49:20.450
Inderjit Dhillon: and a feed forward network.

428
00:49:21.710 --> 00:49:29.620
Inderjit Dhillon: So self attention and okay. But the way you think of the style for tension is, suppose you have

429
00:49:30.440 --> 00:49:35.060
Inderjit Dhillon: the import is just 2 words thinking machines.

430
00:49:35.710 --> 00:49:39.760
Inderjit Dhillon: You have a 4 dimensional vector embedding.

431
00:49:41.550 --> 00:49:43.460
Inderjit Dhillon: Sorry

432
00:49:43.930 --> 00:49:48.830
Inderjit Dhillon: of the word thinking stability 4 way 4 dimensional

433
00:49:48.910 --> 00:49:51.400
Inderjit Dhillon: vector embedding for machines.

434
00:49:52.430 --> 00:49:54.060
Inderjit Dhillon: The self attention network

435
00:49:55.530 --> 00:50:03.550
Inderjit Dhillon: transforms. Hence the name transformer. thinking this 4 dimensional vector into another 4 dimensional right.

436
00:50:04.910 --> 00:50:06.530
Inderjit Dhillon: And again, this goes on

437
00:50:08.180 --> 00:50:13.090
Inderjit Dhillon: the feed forward neural network also translates it into 4 vectors, and of course

438
00:50:13.390 --> 00:50:17.400
Inderjit Dhillon: this then, goes as input to the encoder

439
00:50:18.630 --> 00:50:24.360
Inderjit Dhillon: to the second layer of the right. They can be multiple layers. They can be 6 layers. They can be 12 years.

440
00:50:26.010 --> 00:50:31.600
Inderjit Dhillon: Okay. So now let's unpack this. So this is probably the most interesting part.

441
00:50:33.090 --> 00:50:34.520
Inderjit Dhillon: What is self attention?

442
00:50:36.660 --> 00:50:48.500
Inderjit Dhillon: A game? As we discussed about self attention or attention, and Lsdms. The actual meaning of each word in a sentence

443
00:50:48.800 --> 00:50:53.060
Inderjit Dhillon: maybe related to other words in the sentence.

444
00:50:54.880 --> 00:50:58.320
Inderjit Dhillon: Okay. So, for example, consider the input sentence.

445
00:50:58.800 --> 00:51:03.270
Inderjit Dhillon: The animal did not, did not or didn't didn't

446
00:51:03.790 --> 00:51:07.420
Inderjit Dhillon: across the street because it was too tired.

447
00:51:08.690 --> 00:51:10.960
Inderjit Dhillon: Okay. Now.

448
00:51:12.840 --> 00:51:24.810
Inderjit Dhillon: typically the tokenization that is used in these models is not just tokenizing by each word. There's something called wordp.

449
00:51:25.330 --> 00:51:35.310
Inderjit Dhillon: And so typically the dimensionality is not as high as the total number of words. It's actually much lower. It's maybe around 30,000 or so.

450
00:51:35.630 --> 00:51:36.710
Inderjit Dhillon: So.

451
00:51:36.900 --> 00:51:50.940
Inderjit Dhillon: Given a sentence, you you know. Given this sentence right? The animal didn't cross the street because it was too tired. You extract the word pieces so the word pieces are, though animal didn't

452
00:51:51.200 --> 00:51:54.880
Inderjit Dhillon: cross the street and so on.

453
00:51:55.380 --> 00:51:57.940
Inderjit Dhillon: This is a input sentence

454
00:51:59.550 --> 00:52:05.370
Inderjit Dhillon: if you look at the input sentence, and if you look at the word it

455
00:52:06.420 --> 00:52:09.850
Inderjit Dhillon: Oh. well, what is this? It referring to

456
00:52:12.010 --> 00:52:15.120
Inderjit Dhillon: possibility is that it could be referring to the animal

457
00:52:17.200 --> 00:52:29.210
Inderjit Dhillon: it's possible that could be referring to the street in this case, you know. you know, given the entire sentence. the animal didn't cross the street because it was too tired.

458
00:52:29.610 --> 00:52:30.550
Inderjit Dhillon: You know

459
00:52:30.770 --> 00:52:38.920
Inderjit Dhillon: we know that it is referring to the animal, but if you just saw this word it, it could be actually referring to the animal or the street.

460
00:52:39.560 --> 00:52:42.020
Inderjit Dhillon: So it can basically have attention

461
00:52:42.560 --> 00:52:46.860
Inderjit Dhillon: on any of the words in that sentence. So any word

462
00:52:47.180 --> 00:52:51.360
Inderjit Dhillon: and our attention to any word in the sentence.

463
00:52:51.760 --> 00:52:54.940
Inderjit Dhillon: And that's what self attention models.

464
00:52:56.260 --> 00:53:07.070
Inderjit Dhillon: It models the relationship of every word or every sorry input or every token input to to every other input to.

465
00:53:07.360 --> 00:53:09.600
Inderjit Dhillon: And this attention needs to be learned

466
00:53:10.870 --> 00:53:13.040
Inderjit Dhillon: based upon the task that you have

467
00:53:13.870 --> 00:53:17.310
Inderjit Dhillon: right. So the task might be machine translation.

468
00:53:17.390 --> 00:53:22.290
Inderjit Dhillon: in which case the attention mechanism will reveal

469
00:53:23.120 --> 00:53:30.440
Inderjit Dhillon: the more important aspects of you know how the different award is related to other words.

470
00:53:31.580 --> 00:53:39.280
Inderjit Dhillon: Okay. So so this is actually supposed to show alert neural network. And so this is, you know, dark orange.

471
00:53:39.510 --> 00:53:42.930
Inderjit Dhillon: And this means that

472
00:53:43.130 --> 00:53:47.640
Inderjit Dhillon: the network or the attention was highest between it

473
00:53:47.920 --> 00:53:51.030
Inderjit Dhillon: and the animal, and then we'll see

474
00:53:51.420 --> 00:53:53.830
Inderjit Dhillon: smaller values over here.

475
00:53:56.300 --> 00:53:56.890
Inderjit Dhillon: Yeah.

476
00:53:58.330 --> 00:54:03.130
Inderjit Dhillon: how is it done? How is the tension calculated? Okay. So this is kind of the

477
00:54:03.440 --> 00:54:07.020
Inderjit Dhillon: meat of the material here.

478
00:54:08.430 --> 00:54:10.710
Inderjit Dhillon: Okay. So this is the self attention. Layer

479
00:54:12.450 --> 00:54:23.560
Inderjit Dhillon: input vectors are X one through Xn: so think about the sequence length being in each is, let's say.

480
00:54:24.290 --> 00:54:29.340
Inderjit Dhillon: d dimensional, vector and this toy example D is 4,

481
00:54:29.470 --> 00:54:33.490
Inderjit Dhillon: 1 2 3 4. Okay. So this is x one.

482
00:54:34.610 --> 00:54:36.100
Inderjit Dhillon: This is X 2:

483
00:54:37.590 --> 00:54:42.530
Inderjit Dhillon: Okay. So here we are, just looking at N. Equal to. So we are thinking machines.

484
00:54:44.270 --> 00:54:49.400
Inderjit Dhillon: Then the way it will work is that we'll take each of these. We take this embedding.

485
00:54:50.110 --> 00:54:51.920
Inderjit Dhillon: and we'll actually produce

486
00:54:53.040 --> 00:54:54.560
Inderjit Dhillon: 3 vectors from it.

487
00:54:56.440 --> 00:54:58.060
Inderjit Dhillon: One is called the query.

488
00:54:59.220 --> 00:55:03.010
Inderjit Dhillon: One is called the key. One is called the valley.

489
00:55:04.930 --> 00:55:07.380
Inderjit Dhillon: and the way we will produce these

490
00:55:07.820 --> 00:55:10.040
Inderjit Dhillon: is through linear transformations.

491
00:55:11.050 --> 00:55:15.260
Inderjit Dhillon: So we literally linearly transform in this case

492
00:55:15.450 --> 00:55:20.920
Inderjit Dhillon: a four-dimensional, vector 2, 3 dimensional vectors. 3 of them

493
00:55:21.920 --> 00:55:24.950
Inderjit Dhillon: the query, vector the key, vector

494
00:55:26.390 --> 00:55:28.250
and the value, vector

495
00:55:29.010 --> 00:55:34.990
Inderjit Dhillon: and these are through linear transformation. Then the linear transformations are captured by

496
00:55:35.360 --> 00:55:40.270
Inderjit Dhillon: these 3 matrices. W. Superscript queue for query.

497
00:55:40.830 --> 00:55:43.390
Inderjit Dhillon: Wk: for key

498
00:55:43.790 --> 00:55:46.600
Inderjit Dhillon: Wv. For a value.

499
00:55:46.920 --> 00:55:49.530
Inderjit Dhillon: And these 3 matrices need to be learned.

500
00:55:50.780 --> 00:55:58.370
Inderjit Dhillon: Okay. but once learned. they transform a 4 dimensional vector to a 3 dimensional vector

501
00:55:59.480 --> 00:56:05.380
Inderjit Dhillon: Okay, as the result. And if the operation is Xi. Times, Wq.

502
00:56:06.240 --> 00:56:10.400
Inderjit Dhillon: This has to be a 4 by 3, vector 4 by 3 matrix. Okay.

503
00:56:13.800 --> 00:56:17.720
Inderjit Dhillon: So you have these 3 W

504
00:56:18.220 --> 00:56:19.660
Inderjit Dhillon: Wy.

505
00:56:20.770 --> 00:56:22.890
Inderjit Dhillon: Sorry W. For value.

506
00:56:22.990 --> 00:56:25.830
Inderjit Dhillon: Q. For query K. 4 Key

507
00:56:26.270 --> 00:56:27.510
Inderjit Dhillon: V for value.

508
00:56:28.210 --> 00:56:31.370
Inderjit Dhillon: and you get this vector, so X. One

509
00:56:32.460 --> 00:56:37.370
Inderjit Dhillon: is associated now with Q. One with K, one with V. One

510
00:56:37.930 --> 00:56:42.300
Inderjit Dhillon: so 3 different linear transformations. 3 different vectors.

511
00:56:42.830 --> 00:56:45.920
Inderjit Dhillon: and these are now used to compute attention.

512
00:56:47.340 --> 00:56:51.700
Inderjit Dhillon: Okay, and how are they used to compute attention? Let's look. Let's look at it.

513
00:56:53.330 --> 00:56:57.660
Inderjit Dhillon: or what we are going to do is that for each word

514
00:56:58.790 --> 00:57:06.580
Inderjit Dhillon: we are going to compute the scores to determine how much focus to put on other input words right? We saw it with the

515
00:57:06.600 --> 00:57:09.620
Inderjit Dhillon: you know, with this other sentence right over here.

516
00:57:09.970 --> 00:57:13.900
Inderjit Dhillon: the animal didn't cross the street because it was too tired.

517
00:57:14.320 --> 00:57:20.540
Inderjit Dhillon: And here we have this, you know very simple example of, you know. Think by example over here being carried forward.

518
00:57:21.410 --> 00:57:24.210
Inderjit Dhillon: So the way we will compute the attention

519
00:57:25.110 --> 00:57:36.490
Inderjit Dhillon: for word. I toward. Sorry for word. J. Toward I is by the inner product between the corresponding query and the corresponding key.

520
00:57:43.000 --> 00:57:44.420
Inderjit Dhillon: Okay, so let's see

521
00:57:45.620 --> 00:57:49.980
Inderjit Dhillon: from from thinking this is x one. This is 4 dimensional.

522
00:57:50.610 --> 00:58:02.890
Inderjit Dhillon: We have 4 by 3 matrices that convert x, one to Q. One, a, 4 by 3 matrix. So it converts x, one to K. One another, 4 by 3 matrix that converts

523
00:58:03.320 --> 00:58:04.990
Inderjit Dhillon: x, 1, 2, V. One.

524
00:58:06.010 --> 00:58:08.480
Inderjit Dhillon: So this is competition is carrying on.

525
00:58:09.230 --> 00:58:13.930
Inderjit Dhillon: and at the same time we can do the same combination for the next token

526
00:58:14.670 --> 00:58:17.340
Inderjit Dhillon: at the same time, right. So this

527
00:58:17.350 --> 00:58:23.890
Inderjit Dhillon: competition can be done in parallel with this competition given the next. So this is how it might proceed.

528
00:58:24.050 --> 00:58:30.870
Inderjit Dhillon: You have an x, one, and x 2. We can compute simultaneous the Q. One, Q. 2, K. One, K. 2, V, one, V. 2,

529
00:58:32.030 --> 00:58:33.140
Inderjit Dhillon: and then

530
00:58:34.520 --> 00:58:38.100
Inderjit Dhillon: the attention is, Qi. Transpose Kj.

531
00:58:39.030 --> 00:58:40.030
Inderjit Dhillon: So

532
00:58:44.270 --> 00:58:48.860
Inderjit Dhillon: Q. I transpose Kj: and this is done for every ij

533
00:58:51.130 --> 00:58:53.180
Inderjit Dhillon: in the sequence.

534
00:58:54.000 --> 00:59:01.940
Inderjit Dhillon: So if n is 10, then you will have Qi. Transpose Kj.

535
00:59:02.040 --> 00:59:06.810
Inderjit Dhillon: Where I varies from one to 10, and J. Varies from one to 10,

536
00:59:07.160 --> 00:59:09.280
Inderjit Dhillon: so they'll actually be 100. Well, this

537
00:59:10.900 --> 00:59:14.660
Inderjit Dhillon: okay in the case very simple example. I I have

538
00:59:14.950 --> 00:59:18.380
Inderjit Dhillon: just Q. One K. One, and Q, one K. 2,

539
00:59:19.090 --> 00:59:20.470
Inderjit Dhillon: and then for here

540
00:59:20.680 --> 00:59:24.260
Inderjit Dhillon: I'll have Q. 2, K. One and Q. 2.

541
00:59:24.530 --> 00:59:26.550
Inderjit Dhillon: I've only illustrated this over here.

542
00:59:27.400 --> 00:59:31.890
Inderjit Dhillon: Okay. So here, this says that the attention this word is placing

543
00:59:32.220 --> 00:59:38.390
Inderjit Dhillon: on itself. Is that suppose this answer, this in our product comes out to be 112,

544
00:59:40.070 --> 00:59:44.110
Inderjit Dhillon: and it is larger than the attention that you place

545
00:59:44.720 --> 00:59:49.380
Inderjit Dhillon: on the word Q. 2. On the second word, which is machines.

546
00:59:50.170 --> 00:59:56.240
Inderjit Dhillon: Okay. And similarly, you do. Q. 2: transpose K: one and Q. 2, transpose K: 2.

547
00:59:58.420 --> 01:00:00.300
Inderjit Dhillon: So what you do then is

548
01:00:02.990 --> 01:00:08.000
Inderjit Dhillon: so you compute this inner product. Okay, then you kind of.

549
01:00:08.520 --> 01:00:10.480
Inderjit Dhillon: you know, normalize it

550
01:00:11.020 --> 01:00:22.700
Inderjit Dhillon: by looking at the square root of the dimension. Okay. So this is normalized to 14, because we are dividing my 8,

551
01:00:23.030 --> 01:00:24.940
Inderjit Dhillon: and this is normalized to 12.

552
01:00:26.620 --> 01:00:30.650
Inderjit Dhillon: Then we take soft max of these values

553
01:00:32.930 --> 01:00:35.900
Inderjit Dhillon: again. There is a square root, d, I guess, missing over here.

554
01:00:36.020 --> 01:00:39.100
Inderjit Dhillon: But we take soft mix of these values

555
01:00:39.740 --> 01:00:43.620
Inderjit Dhillon: right? So this will correspond to once we do soft Max.

556
01:00:44.930 --> 01:00:48.330
Inderjit Dhillon: this this sum of numbers for

557
01:00:49.580 --> 01:00:52.850
Inderjit Dhillon: this word we'll add up to one.

558
01:00:54.040 --> 01:01:02.160
Inderjit Dhillon: Then we'll also be able to look at the attention for machines. and that also soft Max will add up to one.

559
01:01:03.820 --> 01:01:10.760
Inderjit Dhillon: But right now we are only looking at this particular word. So what ends up happening is that V. One

560
01:01:12.560 --> 01:01:16.900
Inderjit Dhillon: is then computed with softmax times its value.

561
01:01:18.580 --> 01:01:27.460
Inderjit Dhillon: Okay, remember that V. One was over here. So what this is saying is that this v. One is equal to point 8 8 times this V one.

562
01:01:30.840 --> 01:01:32.200
Inderjit Dhillon: whereas over here

563
01:01:32.350 --> 01:01:36.020
Inderjit Dhillon: V. 2, you can see that it is, you know, very small.

564
01:01:36.360 --> 01:01:47.980
Inderjit Dhillon: that's represented by kind of these grade out colors that it's this vector times point 1, 2, which is much smaller than 0 point, 8, 8,

565
01:01:49.210 --> 01:01:49.920
Inderjit Dhillon: Okay.

566
01:01:50.660 --> 01:01:58.370
Inderjit Dhillon: And then we end up taking the sum of, I guess V. One and v 2,

567
01:01:58.670 --> 01:02:00.020
Inderjit Dhillon: and getting z one.

568
01:02:04.160 --> 01:02:11.880
Inderjit Dhillon: So what has happened is that now. if you think about this process so far, right

569
01:02:12.090 --> 01:02:16.050
Inderjit Dhillon: till here it was only dependent upon this particular world.

570
01:02:17.920 --> 01:02:21.320
Inderjit Dhillon: Then you start looking at the attention to the other world.

571
01:02:23.020 --> 01:02:28.680
Inderjit Dhillon: and then you take V. One and you add the attention from the other world.

572
01:02:29.230 --> 01:02:35.480
Inderjit Dhillon: So now you're bringing in context from the other word in the sentence

573
01:02:36.350 --> 01:02:40.740
Inderjit Dhillon: to the representation, and this then becomes the output.

574
01:02:42.690 --> 01:02:45.770
But one thing that you will notice is.

575
01:02:46.060 --> 01:02:49.280
Inderjit Dhillon: it's not really the cannot really be the output, because

576
01:02:49.570 --> 01:02:54.470
Inderjit Dhillon: well, here I input a 4 dimensional word, 4 dimensional rep vector

577
01:02:55.300 --> 01:02:58.630
Inderjit Dhillon: as the input and I need to output a 4 dimensional. Vector

578
01:02:59.940 --> 01:03:08.450
Inderjit Dhillon: But right now this is only 3 dimensions. Okay. So what does the

579
01:03:09.280 --> 01:03:11.660
Inderjit Dhillon: transformer do

580
01:03:11.760 --> 01:03:19.440
Inderjit Dhillon: before we come to that, you know. Let me kind of just review what we did, right? Which is that we have X.

581
01:03:22.740 --> 01:03:25.680
Inderjit Dhillon: Okay. So now we are looking at, you know.

582
01:03:26.040 --> 01:03:35.820
Inderjit Dhillon: Did so over here. We did this. you know, thinking of this as X one. This is X 2, but I can actually think of the competition and matrix form

583
01:03:36.780 --> 01:03:39.740
Inderjit Dhillon: where I represent this as the first

584
01:03:39.890 --> 01:03:41.210
embedding

585
01:03:41.900 --> 01:03:45.760
Inderjit Dhillon: embedding of the first token. This is the embedding of the second token.

586
01:03:46.220 --> 01:03:58.110
Inderjit Dhillon: So the dimension of this matrix X is number of tokens divided times 4, which is the embedding dimension. Okay? Then. Similarly, I multiply it as before.

587
01:03:58.150 --> 01:04:04.580
Inderjit Dhillon: with a 4 by 3 matrix. And now I got the query, representation of

588
01:04:04.870 --> 01:04:10.190
Inderjit Dhillon: X, the first word. and the quality representation of the second world.

589
01:04:10.520 --> 01:04:13.280
Inderjit Dhillon: So you can make a connection

590
01:04:14.010 --> 01:04:15.390
Inderjit Dhillon: over here

591
01:04:16.710 --> 01:04:20.760
Inderjit Dhillon: by basically these 2 rows that are shown over here.

592
01:04:21.170 --> 01:04:22.930
Inderjit Dhillon: So these 2 vectors

593
01:04:23.020 --> 01:04:26.070
Inderjit Dhillon: become these 2 roles of this matrix.

594
01:04:29.690 --> 01:04:30.430
Inderjit Dhillon: Okay.

595
01:04:31.280 --> 01:04:35.490
Inderjit Dhillon: Similarly, you compute the keys. Now for all vectors.

596
01:04:36.030 --> 01:04:40.490
Inderjit Dhillon: for all the inputs. the values for all

597
01:04:41.580 --> 01:04:42.660
Inderjit Dhillon: the

598
01:04:45.970 --> 01:04:49.600
Inderjit Dhillon: inputs. So this is still here.

599
01:04:51.040 --> 01:04:57.160
Inderjit Dhillon: Okay. So you compute X Times, W. Q. X. Times, W. K. X. Times, Wv.

600
01:04:57.430 --> 01:05:01.260
Inderjit Dhillon: And at the end you get Qkv.

601
01:05:01.510 --> 01:05:05.190
Inderjit Dhillon: For all the words in the sequence.

602
01:05:06.670 --> 01:05:15.520
Inderjit Dhillon: and then you want to do this attention. So here I illustrated. We illustrated only Q. One times. K. One.

603
01:05:16.520 --> 01:05:26.680
Inderjit Dhillon: Let me make it a little bit bigger. Q. One times K. One and Q. One times K. 2. Well. if I look at Q. Transpose

604
01:05:27.230 --> 01:05:30.340
Inderjit Dhillon: Q. Times, K. Transpose this matrix.

605
01:05:31.610 --> 01:05:32.570
Inderjit Dhillon: Then

606
01:05:35.430 --> 01:05:38.100
Inderjit Dhillon: let me make it smaller. Okay.

607
01:05:38.400 --> 01:05:40.810
Inderjit Dhillon: then. Hmm.

608
01:05:46.400 --> 01:05:58.020
Inderjit Dhillon: Sorry. I want to get Q: One: transpose K: One: so you have. Okay. So you have Q: One: transpose K: One. Okay, If you think about U, one, transpose K: one.

609
01:05:58.780 --> 01:06:01.590
Inderjit Dhillon: then I have Q. One: transpose K: 2

610
01:06:01.820 --> 01:06:03.720
Inderjit Dhillon: same, first of all.

611
01:06:04.260 --> 01:06:05.370
Inderjit Dhillon: Second call.

612
01:06:06.900 --> 01:06:07.710
Inderjit Dhillon: Okay.

613
01:06:08.130 --> 01:06:11.750
Inderjit Dhillon: So essentially the so this will be

614
01:06:14.310 --> 01:06:17.490
Inderjit Dhillon: in this example. You know, just 2 values.

615
01:06:17.850 --> 01:06:23.410
Inderjit Dhillon: and they correspond to this one and this value.

616
01:06:24.410 --> 01:06:27.910
Inderjit Dhillon: Okay, and that's the first row, and then you'll have the second row.

617
01:06:29.160 --> 01:06:33.450
Inderjit Dhillon: which will correspond to what's not shown over here. It will correspond to Q. 2.

618
01:06:34.000 --> 01:06:38.470
Inderjit Dhillon: So this corresponds to Q. One, dot K. One.

619
01:06:38.880 --> 01:06:42.120
Inderjit Dhillon: This corresponds to Q. One, dot, k 2.

620
01:06:43.130 --> 01:06:49.640
Inderjit Dhillon: This corresponds to now. Q. 2 times. K. One. This corresponds to Q. 2 times K. 2.

621
01:06:50.910 --> 01:06:56.110
Inderjit Dhillon: So you get a 2 by 2 matrix. You divide by a square root of the dimensionality.

622
01:06:56.570 --> 01:07:00.500
Inderjit Dhillon: and then you multiply it by value to get the new value.

623
01:07:02.880 --> 01:07:03.640
Inderjit Dhillon: Okay.

624
01:07:12.960 --> 01:07:15.330
Inderjit Dhillon: And so Z is given like this.

625
01:07:20.400 --> 01:07:28.310
Inderjit Dhillon: and it's essentially doing this computation because you can look at this once. You compute the soft Max Times. We.

626
01:07:29.860 --> 01:07:35.000
Inderjit Dhillon: it's basically multiplying the softmax of this value and then computing the sum.

627
01:07:38.040 --> 01:07:42.540
Inderjit Dhillon: Okay, and so that's how it gets this matrix. Z:

628
01:07:44.320 --> 01:07:45.640
Inderjit Dhillon: But like I said.

629
01:07:45.790 --> 01:07:49.590
Inderjit Dhillon: it's now translated each of this X.

630
01:07:50.710 --> 01:08:04.180
Inderjit Dhillon: You know it's basically transform this X to first. rural, c. This X to the second row, c. But we would want to get 4 dimensional vectors as outputs.

631
01:08:04.560 --> 01:08:08.630
Inderjit Dhillon: So what the transformer architecture does is, it says, Well.

632
01:08:08.660 --> 01:08:21.710
Inderjit Dhillon: instead of just doing one of these operations that I just described. I'm actually going to do 8 of the multiple of these typically 8. And that's what's called multiple multi-headed attention.

633
01:08:22.149 --> 01:08:26.370
Inderjit Dhillon: They use multiple sets of key value query weights.

634
01:08:29.180 --> 01:08:32.200
Inderjit Dhillon: Each head will output a vector Zi.

635
01:08:32.729 --> 01:08:33.490
Inderjit Dhillon: Okay.

636
01:08:33.750 --> 01:08:40.620
Inderjit Dhillon: So when I say multiple heads, that means i'll have multiple W's for W, not for the

637
01:08:40.660 --> 01:08:44.399
Inderjit Dhillon: 0 at head there will be one for the first head, and so on.

638
01:08:45.120 --> 01:08:48.609
Inderjit Dhillon: Similarly, W. Not sorry. W.

639
01:08:49.800 --> 01:08:55.580
Inderjit Dhillon: Not okay. and W. Not V. And similar, so they'll be.

640
01:08:55.800 --> 01:08:59.609
Inderjit Dhillon: you know. Let's suppose there are 8 attention heads like over here.

641
01:09:01.350 --> 01:09:04.000
Inderjit Dhillon: There will be 8 sets of these matrices.

642
01:09:05.180 --> 01:09:10.240
Inderjit Dhillon: and you will get 8 sets of outputs Z. 0, Z: one, the

643
01:09:10.330 --> 01:09:15.340
Inderjit Dhillon: 7. So you've gone from 4 dimensions

644
01:09:17.979 --> 01:09:18.979
Inderjit Dhillon: to

645
01:09:19.300 --> 01:09:24.109
Inderjit Dhillon: 3 times 8 heads. So 3 dimensions times 8 is 24 dimension.

646
01:09:28.830 --> 01:09:41.479
Inderjit Dhillon: Okay. Now, you want to try and map 24 dimensions back on to 4 dimensions. So what do you do? Well, you just use a linear transform, which goes from 24 to 8. So you basically just use a

647
01:09:43.399 --> 01:09:45.319
Inderjit Dhillon: So you can calculate all of them

648
01:09:45.330 --> 01:09:50.689
Inderjit Dhillon: right. So now you can just count yourself. This is 3 times 8.

649
01:09:51.149 --> 01:09:55.120
Inderjit Dhillon: So this is 24 dimensions. and then

650
01:09:55.210 --> 01:09:59.620
Inderjit Dhillon: you have a 24 by 4 dimension, matrix.

651
01:09:59.790 --> 01:10:03.080
Inderjit Dhillon: and as a result you get 4 dimensions.

652
01:10:04.590 --> 01:10:08.450
Inderjit Dhillon: So the overall architecture is the following.

653
01:10:09.810 --> 01:10:13.590
Inderjit Dhillon: You're given an input sentence, you embed each word.

654
01:10:14.620 --> 01:10:26.520
Inderjit Dhillon: This is only needed at the 0, at least headquarters. Otherwise the output of layer L minus one becomes the input of layer. L.

655
01:10:27.410 --> 01:10:29.790
Inderjit Dhillon: Then you have 8 of these heads

656
01:10:30.580 --> 01:10:34.540
Inderjit Dhillon: you compute. There are these 3 sets of matrices. In each head

657
01:10:34.880 --> 01:10:46.740
Inderjit Dhillon: you get query, key values for each of these 3. Then you get. does he not? Then you have a linear transform. and then you get this out, for the transformer

658
01:10:47.000 --> 01:10:49.670
Inderjit Dhillon: transforms this to this

659
01:10:52.190 --> 01:10:53.750
Inderjit Dhillon: Sorry.

660
01:11:04.530 --> 01:11:11.250
Inderjit Dhillon: Now, one thing that you have noticed is that Well, there's nothing that actually captured the position

661
01:11:11.660 --> 01:11:14.490
Inderjit Dhillon: of this word.

662
01:11:14.980 --> 01:11:21.740
Inderjit Dhillon: Okay. So we'll come a little bit later, and we'll talk about position encoding that you can add basically something else

663
01:11:21.850 --> 01:11:28.180
Inderjit Dhillon: that captures the position of the word in that particular sentence, because, so far

664
01:11:28.530 --> 01:11:34.200
Inderjit Dhillon: you know, it's just been the abandoning of the token and nothing to capture the position. And you can basically have

665
01:11:34.550 --> 01:11:43.080
Inderjit Dhillon: 4 dimensional vectors in this case that we can add to the word embedding word embedding to get the input of the transfer.

666
01:11:44.720 --> 01:11:45.470
Inderjit Dhillon: Okay.

667
01:11:46.370 --> 01:11:53.140
Inderjit Dhillon: So with this I will stop. I've covered quite a lot today, like I said. Transformers are

668
01:11:53.440 --> 01:11:57.540
Inderjit Dhillon: sort of really one of the most important

669
01:11:57.790 --> 01:12:08.960
Inderjit Dhillon: architectures right now. and I think they are quite, quite fascinating in terms of how they work.

670
01:12:11.680 --> 01:12:14.760
Inderjit Dhillon: So any questions

671
01:12:15.840 --> 01:12:17.180
Inderjit Dhillon: that you might have

672
01:12:28.570 --> 01:12:30.120
Inderjit Dhillon: any questions.

673
01:12:33.600 --> 01:12:36.760
Inderjit Dhillon: There's actually a quite a beautiful blog

674
01:12:37.290 --> 01:12:44.630
Inderjit Dhillon: article that you know we've actually taken some of these visuals from.

675
01:12:44.850 --> 01:12:51.770
Inderjit Dhillon: and that's also going to be used by some students in presenting this material next Wednesday.

676
01:12:51.990 --> 01:13:01.840
Inderjit Dhillon: And so, you know, we'll talk a little bit more about that, then. so we'll continue to talk about this next time we'll talk about

677
01:13:01.930 --> 01:13:11.110
Inderjit Dhillon: transformer architectures, and how they can be used in conjunction with something called self-supervised learning

678
01:13:11.400 --> 01:13:23.280
Inderjit Dhillon: to learn these modern day large language models like you know that power chat gpt things like Gpd, 3 Gpd. 3.5, and Gp. For

679
01:13:26.350 --> 01:13:29.340
Inderjit Dhillon: so if there are no questions, I will

680
01:13:30.410 --> 01:13:34.970
Inderjit Dhillon: conclude now. But if there are any questions feel free to ask. Now.

681
01:13:42.020 --> 01:13:55.230
Inderjit Dhillon: okay, if there are no more questions, just a reminder, you know. Make sure that you are signed up for a presentation over the next 2 weeks every student must be part of a team that is presenting.

682
01:13:55.430 --> 01:14:00.360
Inderjit Dhillon: And then for Wednesday I will continue. You know talking about

683
01:14:00.430 --> 01:14:09.400
Inderjit Dhillon: transformers. Talk about self-supervised learning. And then these language models like a bird that can be used for

684
01:14:09.520 --> 01:14:21.870
Inderjit Dhillon: you know different natural language processing tasks, and that lead to kind of state of the Okay. Well, thank you for listening, and I will see you on Wednesday.

685
01:14:22.980 --> 01:14:23.870
Inderjit Dhillon: Thank you.

WEBVTT

1
00:00:24.820 --> 00:00:26.530
Inderjit Dhillon: Okay, hello, everybody.

2
00:00:30.670 --> 00:00:33.170
Inderjit Dhillon: I see people still trickling in.

3
00:00:39.980 --> 00:00:44.070
Inderjit Dhillon: Okay. So today I will continue talking about

4
00:00:44.550 --> 00:00:54.500
Inderjit Dhillon: transformers and how they are used in natural language processing, and we'll come across a very really important idea of using

5
00:00:54.600 --> 00:00:59.540
Inderjit Dhillon: self supervised learning. But before I start, is there any question or

6
00:01:04.459 --> 00:01:15.910
Inderjit Dhillon: okay? If not let me actually start off today's lecture by showing you a video. And this is a video kind of that says how

7
00:01:16.190 --> 00:01:19.360
Inderjit Dhillon: important transformers are. So let me

8
00:01:20.100 --> 00:01:22.180
Inderjit Dhillon: share my screen

9
00:01:24.100 --> 00:01:25.280
Inderjit Dhillon: and

10
00:01:27.490 --> 00:01:29.100
Inderjit Dhillon: share desktop.

11
00:01:36.700 --> 00:01:38.060
Inderjit Dhillon: Looking back.

12
00:01:39.130 --> 00:01:45.530
Inderjit Dhillon: can somebody just tell me whether they can see the video and the Youtube video that I'm going to show.

13
00:01:46.570 --> 00:01:49.270
Nilesh Gupta: Yeah. Okay, Great.

14
00:01:49.350 --> 00:02:01.290
Inderjit Dhillon: So this is on this Karati talking about the you know, the transformer architecture and kind of how important it has been in deep learning

15
00:02:02.200 --> 00:02:11.550
Inderjit Dhillon: what is the most beautiful or surprising idea in deep learning or AI in general, that you've come across. You've seen this field explode

16
00:02:11.950 --> 00:02:16.580
Inderjit Dhillon: and grow in interesting ways. Just what what cool ideas like

17
00:02:16.600 --> 00:02:20.540
Inderjit Dhillon: like we made you sit back and go small, bigger, small.

18
00:02:21.670 --> 00:02:26.080
Inderjit Dhillon: Well, the one that I've been thinking about recently the most probably is the

19
00:02:26.770 --> 00:02:29.350
Inderjit Dhillon: the transformer architecture.

20
00:02:29.470 --> 00:02:45.190
Inderjit Dhillon: So basically neural networks have a lot of architectures that we're Trendy have coming gone for different in a sensory modalities like for vision audio text, you would process them with different look in your own nuts. And recently we've seen these, this convergence towards one architecture, the transformer.

21
00:02:45.200 --> 00:02:58.050
Inderjit Dhillon: and you can feed it video, or you can feed it, You know images or speech or text, and it just couples it up, and it's kind of like a. They have a general purpose. a computer that is also trainable and very efficient to run on our hardware.

22
00:02:58.110 --> 00:03:01.240
Inderjit Dhillon: And so this paper came out in 20,

23
00:03:01.250 --> 00:03:09.260
Inderjit Dhillon: 16. I want to say attention is all you need. The question is all you need. You criticize the paper title in retrospect. It wasn't

24
00:03:10.760 --> 00:03:27.840
Inderjit Dhillon: it didn't foresee the bigness of the impact that it was going to have. Yeah, i'm not sure if the authors were aware of the impact that the paper would go on to have. Probably they weren't, but I think they were aware of some of the motivations and design decisions beyond the transformer, and they chose not to, I think, expand on it in that way in a paper.

25
00:03:27.950 --> 00:03:31.200
Inderjit Dhillon: and so I think they had an idea that there was more

26
00:03:31.410 --> 00:03:49.700
Inderjit Dhillon: than just the surface of just like what we're just doing translation. And here's a better architecture. You're not just doing translation. This is like a really cool, differentiable, optimizable, efficient computer that you proposed, and maybe they didn't have all of that for site. But I think it's really interesting. Isn't it funny? Sorry to interrupt the that title is meable that they want

27
00:03:49.700 --> 00:04:00.280
Inderjit Dhillon: for such a profound idea they want with the I don't think anyone use that kind of title before right. That's all you need. Yeah, it's like a meme or something. It's not funny that one

28
00:04:00.330 --> 00:04:02.330
Inderjit Dhillon: like. Maybe

29
00:04:02.420 --> 00:04:10.670
Inderjit Dhillon: if it was a more serious, we don't have the impact. Honestly, I. Yeah, there is an element of me that honestly agrees with you and prefers it this way. Yes.

30
00:04:10.930 --> 00:04:30.120
Inderjit Dhillon: if it was too grand, it would over promise. And then on the developer potentially. So you want to just mean your way to greatness. That should be a t-shirt so you you tweeted the transformers and magnificent neural network architecture, because there is a general purpose, differentiable computer. It is simultaneously expressive

31
00:04:30.120 --> 00:04:36.140
Inderjit Dhillon: in the forward pass optimizable via proper back propagation, gradient descent, and efficient.

32
00:04:36.360 --> 00:04:42.990
Inderjit Dhillon: I parallelism, compute graph. Can you discuss some of the those details expressed and optimizable, efficient?

33
00:04:43.060 --> 00:04:46.900
Inderjit Dhillon: Yeah, or or in general, whatever it comes to your heart?

34
00:04:47.110 --> 00:04:55.500
Inderjit Dhillon: You want to have a general-purpose computer that you can train on arbitrary problems like, say, the task of next work prediction or detecting. If there's a cat in the image or something like that.

35
00:04:55.830 --> 00:05:10.260
Inderjit Dhillon: And you want to train this computer. So you want to send it its weights. And I think there's a number of design criteria, that sort of overlap in the transformer simultaneously, that made it very successful. And I think the authors were kind of deliberately trying to make this really

36
00:05:10.370 --> 00:05:18.920
Inderjit Dhillon: powerful architecture. And so in a basically it's very powerful in the forward past, because it's able to express

37
00:05:19.170 --> 00:05:34.330
Inderjit Dhillon: very a general co computation as sort of something that looks like message passing. You have nodes and in store vectors, and these nodes get to basically look at each other, and it's each other's. Vectors, and they get to communicate and basically knows get to

38
00:05:34.330 --> 00:05:50.980
Inderjit Dhillon: broadcast. Hey? I'm looking for certain things. And then other nodes get to broadcast. Hey? These are the things I have. Those are the keys and the values. So it's not just attention. Yeah, exactly. Transformers much more than just the attention component. It's got many pieces architectural that went into it the residual connection of the way it's arranged there's a multi-layer perceptron, and there the way it's

39
00:05:50.980 --> 00:05:58.770
Inderjit Dhillon: a stacked and so on. But basically there's a message passing scheme where nodes get to look at each other, decide what's interesting and then update each other.

40
00:05:58.830 --> 00:06:15.830
Inderjit Dhillon: And so I think the when you get to the details of it. I think it's a very expressive function, so it can express lots of different types of algorithms and forward pass. Not only that, but the way it's designed with the residual connections, layer normalizations, the softmatics attention and everything. It's also optimizable. This is really big deal. Because

41
00:06:15.860 --> 00:06:27.370
Inderjit Dhillon: there's lots of computers that are powerful that you can't optimize or they're not easy to optimize using the techniques that we have, which is back propagation ingredients. And these are first order methods. Very simple optimizers, really, and so

42
00:06:27.460 --> 00:06:30.070
Inderjit Dhillon: you also need it to be optimizable.

43
00:06:30.080 --> 00:06:54.560
Inderjit Dhillon: And then, lastly, you wanted to run efficiently in our hardware. Our hardware is a massive throughput machine. By Gpus, they prefer lots of parallelism. So you don't want to do lots of sequential operations. You want to do a lot of operations serially, and then Transformer is designed with that in mind as well, and so it's designed for our hardware, and is designed to both be very expressive in a forward pass, but also very optimizable in the backward pass. And you said that

44
00:06:54.560 --> 00:07:01.270
Inderjit Dhillon: the residual connection support a kind of ability to learn short algorithms fast and first, and then gradually extend them

45
00:07:01.430 --> 00:07:06.470
Inderjit Dhillon: longer during training. Yeah, what's what's the idea of learning short algorithms right?

46
00:07:06.490 --> 00:07:21.990
Inderjit Dhillon: Think of it as a so, basically a transformer is a series of blocks, right? And these blocks have attention and a little bit of a perceptual. And so you. You go off into a block, and you come back to this visual pathway, and then you go off and you come back, and then you have a number of layers arranged sequentially.

47
00:07:22.120 --> 00:07:34.440
Inderjit Dhillon: And so the way to look at it, I think, is because of the residual pathway in the backward past. The gradients sort of flow along it uninterrupted, because addition distributes the gradient equally to all of its branches.

48
00:07:34.450 --> 00:07:39.870
Inderjit Dhillon: So the gradient from the supervision at the top just floats directly to the first layer.

49
00:07:40.020 --> 00:07:53.840
Inderjit Dhillon: and the all the your visual connections are arranged, so that in the beginning, at doing initialization they contribute nothing to the residual pathway. So what this kind of looks like is, imagine the transformers kind of like a python function like a deaf.

50
00:07:54.040 --> 00:08:05.520
Inderjit Dhillon: and you get to do various kinds of like lines of code. So you have a 100 layers deep, transformer. Typically, they would be much shorter, say 2020 lines of code, and you can do something in them.

51
00:08:05.610 --> 00:08:23.980
Inderjit Dhillon: And so think of during the optimization. Basically, what it looks like is first, you optimize the first line of code, and then the second line of code can kick in, and the third line of code in can. And I kind of feel like because of the residual path when the dynamics of the optimization. You can sort of learn a very short algorithm that gets the proxy tensor. But then the other layers can serve, kick in and start to create a contribution.

52
00:08:23.980 --> 00:08:28.180
Inderjit Dhillon: and at the end of it you are. You're optimizing over an algorithm. That is a 20 lines of code.

53
00:08:28.640 --> 00:08:43.640
Inderjit Dhillon: except these lines of point are very complex, because it's an entire block of a transformer. You can do a lot in there. Well, it's really interesting is that this transformer architecture actually has been a remarkably resilient basically a transformer that came out in 2,016 is the transformer you would use today except you reshuffle some delay in terms.

54
00:08:43.809 --> 00:09:04.130
Inderjit Dhillon: The earlier normalizations have been reshuffled to a pre norm formulation, and so it's been remarkably stable. But there's a lot of bells and whistles that people have attached to it and try to improve it. I do think that basically it's a it's a big step in simultaneously up optimizing for lots of properties of a desirable neural network architecture, and I think that people have been trying to change it. But it's proven remarkably resilient.

55
00:09:04.150 --> 00:09:18.350
Inderjit Dhillon: But I do think that there should be even better pictures potentially. But it's you are. You admire the resilience Here, there's something profound about this architecture that that at least so maybe we can. Everything could be turned into a

56
00:09:18.820 --> 00:09:30.850
Inderjit Dhillon: into a problem that transformers can solve. Currently, it definitely looks like the transformers taking over AI, and you can feed basically arbitrary problems into it, and it's a a general refreshable computer, and it's extremely powerful. And

57
00:09:30.980 --> 00:09:43.210
Inderjit Dhillon: and this conversions in AI has been really interesting to watch for me personally. What else do you think could be discovered here about Transform? Is that good, surprising thing, or or is it a stable?

58
00:09:43.840 --> 00:09:54.190
Inderjit Dhillon: I want a stable place. Is there something interesting we might discover about transformers like a Ha! Moments, maybe has to do with memory. maybe knowledge, representation. That that stuff

59
00:09:54.400 --> 00:10:07.370
Inderjit Dhillon: definitely does that guys today is just pushing like basically right now, the Zeitgeist is, do not touch the transformer. We touch everything else. Yes, so people are scaling up the data sets, making them much much bigger. They're working on the evaluation, making the evaluation much, much bigger.

60
00:10:07.470 --> 00:10:08.700
Inderjit Dhillon: And

61
00:10:08.860 --> 00:10:16.540
Inderjit Dhillon: they're basically keeping the architecture unchanged. And that's how we that's the last 5 years of progress in AI kind of

62
00:10:24.600 --> 00:10:29.620
Inderjit Dhillon: Okay. So I hope you enjoyed that video. So kind of just.

63
00:10:29.810 --> 00:10:46.020
Inderjit Dhillon: It gives you a sense of how kind of important this paper attention is. All you can need. Is it all in your transformers are taking natural language processing well by store. It's incredible. Models are breaking multiple Lp records and pushing the state of the art.

64
00:10:46.440 --> 00:10:48.630
Inderjit Dhillon: They are used in many applications.

65
00:10:52.460 --> 00:10:57.690
Inderjit Dhillon: Okay, Sorry. Probably a good video, but i'll move on to lecture myself.

66
00:10:58.940 --> 00:11:09.880
Inderjit Dhillon: Okay, so and so today, let's continue on that. We introduced the transformer last time, and today I will actually spend again some time on the architecture.

67
00:11:09.960 --> 00:11:26.550
Inderjit Dhillon: And as Karpathi mentioned, you know it's actually used not just for natural language, but also there's vision transformers. There'll be a talk in class next week on vision transformers. It can handle video and so on. So just very powerful and kind of taking over

68
00:11:26.550 --> 00:11:39.810
Inderjit Dhillon: AI and deep learning, and and the Karpathy actually also has the person who was interviewed in that video also has a great series of lectures on Youtube that I encourage everyone

69
00:11:39.840 --> 00:11:52.310
Inderjit Dhillon: to to look at, and even, you know he goes into the guts of I think it's called, Make more series. and it actually produces. He will

70
00:11:52.430 --> 00:12:04.810
Inderjit Dhillon: develop simple language models all the way. Going up to. I think he calls it the Nano Gp model. So encourage everybody kind of to look at it. And if you're really curious, you know the

71
00:12:04.960 --> 00:12:06.030
Inderjit Dhillon: oh

72
00:12:06.120 --> 00:12:15.370
Inderjit Dhillon: program along with him. So now let me share my screen, which has the slides.

73
00:12:23.240 --> 00:12:24.220
Inderjit Dhillon: Okay?

74
00:12:24.630 --> 00:12:37.950
Inderjit Dhillon: Well, hopefully, you can all see the slides. So let's talk a little bit more about transformers. And really another very important concept, in addition to attention and transformers the concept of using

75
00:12:38.000 --> 00:12:55.330
Inderjit Dhillon: self-supervised learning instead of supervised learning and unsupervised. Learning Okay, so this slide is kind of a recap from one of the slides last time where you know attention was first put in Lsdms or Rms for neural machine translation.

76
00:12:55.560 --> 00:13:08.020
Inderjit Dhillon: and then we'll do a recap of the transformer architecture. You know it's a very, very important neural network architecture, so it's kind of good to go again over it.

77
00:13:08.080 --> 00:13:14.930
Inderjit Dhillon: and this paper, I think, publicly appeared in 2,017, but probably there was an archive version in 2,016,

78
00:13:16.010 --> 00:13:23.640
Inderjit Dhillon: and it really proposed transformer for machine translation. And then there was a paper on bird which

79
00:13:23.740 --> 00:13:28.500
Inderjit Dhillon: really kind of made a huge impact on natural language processing

80
00:13:28.550 --> 00:13:40.660
Inderjit Dhillon: where they use the transformer and pre-training and ended up achieving sorta which stands for state of the art on many Nlp tasks. And then, like I said.

81
00:13:40.760 --> 00:13:54.770
Inderjit Dhillon: there's a vision transform also. So transformers can actually also handle images. So just kind of recapping from last time the transformer, let's say, for machine translation.

82
00:13:54.910 --> 00:14:01.870
Inderjit Dhillon: You pass all the input tokens at the same time to the transformer. So this is a

83
00:14:01.880 --> 00:14:06.440
Inderjit Dhillon: natural language pro machine translation tasks, you know, French

84
00:14:06.750 --> 00:14:08.610
Inderjit Dhillon: to English.

85
00:14:08.900 --> 00:14:15.440
Inderjit Dhillon: and you give each of these inputs all input tokens.

86
00:14:15.660 --> 00:14:17.790
Inderjit Dhillon: all given simultaneously.

87
00:14:18.250 --> 00:14:22.910
Inderjit Dhillon: And you know, last time we saw, and we'll recap again that the transformer

88
00:14:23.070 --> 00:14:29.910
Inderjit Dhillon: basically has multiple layers of encoder where and

89
00:14:30.090 --> 00:14:42.170
Inderjit Dhillon: of a vector representation of the input token gets transformed. So there's an input representation. And then there's an output which keeps on kind of getting modified by the multiple layers of the transformer.

90
00:14:42.210 --> 00:14:43.640
Inderjit Dhillon: And then there is

91
00:14:43.910 --> 00:14:47.910
the decoder layer for machine translation that

92
00:14:48.020 --> 00:14:50.480
Inderjit Dhillon: end up doing the translations.

93
00:14:51.220 --> 00:14:57.610
Inderjit Dhillon: And once we so if you open up the encoder layer.

94
00:14:57.890 --> 00:15:10.040
Inderjit Dhillon: it has what's called self attention and the feed-forward network, whereas the Dec order layer has self attention, but also has attention between the output and the input

95
00:15:10.280 --> 00:15:13.210
Inderjit Dhillon: And then there is a a feed forward now.

96
00:15:14.990 --> 00:15:20.870
Inderjit Dhillon: So we looked at the in order mainly last time, and that's what we'll do today.

97
00:15:21.030 --> 00:15:28.440
Inderjit Dhillon: So each word has a corresponding latent vector right? So, initially, this would be the embedding for each word.

98
00:15:28.600 --> 00:15:32.910
Inderjit Dhillon: So again, carrying the same example as that we had last time.

99
00:15:32.980 --> 00:15:40.340
Inderjit Dhillon: So thinking machines. Just think of the dimension over here is 4.

100
00:15:40.710 --> 00:15:52.490
Inderjit Dhillon: Right in practice. It will be higher, like 64, or maybe even larger. right? And what the self attention there does, is it? It will. Oh.

101
00:15:53.510 --> 00:16:01.160
Inderjit Dhillon: transform this into another. Vector, and finally you will get a 4 dimensional vector out over here as the

102
00:16:02.640 --> 00:16:08.330
Inderjit Dhillon: so each layer of the encoder will receive a list of the vectors as input

103
00:16:08.830 --> 00:16:11.210
Inderjit Dhillon: pass them through a self attention

104
00:16:11.530 --> 00:16:14.900
Inderjit Dhillon: and then pass them through a feed forward.

105
00:16:16.170 --> 00:16:20.060
Inderjit Dhillon: Okay, and like, I said, the self attention is

106
00:16:21.720 --> 00:16:23.970
Inderjit Dhillon: talking about.

107
00:16:24.220 --> 00:16:27.900
Inderjit Dhillon: You know the the Self Attention Layer

108
00:16:28.270 --> 00:16:38.920
Inderjit Dhillon: Self attention itself is that, you know. Suppose you have a sentence such as this. The animal didn't cross the street because it was too tired, you know. Where should

109
00:16:38.990 --> 00:16:44.120
Inderjit Dhillon: it put its attention on to the animal to the street.

110
00:16:44.410 --> 00:16:50.340
Inderjit Dhillon: to something else. And through the Pre. The the task that we train on

111
00:16:50.780 --> 00:16:58.350
Inderjit Dhillon: these connections are learned. Okay. The self attention is learned, and how is their attention actually done?

112
00:16:58.800 --> 00:17:02.730
Inderjit Dhillon: Well. you have the input vectors.

113
00:17:03.230 --> 00:17:13.839
Inderjit Dhillon: You have the self attention parameters. These are matrices. W. Q. Q. For query, K. For Key V for value.

114
00:17:14.050 --> 00:17:19.770
Inderjit Dhillon: So you given X I. The it token embedding

115
00:17:20.460 --> 00:17:21.579
Inderjit Dhillon: or vector

116
00:17:21.950 --> 00:17:27.839
Inderjit Dhillon: you have 3 linear transformations. Okay, 3 transformations. That map

117
00:17:28.079 --> 00:17:39.440
Inderjit Dhillon: this 4 dimensional vector. Over here, too. 3 3 dimensional vectors. Okay. And so this is the query. Xi. Times, W. Q. Matrix.

118
00:17:39.970 --> 00:17:42.810
Inderjit Dhillon: The keys and the values.

119
00:17:42.960 --> 00:17:51.820
Inderjit Dhillon: And what we will see is that the queries and the keys are used, then to learn the attention between the to.

120
00:17:52.400 --> 00:17:54.230
Inderjit Dhillon: So for each word I

121
00:17:56.020 --> 00:18:01.380
Inderjit Dhillon: compute the scores which determine how much focus to put on other input words

122
00:18:01.660 --> 00:18:10.550
Inderjit Dhillon: and the in attention score for word J toward I is Qi transpose Kj: where you remember.

123
00:18:10.950 --> 00:18:14.450
Inderjit Dhillon: Yeah, I transpose Qi: transpose and

124
00:18:14.610 --> 00:18:20.490
Inderjit Dhillon: K. J: that would be the Gs Token. Okay. So over here.

125
00:18:22.480 --> 00:18:30.330
Inderjit Dhillon: Q. One transpose. K: One: that's the attention to itself. you know, Suppose it comes out to 112,

126
00:18:30.590 --> 00:18:36.500
Inderjit Dhillon: And suppose the attention of thinking of all thinking to machines

127
00:18:36.720 --> 00:18:46.110
Inderjit Dhillon: comes out to be 96. Okay, I don't know if you may, some of you know. But thinking machines was actually a company in the

128
00:18:46.170 --> 00:18:47.050
Inderjit Dhillon: lead.

129
00:18:48.110 --> 00:18:57.450
Inderjit Dhillon: 19 eighties early, 19 nineties, and they, you know, used to do connection machines and kind of high performance computing.

130
00:18:58.530 --> 00:19:05.720
Inderjit Dhillon: So once you do the self attention for each word, I

131
00:19:06.130 --> 00:19:15.120
Inderjit Dhillon: then you have these: the attention. My for K. One through K. N. Where n is the number of tokens.

132
00:19:15.130 --> 00:19:22.630
Inderjit Dhillon: and you pass it through a soft max there. which means that this Si vector is now essentially

133
00:19:22.710 --> 00:19:27.130
Inderjit Dhillon: a non negative vector

134
00:19:27.200 --> 00:19:29.100
Inderjit Dhillon: that sums up to one.

135
00:19:29.400 --> 00:19:39.840
Inderjit Dhillon: So you end up, end up taking a convex combination of the value vectors over here. Okay. So remember that there is this value. This is also going to be learned

136
00:19:40.190 --> 00:19:48.180
Inderjit Dhillon: Right? So this is the softmax, you know. I think we've you know there's a division by a particular number.

137
00:19:48.290 --> 00:19:53.950
Inderjit Dhillon: and then the soft max value gets divided. Times the value, and this

138
00:19:54.070 --> 00:19:58.660
Inderjit Dhillon: is what this is. Okay, Z. I equals this

139
00:19:59.130 --> 00:20:10.170
Inderjit Dhillon: note that the I is still because Vi is 3 dimensional. Zia is also going to be 3 dimensional. So you can all write this into in a matrix form.

140
00:20:10.190 --> 00:20:13.160
Inderjit Dhillon: right? If I now stack the tokens.

141
00:20:13.590 --> 00:20:19.180
Inderjit Dhillon: So this is x one. This is x 2, that's the 2 tokens.

142
00:20:19.320 --> 00:20:21.960
Inderjit Dhillon: and then you have since the

143
00:20:22.120 --> 00:20:32.650
Inderjit Dhillon: import embedding is 4 dimensional and the output. The queries are 3 dimensional. These matrices, Wq. Are, you know.

144
00:20:32.920 --> 00:20:35.960
Inderjit Dhillon: over here? 4 by 3?

145
00:20:36.370 --> 00:20:37.090
Inderjit Dhillon: Okay.

146
00:20:39.070 --> 00:20:40.280
Inderjit Dhillon: And so you have

147
00:20:41.000 --> 00:20:46.180
Inderjit Dhillon: X. Times, W. Q. X. Times, W. K. X. Times, W. V.

148
00:20:46.220 --> 00:20:50.060
Inderjit Dhillon: And if you think back to the video that you heard about

149
00:20:50.310 --> 00:21:02.480
Inderjit Dhillon: these are actually pretty efficient matrix multiplications. Right? So matrix multiplication can actually be implemented very well on the modern, especially on the modern gpus and the tpus.

150
00:21:02.680 --> 00:21:09.420
Inderjit Dhillon: And another thing to consider is that all these competitions are actually can be done independently.

151
00:21:10.920 --> 00:21:16.490
Inderjit Dhillon: This can be done independently of this can be done independently of this. So there is a fair amount of

152
00:21:16.530 --> 00:21:19.030
Inderjit Dhillon: parallelism also over here.

153
00:21:19.750 --> 00:21:35.010
Inderjit Dhillon: and then Z, as we saw, is like a 2 number of tokens by 3 dimensional output, and that's computed by this operation. Q. K. Transpose with the normalization Times, a. V.

154
00:21:36.820 --> 00:21:39.080
Inderjit Dhillon: Now. the

155
00:21:39.260 --> 00:21:49.640
Inderjit Dhillon: So far, we have only talked about what's called 1 one head of the transformer. Typically, there are multiple heads, for example, 8 heads.

156
00:21:49.720 --> 00:21:52.480
Inderjit Dhillon: So that means that you end up having

157
00:21:52.500 --> 00:22:00.740
Inderjit Dhillon: 8 of these going on. And, like, I said before, they can actually all be done independently of each other. Right?

158
00:22:00.800 --> 00:22:06.260
Inderjit Dhillon: So our attention, Head 0 will produce. Q. Not

159
00:22:07.200 --> 00:22:09.480
Inderjit Dhillon: through W. Not queue

160
00:22:09.550 --> 00:22:23.610
Inderjit Dhillon: attention head. One will produce Q. One with W, one queue, and in each of these we'll compute the self attention multiplied by the values to basically get an output from each of the heads.

161
00:22:24.140 --> 00:22:30.880
Inderjit Dhillon: So attention, Head number 0 would be in this case 2 tokens. There are times 3 dimensions

162
00:22:31.150 --> 00:22:40.080
Inderjit Dhillon: similar to your attention. Head. one would be another, 2 by 3 dimensional, vector and so on. So if you have 8 heads.

163
00:22:40.350 --> 00:22:55.000
Inderjit Dhillon: you will actually have dimensionality over here to be 8 times 3, which is 24. But remember that we said that it needs to the transform or transforms each 4 dimensional vector in this case right

164
00:22:55.040 --> 00:22:57.080
Inderjit Dhillon: to another 4 dimensional effect.

165
00:22:57.110 --> 00:23:05.450
Inderjit Dhillon: Right? So what we do is then we end up kind of concatenating them together.

166
00:23:05.740 --> 00:23:08.590
Inderjit Dhillon: Z: not d, one through C 7,

167
00:23:08.650 --> 00:23:15.800
Inderjit Dhillon: right? So it's 2 by 24, and then we end up just using a weight matrix

168
00:23:15.820 --> 00:23:20.950
Inderjit Dhillon: to map it back into 4 dimensions. So this will be 24

169
00:23:21.060 --> 00:23:23.050
Inderjit Dhillon: by 4 matrix.

170
00:23:23.130 --> 00:23:30.260
Inderjit Dhillon: Okay. And that is like a fully connected layer that ends up

171
00:23:30.470 --> 00:23:34.430
Inderjit Dhillon: being No. This, this

172
00:23:34.460 --> 00:23:40.880
Inderjit Dhillon: this matrix, and then this can be sent forward to the feed-forward network right, if you

173
00:23:40.890 --> 00:23:42.210
Inderjit Dhillon: remember.

174
00:23:43.580 --> 00:23:47.960
Inderjit Dhillon: let me just go back to the architecture over here right? So this is

175
00:23:48.120 --> 00:23:53.230
Inderjit Dhillon: thinking you know this. Is now the output. After

176
00:23:53.350 --> 00:24:11.470
Inderjit Dhillon: doing this, having this fully connected layer that converts the 24 dimensional vector to a 4 dimensional, and then this is sent down to the feed forward network. The one thing that we Haven't shown is there are residual connections over here which you heard about in the in the video, but

177
00:24:13.330 --> 00:24:16.080
Inderjit Dhillon: they are not included in the description over here.

178
00:24:21.240 --> 00:24:25.970
Inderjit Dhillon: Okay. So this is kind of the overall architecture.

179
00:24:26.190 --> 00:24:33.100
Inderjit Dhillon: of course, in the input, layer this is the word embedding. But then on the transformer layers which are

180
00:24:33.130 --> 00:24:39.810
Inderjit Dhillon: above the input layer, these just end up being the outputs of the previous transformer. Right?

181
00:24:39.950 --> 00:24:45.400
Inderjit Dhillon: So you per layer you have in this example, and you have 8 heads.

182
00:24:45.970 --> 00:24:53.060
Inderjit Dhillon: Okay, and they end up giving these queue query key values that then are multiplied by

183
00:24:53.240 --> 00:24:55.400
Inderjit Dhillon: the values

184
00:24:55.690 --> 00:25:07.610
Inderjit Dhillon: Get Z: not. Take a a fully connected layer. and out we get 2 by 4 dimensional, which is exactly the same as the input right.

185
00:25:09.910 --> 00:25:23.130
Inderjit Dhillon: One thing we did not talk about last time we started to mention it at the end. The above architecture is kind of ignoring the sequential information, right? They're all being done in parallel. But so far we haven't actually have any information

186
00:25:23.440 --> 00:25:31.150
Inderjit Dhillon: that you know. This is what position this token is in the sentence.

187
00:25:31.390 --> 00:25:33.500
Inderjit Dhillon: So what you can do is you can

188
00:25:34.740 --> 00:25:42.030
Inderjit Dhillon: add what's called the positional encoding. Vector okay, this can either be a fixed vector or it can be learned.

189
00:25:43.070 --> 00:25:47.100
Inderjit Dhillon: And so you have the embedding of the

190
00:25:48.570 --> 00:25:54.130
Inderjit Dhillon: input word, and then you have positional encoding. And that is what gives you in the first layer.

191
00:25:55.080 --> 00:25:58.860
Inderjit Dhillon: Okay? And so this is the way to kind of capture

192
00:25:58.880 --> 00:26:03.700
Inderjit Dhillon: the position of each Xi or each token, or it would.

193
00:26:04.150 --> 00:26:06.470
Inderjit Dhillon: So when the transformer paper came out.

194
00:26:07.730 --> 00:26:19.020
Inderjit Dhillon: it led to kind of state of the art. This is, of course, in 2,017, which is a very long time in the way things are evolving these days.

195
00:26:19.270 --> 00:26:29.250
Inderjit Dhillon: so they have a transformer base model, which was smaller than the transformer big model. And these are what's called there. There's some metric called the blue metric.

196
00:26:29.360 --> 00:26:36.410
Inderjit Dhillon: and that measures the state of the art. And you can see that for English to

197
00:26:36.960 --> 00:26:38.120
Inderjit Dhillon: German

198
00:26:38.250 --> 00:26:43.660
Inderjit Dhillon: you get the highest blue score using this transformer big architecture.

199
00:26:44.370 --> 00:26:50.640
Inderjit Dhillon: Okay? And then this is the training cost in terms of flops.

200
00:26:50.710 --> 00:26:54.830
Inderjit Dhillon: So you can see a lot of floating point operations done over here

201
00:26:55.110 --> 00:27:05.010
Inderjit Dhillon: 3.3 times 10 to the power 18, and this is slightly more, almost a factor of 10 times more in getting a bigger architecture. And of course.

202
00:27:05.450 --> 00:27:10.170
Inderjit Dhillon: you know, like we we've talked about these

203
00:27:11.740 --> 00:27:18.340
Inderjit Dhillon: large models. A lot of transformers, more and more large transformers are being used now.

204
00:27:19.240 --> 00:27:28.220
Inderjit Dhillon: So now let's talk a little bit about. Oh. how to actually use these transformers.

205
00:27:29.060 --> 00:27:39.880
Inderjit Dhillon: Actually, we don't have to necessarily restrict ourselves to transformers in the beginning. All right. We can actually talk about the transformers. Sorry, Nlp. In general.

206
00:27:40.160 --> 00:27:45.800
Inderjit Dhillon: and think about how to do unsupervised pre-training in natural language processing.

207
00:27:45.980 --> 00:27:51.840
Inderjit Dhillon: So let's review that, and then we'll come to how it is done in transformers. And that's how you get

208
00:27:52.000 --> 00:27:58.910
Inderjit Dhillon: these latest wonders like Gpd 4 and chat chat.

209
00:28:00.670 --> 00:28:18.040
Inderjit Dhillon: Okay, so unsupervised or self-supervised pre-training for Nlp. Is, you know there's a huge amount of just text data sitting out there like Wikipedia pages, web pages, chats. Oh, and so on.

210
00:28:18.540 --> 00:28:22.360
Inderjit Dhillon: It's all unlabeled, or most of it is unlabeled right?

211
00:28:23.460 --> 00:28:39.130
Inderjit Dhillon: And then, of course, to try and get labeled data is actually much more expensive. Right? So if you, for example, want to judge whether you know something is an email is spam or not. You have to label some of these image emails

212
00:28:39.130 --> 00:28:45.410
Inderjit Dhillon: and then create a training data set through which you can. Then you'd learn your machine learning model.

213
00:28:45.710 --> 00:28:51.390
Inderjit Dhillon: But how can we use this huge amount of unlabeled data right to

214
00:28:51.510 --> 00:28:55.080
Inderjit Dhillon: to obtain

215
00:28:55.430 --> 00:29:08.580
Inderjit Dhillon: meaningful representations in the Nlp context. So natural language, processing, context of words and sentences. Okay. So one idea is that i'm actually going to use this very large corpus

216
00:29:08.930 --> 00:29:15.940
Inderjit Dhillon: to learn useful world representations. Okay. So we are kind of starting from the beginning.

217
00:29:16.290 --> 00:29:21.700
Inderjit Dhillon: What we'll do is we'll learn a vector for each word based on the corpus.

218
00:29:22.380 --> 00:29:33.240
Inderjit Dhillon: And what the hope is that these vectors end up representing some semantic meaning. And then in the downstream application, they can then be used for many tasks.

219
00:29:33.590 --> 00:29:43.790
Inderjit Dhillon: Okay, for example. they could. We used as the word embedding matrices for the deep neural network learning models which are used in classification and translation.

220
00:29:43.900 --> 00:29:51.100
Inderjit Dhillon: Of course, that's not how things are right now. But this was the initial motivation for kind of developing these world embeddings.

221
00:29:51.210 --> 00:29:58.120
Inderjit Dhillon: Okay, so we talked a little bit about 2 different perspectives. You know this is called the Glove Method.

222
00:29:58.350 --> 00:30:00.190
Inderjit Dhillon: and the word to work

223
00:30:00.200 --> 00:30:04.350
Inderjit Dhillon: pretty similar timeframe. But almost 10 years ago, right

224
00:30:09.030 --> 00:30:17.570
Inderjit Dhillon: now. Given a large text, corpus, the fundamental problem is, how do you learn low, dimensional features to represent the world?

225
00:30:19.420 --> 00:30:20.270
Inderjit Dhillon: Okay.

226
00:30:20.540 --> 00:30:24.910
Inderjit Dhillon: So what you have is for each word wi

227
00:30:25.480 --> 00:30:28.780
Inderjit Dhillon: You can define the context of the world

228
00:30:29.110 --> 00:30:32.940
Inderjit Dhillon: as the words that surround it in an l-sized window.

229
00:30:33.360 --> 00:30:37.040
Inderjit Dhillon: Okay, so suppose this is the word under consideration.

230
00:30:37.600 --> 00:30:42.790
Inderjit Dhillon: I can so think of this as a English or

231
00:30:42.860 --> 00:30:49.060
Inderjit Dhillon: a sentence in any particular language, which is a ordered sequence of words.

232
00:30:49.840 --> 00:30:52.910
Inderjit Dhillon: So what you can say is that you can look at wi.

233
00:30:53.080 --> 00:30:57.800
Inderjit Dhillon: and you can look at the context of wi as the l words

234
00:30:58.150 --> 00:31:00.160
Inderjit Dhillon: that occur before wi.

235
00:31:00.970 --> 00:31:04.460
Inderjit Dhillon: and the l words that occur after wi

236
00:31:04.480 --> 00:31:12.180
Inderjit Dhillon: right. So, for example, if you think about machine learning as becoming a more and more common phrase.

237
00:31:12.320 --> 00:31:17.490
Inderjit Dhillon: then in many cases this could be machine, and this could be in learning. And so

238
00:31:17.580 --> 00:31:23.080
Inderjit Dhillon: there'll be many pieces of text where the context of wi

239
00:31:23.090 --> 00:31:26.460
Inderjit Dhillon: will include the word learning.

240
00:31:26.780 --> 00:31:31.490
Inderjit Dhillon: So what you can do is you can get a collection of these word context pairs.

241
00:31:31.540 --> 00:31:38.330
Inderjit Dhillon: and we can denote this set by D. So, as an example, suppose you have a sentence.

242
00:31:38.560 --> 00:31:43.730
Inderjit Dhillon: The quick brown fox jumps over the lazy dog.

243
00:31:45.090 --> 00:31:49.350
Inderjit Dhillon: I can now look at the word, though. Then quick.

244
00:31:49.720 --> 00:31:53.480
Inderjit Dhillon: Then the third word Brown, then fox, and so on.

245
00:31:53.920 --> 00:31:57.040
Inderjit Dhillon: And suppose I have a window of size 2

246
00:31:58.090 --> 00:31:59.760
Inderjit Dhillon: 2 following.

247
00:32:00.090 --> 00:32:09.860
Inderjit Dhillon: and if there are 2 preceding the word. and then you can make a list of pairs right? So for the word, though

248
00:32:10.170 --> 00:32:11.270
Inderjit Dhillon: I have quick

249
00:32:11.510 --> 00:32:14.320
Inderjit Dhillon: and brown, these are the 2 contexts

250
00:32:14.720 --> 00:32:17.260
Inderjit Dhillon: for the word quick. I have

251
00:32:17.560 --> 00:32:20.580
Inderjit Dhillon: quick! The quick, brown, quick.

252
00:32:20.660 --> 00:32:23.650
Inderjit Dhillon: but quick, fox! Quick! The

253
00:32:23.760 --> 00:32:29.040
Inderjit Dhillon: quick, brown, quick, fox! So you can see that from here.

254
00:32:29.600 --> 00:32:32.100
Inderjit Dhillon: You now, quick, the

255
00:32:33.750 --> 00:32:35.260
Inderjit Dhillon: quick, brown.

256
00:32:36.570 --> 00:32:37.700
Inderjit Dhillon: and

257
00:32:40.080 --> 00:32:41.120
Inderjit Dhillon: you have

258
00:32:41.140 --> 00:32:42.320
Inderjit Dhillon: quick fox.

259
00:32:43.960 --> 00:32:48.350
Inderjit Dhillon: and you do it for all each of the words.

260
00:32:48.870 --> 00:32:56.280
Inderjit Dhillon: And then what you do is you use the bag of world models. Okay, so what is?

261
00:32:56.440 --> 00:33:01.810
Inderjit Dhillon: Well, back of word is again a way of describing each word.

262
00:33:03.770 --> 00:33:08.160
Inderjit Dhillon: Okay. suppose I have context words, c. One through CD:

263
00:33:09.190 --> 00:33:14.630
Inderjit Dhillon: Right? I can actually put them. I can put the words as rows in a particular matrix.

264
00:33:15.490 --> 00:33:25.670
Inderjit Dhillon: The context words are the columns, and each row now will represent a d dimensional feature vector

265
00:33:25.990 --> 00:33:27.800
Inderjit Dhillon: for the particular work.

266
00:33:29.360 --> 00:33:40.810
Inderjit Dhillon: Okay. Now, of course, this will be extremely sparse. but it is one when the D. Will be very large, because the number of context words could be very large. But let's just see how we would do it.

267
00:33:41.070 --> 00:33:42.220
Inderjit Dhillon: So

268
00:33:42.630 --> 00:33:53.810
Inderjit Dhillon: if I now have a count of the number of Times words W. Occurs with context, I. So we are at a particular word. And now i'm counting the number of times

269
00:33:53.960 --> 00:34:02.680
Inderjit Dhillon: the world appears with the Ayat context context world. And let's suppose this is the number. This is how we denote it.

270
00:34:03.260 --> 00:34:10.650
Inderjit Dhillon: And then for each word I can actually form a d dimensional, and you know it is going to be extremely sparse, like I said.

271
00:34:10.920 --> 00:34:14.699
Inderjit Dhillon: to describe W.

272
00:34:15.090 --> 00:34:20.330
Inderjit Dhillon: And so, in the first iteration we can think about just having this

273
00:34:20.389 --> 00:34:27.639
Inderjit Dhillon: these raw numbers over here as the representation of a word in terms of these context words.

274
00:34:27.989 --> 00:34:36.989
Inderjit Dhillon: But we can actually do a little bit better. It's similar to kind of the tfid of representation that I we talked about last time.

275
00:34:37.300 --> 00:34:42.150
Inderjit Dhillon: and the whole idea is that you want to try and consider the frequency of each word

276
00:34:42.360 --> 00:34:49.179
Inderjit Dhillon: and of each context. So the whole idea is that if a word is, you know, occurs a lot.

277
00:34:49.610 --> 00:34:56.469
Inderjit Dhillon: then you need to somehow normalize it. If a context occurs a lot, you somehow need to normalize it. But right.

278
00:34:56.510 --> 00:35:00.660
Inderjit Dhillon: So instead of using the co co-occurrence sound.

279
00:35:00.820 --> 00:35:05.590
Inderjit Dhillon: we use something called point-wise mutual information, which is essentially which is just

280
00:35:05.650 --> 00:35:07.540
Inderjit Dhillon: log of

281
00:35:08.340 --> 00:35:16.560
Inderjit Dhillon: this joint distribution with the marginal distribution. So P hat Wc

282
00:35:16.740 --> 00:35:21.180
Inderjit Dhillon: it's the empirical probability that W. And C. Occur together.

283
00:35:21.340 --> 00:35:27.200
Inderjit Dhillon: and it will be this quantity divided by the total number of pairs which is

284
00:35:27.450 --> 00:35:35.660
Inderjit Dhillon: absolute value of so, and then the P. Hat of W. Is the empirical probability of

285
00:35:35.930 --> 00:35:42.220
Inderjit Dhillon: the word W. So it's essentially the number of time word occurs.

286
00:35:42.260 --> 00:35:45.330
Inderjit Dhillon: So you was essentially counted over all the contexts.

287
00:35:45.640 --> 00:35:51.190
Inderjit Dhillon: It's the number of times the word W. Occurred in your document, Corpus.

288
00:35:51.720 --> 00:35:56.740
Inderjit Dhillon: Similarly, I can look at number of times. The context occurs.

289
00:35:56.990 --> 00:36:00.990
Inderjit Dhillon: The number of times the context occurs in the and

290
00:36:01.140 --> 00:36:05.300
Inderjit Dhillon: basically this then becomes this. So

291
00:36:06.490 --> 00:36:11.990
Inderjit Dhillon: number of times Wc. Occur together, divided by D. Right?

292
00:36:12.110 --> 00:36:14.980
Inderjit Dhillon: So it will. It will be.

293
00:36:15.330 --> 00:36:20.880
Inderjit Dhillon: You know this quantity is number of Wc.

294
00:36:21.020 --> 00:36:28.490
Inderjit Dhillon: Divided by D. This quantity is number of w, divided by D.

295
00:36:28.720 --> 00:36:31.450
Inderjit Dhillon: This value is number of

296
00:36:32.240 --> 00:36:37.010
Inderjit Dhillon: C, divided by D, and you'd see that you know. When you do this.

297
00:36:37.070 --> 00:36:40.470
Inderjit Dhillon: one of the D's will cancel, and this will go over here.

298
00:36:40.590 --> 00:36:46.430
Inderjit Dhillon: so that's why there's a upset value of D, which is the number of pairs in the

299
00:36:48.490 --> 00:37:01.200
Inderjit Dhillon: and then sometimes these. This number could be negative, and the people have found that it's actually better to use the positive part of this. So they'll take a max of this with 0, because there's a logarithm sitting over.

300
00:37:02.690 --> 00:37:06.660
Inderjit Dhillon: And so what you can do is you can assemble all this in a matrix.

301
00:37:06.850 --> 00:37:11.880
Inderjit Dhillon: which is an N, which is the number of words times D, which is a number of contexts.

302
00:37:12.360 --> 00:37:24.180
Inderjit Dhillon: Okay, so each row is the word. Each column is a context. Okay, and you can essentially get this. And then the question is, what do you do remember this is still a very, very large.

303
00:37:24.310 --> 00:37:33.480
Inderjit Dhillon: sparse matrix. Then you want to try and get a low rank. some dimensionality reduction you can do. And so one way to do it is

304
00:37:33.580 --> 00:37:39.620
Inderjit Dhillon: you can actually take this matrix, and you can compute its Svd: so this would correspond to.

305
00:37:39.700 --> 00:37:48.560
Inderjit Dhillon: and a U. Sigma half of the Svd. This will correspond to Sigma half times V in the Svd. And then so you can

306
00:37:48.660 --> 00:37:50.960
Inderjit Dhillon: compute the S. 3D right.

307
00:37:50.970 --> 00:37:51.910
Inderjit Dhillon: or

308
00:37:51.950 --> 00:37:59.880
Inderjit Dhillon: you can actually also compute kind of another. So so that's what.

309
00:38:00.590 --> 00:38:11.430
Inderjit Dhillon: Hmm. What what glove does. And and we we won't get into exactly how this is computed, because just computing the Svd. Of a very, very large sparse matrix, where W. Is quite large in terms of

310
00:38:11.550 --> 00:38:13.240
Inderjit Dhillon: the dimensions.

311
00:38:13.400 --> 00:38:20.690
Inderjit Dhillon: which means the number of singular vectors. It can be complicated, right? So you can actually compute this also, using

312
00:38:20.840 --> 00:38:25.830
Inderjit Dhillon: kind of the notion of, you know, using neural networks

313
00:38:25.980 --> 00:38:29.290
Inderjit Dhillon: in particular things like auto encoders. Okay?

314
00:38:29.430 --> 00:38:42.200
Inderjit Dhillon: And then you might explicitly want to kind of measure or capture the bias of the word or the number of times it appears, or the bias of the context. And instead of doing this.

315
00:38:42.310 --> 00:38:48.290
Inderjit Dhillon: you know, you can actually do this. Okay. And this is actually very similar, because, you know if you think about it.

316
00:38:48.590 --> 00:38:51.940
Inderjit Dhillon: This is W.

317
00:38:53.050 --> 00:38:56.300
Inderjit Dhillon: Vw. Times, E.

318
00:38:56.700 --> 00:38:57.940
Inderjit Dhillon: Times.

319
00:38:58.040 --> 00:39:03.320
Inderjit Dhillon: We transpose E: transpose Times, Vc. Transpose.

320
00:39:03.830 --> 00:39:06.150
Inderjit Dhillon: Okay. So you're basically trying to fix

321
00:39:06.220 --> 00:39:13.870
Inderjit Dhillon: the factor, so that you can have that interpretability that it's the bias that you're doing, and it's actually something similar that you can do

322
00:39:13.960 --> 00:39:24.720
Inderjit Dhillon: when you have, for example, your homework problems where you have. I think you don't even in homework, one that you have biased for a user in terms of their reviews. It

323
00:39:24.730 --> 00:39:41.500
Inderjit Dhillon: and bias for movies. So it's actually very similar to that. Okay? And then, once you solve this problem or empirically solve this problem, you can essentially use W. As the word embedding matrix, right? So if W. Is.

324
00:39:41.870 --> 00:39:44.570
Inderjit Dhillon: you know, let's say in

325
00:39:45.190 --> 00:39:53.720
Inderjit Dhillon: W. Is n divided by M times 64, you'll basically get 64 dimensional embedding of each word.

326
00:39:55.670 --> 00:39:57.630
Inderjit Dhillon: Then there is, so that's glove.

327
00:39:57.790 --> 00:40:11.220
Inderjit Dhillon: And then there is a word to back which, since which I actually uses a neural network for learning the word embedding. So they don't do the above kind of context vector idea.

328
00:40:11.300 --> 00:40:17.580
Inderjit Dhillon: but they use something similar, right? So i'll just repeat it for kind of complete this as to what they use.

329
00:40:17.660 --> 00:40:22.950
Inderjit Dhillon: so they use the idea of using something called sibo, which is

330
00:40:23.070 --> 00:40:25.430
Inderjit Dhillon: all continuous bag of words.

331
00:40:25.550 --> 00:40:33.010
Inderjit Dhillon: and then also something called Skipgrad. Okay, so let's see what they are right. They are basically become prediction problems.

332
00:40:33.340 --> 00:40:39.940
Inderjit Dhillon: I predict the target words given the neighbors and predict the neighbors given the target votes

333
00:40:40.750 --> 00:40:45.120
Inderjit Dhillon: so very similar to what we had before. Suppose you have a target word here.

334
00:40:45.310 --> 00:40:55.400
Inderjit Dhillon: and then you look at the context on the left and the right. Here we are looking at just the context window of one right. You can use that to identify the context

335
00:40:55.690 --> 00:41:06.940
Inderjit Dhillon: and then give for this word. Wt. I can say, given this context Here they are using a context of size 2 before and 2 after.

336
00:41:07.070 --> 00:41:18.480
Inderjit Dhillon: Think of these as inputs. And i'm going to use the average some weighted average of the input embeddings

337
00:41:18.970 --> 00:41:20.920
Inderjit Dhillon: to get Wt

338
00:41:22.000 --> 00:41:31.630
Inderjit Dhillon: okay, and you can put even a non-linearity over here. and then you can do some the opposite of it. Which is you and Wt.

339
00:41:31.850 --> 00:41:35.360
Inderjit Dhillon: Can I now produce, like the

340
00:41:35.470 --> 00:41:39.620
Inderjit Dhillon: neighbors using the target work?

341
00:41:40.020 --> 00:41:42.940
Inderjit Dhillon: Okay. So

342
00:41:44.320 --> 00:41:48.570
Inderjit Dhillon: I want to spend too much time over it. But in some sense

343
00:41:48.660 --> 00:41:55.000
Inderjit Dhillon: you are trying to learn the probability of Wt. Plus J. Given Wt: where

344
00:41:55.170 --> 00:42:01.940
Inderjit Dhillon: J is like a future word in this context. So you can see that this idea

345
00:42:02.020 --> 00:42:11.080
Inderjit Dhillon: is like kind of predicting the next word or jet, or down the road is, is She's been around for for a while. Okay.

346
00:42:11.300 --> 00:42:25.730
Inderjit Dhillon: and I I won't. Go into the details of this, but you can think of. you know, probability of output given the particular context is, you can model as a as a soft Max.

347
00:42:26.630 --> 00:42:35.270
Inderjit Dhillon: the really kind of neat thing about this Once this word to work. Paper came out, and people started using these embeddings is

348
00:42:35.290 --> 00:42:41.680
Inderjit Dhillon: that you know the working and the word Queen would have, you know. Let's say 64 dimensional embeddings.

349
00:42:41.770 --> 00:42:57.040
Inderjit Dhillon: And what they found is that, you know, once they did these embeddings they found relationships such as you know. King is to queen, just as man as to woman. and the way they kind of also looked at it is that if you take the

350
00:42:57.230 --> 00:42:59.860
Inderjit Dhillon: vector queen minus king.

351
00:43:00.170 --> 00:43:04.500
Inderjit Dhillon: then it's very close to the vector woman minus man.

352
00:43:04.920 --> 00:43:08.500
Inderjit Dhillon: Similarly, you know, when we have warped ends, people found that

353
00:43:08.730 --> 00:43:15.690
Inderjit Dhillon: you know there'll be a relationship. Walking is to walk as swimming is to swam, so the past tense.

354
00:43:15.920 --> 00:43:19.010
Inderjit Dhillon: and then, similarly other kinds of relationships.

355
00:43:19.030 --> 00:43:27.410
Inderjit Dhillon: Spain and Madrid, Italy, and Rome country and its capital, Turkey, Ankara, and so on.

356
00:43:27.650 --> 00:43:28.250
Inderjit Dhillon: Okay.

357
00:43:28.700 --> 00:43:40.020
Inderjit Dhillon: So this actually did create quite a a lot of impact when it when it came out in 2,014 and a lot of people were using it, for you know their neural network applications.

358
00:43:43.270 --> 00:43:54.890
Inderjit Dhillon: But now let's move a little bit towards kind of the modern setting, which is, you know, come closer to what's being done today with transforms.

359
00:43:55.590 --> 00:43:59.860
Inderjit Dhillon: So now suppose I have

360
00:44:00.150 --> 00:44:09.560
Inderjit Dhillon: the World Bank? Well. it can appear in a sentence that has open a bank account versus on the river bank.

361
00:44:11.470 --> 00:44:14.100
Inderjit Dhillon: Yeah. So the embedding of the World Bank

362
00:44:14.280 --> 00:44:29.140
Inderjit Dhillon: it's actually can we? All the meaning of the World Bank can be quite different. So why should the embedding be identical in both cases? So all these previous methods were trying to get one embedding for a token.

363
00:44:29.340 --> 00:44:34.800
Inderjit Dhillon: But here, you know, the the the question is, why not have like a contextual

364
00:44:35.110 --> 00:44:36.730
Inderjit Dhillon: world representation?

365
00:44:38.070 --> 00:44:38.980
Inderjit Dhillon: Okay.

366
00:44:40.080 --> 00:44:47.760
Inderjit Dhillon: So to do that. what we'll do is train a model to extract the contextual representation on a text

367
00:44:47.770 --> 00:44:49.850
Inderjit Dhillon: Corpus. Right? So

368
00:44:49.990 --> 00:45:06.770
Inderjit Dhillon: the idea is that you know you'll have sentences like the the movie was terribly exciting, and you would learn the representation of, for example, terribly in the context that it was present in. You know the movie walls terribly exciting it, although that sentence

369
00:45:07.320 --> 00:45:13.220
Inderjit Dhillon: So one approach, which was done in 2,017, was

370
00:45:13.420 --> 00:45:22.280
Inderjit Dhillon: to train a standard kind of neural network translation model. Let's say we Lsdm: so.

371
00:45:22.580 --> 00:45:29.140
Inderjit Dhillon: and then take the encoder kind of output directly, as the contextualized word embeddings.

372
00:45:29.860 --> 00:45:35.290
Inderjit Dhillon: We we won't go into detail because kind of that's not how it's done right. Now.

373
00:45:36.230 --> 00:45:41.330
Inderjit Dhillon: Now let's go on to the really really important concept over here.

374
00:45:43.310 --> 00:45:52.890
Inderjit Dhillon: which is kind of using doing pre-training or self-supervised learning, using a language model. So the language model pre training task.

375
00:45:54.020 --> 00:46:01.270
Inderjit Dhillon: So suppose I define my task to be, predict the next word given the prefix

376
00:46:02.280 --> 00:46:04.750
Inderjit Dhillon: Well, does it need any label.

377
00:46:06.660 --> 00:46:13.660
Inderjit Dhillon: There is no labeling need to be done while I have a sentence. and from that sentence I can actually create

378
00:46:14.030 --> 00:46:20.130
Inderjit Dhillon: these supervision tasks, which is predict the next word given the prefix.

379
00:46:20.320 --> 00:46:25.410
Inderjit Dhillon: So, for example, if you have a sentence. let's stick to improvisation.

380
00:46:26.000 --> 00:46:29.760
Inderjit Dhillon: Well, I can create a supervised task out of it

381
00:46:30.500 --> 00:46:37.570
Inderjit Dhillon: that predicts the word improvisation. Given that the first 3 words are, let's stick to.

382
00:46:38.320 --> 00:46:46.260
Inderjit Dhillon: So that is the very important notion of self-supervision that you've taken like unlabeled data.

383
00:46:46.680 --> 00:46:53.210
Inderjit Dhillon: and you have created now super wise tasks from it. Okay. So, for example, over here

384
00:46:53.400 --> 00:47:00.120
Inderjit Dhillon: we are looking at, You know the architecture is still like an Lstm. A Multi layer. Lsdm.

385
00:47:00.420 --> 00:47:03.840
Inderjit Dhillon: But you know you can create this task.

386
00:47:03.970 --> 00:47:15.850
Inderjit Dhillon: and then you can learn the parameters of the Lsdm by processing over millions and billions and trillions actually, these days

387
00:47:15.940 --> 00:47:20.480
Inderjit Dhillon: of tokens or sentences. Okay.

388
00:47:22.190 --> 00:47:35.650
Inderjit Dhillon: And one of the kind of important papers that appeared historically was Elmo. It's not really used much now, because they were working with Lsdms.

389
00:47:35.790 --> 00:47:38.930
Inderjit Dhillon: but they actually have this idea, which is

390
00:47:39.250 --> 00:47:44.420
Inderjit Dhillon: to do not just a forward Lsdm: the way you think of.

391
00:47:44.660 --> 00:47:54.140
Inderjit Dhillon: you know, predicting the next word, but also like a backward Lsdm. Like backward language model that starts kind of at the end of the sentence

392
00:47:54.320 --> 00:47:55.950
Inderjit Dhillon: and proceeds backward.

393
00:47:56.380 --> 00:48:09.710
Inderjit Dhillon: So what Elmore did was they trained a forward and backward Lsd. A model on large purposes. and then use the hidden states of each token right to remember that the hidden state

394
00:48:09.820 --> 00:48:14.320
Inderjit Dhillon: captures not just this part. but it's contextual part.

395
00:48:15.640 --> 00:48:22.630
Inderjit Dhillon: So that's how you get the contextual world embedding right? So use the hidden States for it token

396
00:48:22.720 --> 00:48:26.060
Inderjit Dhillon: to compute a vector representation of each word.

397
00:48:26.360 --> 00:48:40.630
Inderjit Dhillon: and then you replace the word embedding by. So you know you may start with some word embedding, but then you replace it by Elmo's embedding. Elmo is a particular model.

398
00:48:42.350 --> 00:48:46.780
Inderjit Dhillon: and and and in that timeframe it actually

399
00:48:50.680 --> 00:49:05.850
Inderjit Dhillon: so. So one of the reasons this is this is great is because you can now think of it as a that. You can actually apply it to many many different problems using these multiple

400
00:49:06.010 --> 00:49:22.340
Inderjit Dhillon: feature representations off. Let's say a sentence or a word in a sentence right? And so it ended up giving. Let's see, you know, the previous based lines were here, and Elmo gave these results right? So

401
00:49:22.530 --> 00:49:38.180
Inderjit Dhillon: 24% improvement, 5% improvement 17.2. And These are kind of different tasks. Right? I think the squad is question answering. and alli, I think, is natural language, inference, and so on. Any art is named entity, recognition. And so

402
00:49:40.510 --> 00:49:55.590
Inderjit Dhillon: then we come to the very, very next big thing which is after the transformer model transform, like I said, in the initial part it was applied to on machine translation, neural machine translation.

403
00:49:55.790 --> 00:50:00.840
Inderjit Dhillon: And then it was applied to language, modeling, or natural language process.

404
00:50:01.270 --> 00:50:06.540
Inderjit Dhillon: Okay. tasks which are more general than just your neural machine translation.

405
00:50:07.200 --> 00:50:10.300
Inderjit Dhillon: So the key idea there to 2 things

406
00:50:10.490 --> 00:50:13.840
Inderjit Dhillon: was to actually replace the Lsdm by a transformer

407
00:50:15.010 --> 00:50:22.800
Inderjit Dhillon: and then use the bi-directional transform right so board stands for.

408
00:50:22.900 --> 00:50:32.690
Inderjit Dhillon: Let's see if I can remember it. The bi-directional encoder representations using transforms okay

409
00:50:34.040 --> 00:50:43.060
Inderjit Dhillon: Now when they do bi-directional, then one of the questions: comes. What is the kind of pre training or the self-supervised task that comes out?

410
00:50:43.090 --> 00:50:43.790
Inderjit Dhillon: Okay.

411
00:50:44.170 --> 00:50:51.700
Inderjit Dhillon: And what they did was they use something called a masked language model. So what they would do is they would hide a word

412
00:50:52.750 --> 00:50:56.930
Inderjit Dhillon: in the input and then ask the model to predict it.

413
00:50:58.290 --> 00:51:02.850
Inderjit Dhillon: and they do it for 15 of the tokens that are present.

414
00:51:03.820 --> 00:51:11.970
Inderjit Dhillon: Okay, so that was one of the mass language modeling tasks. That was the mass language modeling task, and then they had another pre training task.

415
00:51:12.740 --> 00:51:17.840
Inderjit Dhillon: which is, they would take pairs of sentences that are contiguously in text.

416
00:51:18.080 --> 00:51:29.770
Inderjit Dhillon: and they would ask the model to predict which one follows the other. Okay, so obviously you can have you know Again, this is self-supervised learning because you create

417
00:51:29.850 --> 00:51:31.670
Inderjit Dhillon: do you supervise tasks.

418
00:51:31.940 --> 00:51:41.910
Inderjit Dhillon: and then you can train them. So again, the big difference between, let's say, Elmo, which essentially produces embeddings

419
00:51:43.960 --> 00:51:47.550
Inderjit Dhillon: and bird, and this is a recipe that is now.

420
00:51:47.700 --> 00:51:50.510
Inderjit Dhillon: you know, pretty much widely adopted now

421
00:51:50.760 --> 00:51:54.420
Inderjit Dhillon: is that you fine tune

422
00:51:55.110 --> 00:51:59.880
Inderjit Dhillon: both the word weight. So the bird is a transformer model.

423
00:52:00.570 --> 00:52:04.650
Inderjit Dhillon: and so it has weights associated with it right?

424
00:52:05.580 --> 00:52:11.600
Inderjit Dhillon: Typically, and we'll come to this. There's a task dependent model that you have.

425
00:52:12.010 --> 00:52:16.550
Inderjit Dhillon: So typically it is like one additional layer.

426
00:52:18.120 --> 00:52:27.920
Inderjit Dhillon: But the recipe typically is that you, you know. Given the particular task you change potentially all the weights in the board transformer

427
00:52:28.020 --> 00:52:34.820
Inderjit Dhillon: and the model. and this is called fine tuning. You're given a base model, and then you find you

428
00:52:35.860 --> 00:52:46.160
Inderjit Dhillon: okay. So I've kind of said a lot. So now kind of let's revisit what I said. So the masked language model is, you predict

429
00:52:47.070 --> 00:52:51.170
Inderjit Dhillon: some set of words by the rest of the sentence.

430
00:52:51.420 --> 00:53:02.870
Inderjit Dhillon: Okay, the mass language. What we do predicted for each of the world. They do it for 15%. So, for example, over here. hopefully, you can see it. You can see that this is

431
00:53:03.060 --> 00:53:15.520
Inderjit Dhillon: W. 4 is a vector all certain dimensions it's replaced by vector that represents mask. That means it's actually not being shown to the model.

432
00:53:16.460 --> 00:53:28.030
Inderjit Dhillon: but it knows the representations or embeddings of these surrounding context words. Remember, this is the we talked about. The transformer encoder. The transformer in order.

433
00:53:28.240 --> 00:53:32.640
Inderjit Dhillon: you know, transforms W. One to o one right?

434
00:53:32.690 --> 00:53:40.450
Inderjit Dhillon: And of course there can be many layers like this. And then, if this is kind of the output of all the layers of the transformer.

435
00:53:40.610 --> 00:53:47.760
Inderjit Dhillon: then you may have, you know. Let's say you are trying to do a classification task, predict whether an email is spam or not.

436
00:53:47.960 --> 00:53:57.020
Inderjit Dhillon: Then you have a fully connected layer. plus, you know, some activations, and so on. And the task that you use over here

437
00:53:57.210 --> 00:54:00.900
Inderjit Dhillon: right this is, you know. Embedding 2

438
00:54:01.060 --> 00:54:08.140
Inderjit Dhillon: woke up the softmax right and over here is what you are trying to predict that you must.

439
00:54:08.290 --> 00:54:17.330
Inderjit Dhillon: So that becomes the super wise tasks. But the supervised task is created from unsupervised data. So this is called

440
00:54:17.500 --> 00:54:20.520
Inderjit Dhillon: kind of self supervision, and this is the master language model.

441
00:54:22.980 --> 00:54:28.490
Inderjit Dhillon: So we talked a little bit about bird fine tuning, so

442
00:54:28.840 --> 00:54:39.270
Inderjit Dhillon: the tasks could be very different, right? So they could be a classification task. They could be an entailment task. There's something asking whether

443
00:54:39.810 --> 00:54:43.560
Inderjit Dhillon: you know which of the texts are similar to each other.

444
00:54:44.160 --> 00:54:49.740
Inderjit Dhillon: You can have a multiple choice kind of answer. Where you have.

445
00:54:50.700 --> 00:54:54.940
Inderjit Dhillon: You know this is a start token to the Delimiter token.

446
00:54:55.290 --> 00:55:02.650
Inderjit Dhillon: there's context, there's answer one. And then you kind of extract the let's say the last token or a meeting.

447
00:55:02.990 --> 00:55:10.370
Inderjit Dhillon: And sorry this is this is the the input. you put it to a transformer.

448
00:55:10.630 --> 00:55:21.330
Inderjit Dhillon: and then you have to do something which is specialized to the task. Right? So here you might have a let's say, a linear predictor, for on top of the transformer.

449
00:55:21.460 --> 00:55:25.220
Inderjit Dhillon: or whether you know the first choice is correct or not.

450
00:55:25.510 --> 00:55:35.220
Inderjit Dhillon: And so, if it is a 3 multiple choice question. Then. you know, this is. this will need to be trained.

451
00:55:35.730 --> 00:55:41.450
Inderjit Dhillon: and the entire transformer needs weights might change, and that's what's called fine.

452
00:55:42.040 --> 00:55:44.390
Inderjit Dhillon: Yeah. So, for example, over here.

453
00:55:44.730 --> 00:55:55.300
Inderjit Dhillon: you, if you're trying to do classification, you will not only learn the linear, layer, but you'll actually change the weight of the transformer, and that's called finding

454
00:55:59.390 --> 00:56:02.330
Inderjit Dhillon: Okay. And then

455
00:56:02.380 --> 00:56:06.510
Inderjit Dhillon: so birth base that model

456
00:56:06.760 --> 00:56:10.650
Inderjit Dhillon: was about 110 millionparameters.

457
00:56:11.330 --> 00:56:20.350
Inderjit Dhillon: and birth large. was a model which was 340 millionparameters. Okay, now, just keep this in mind that

458
00:56:21.080 --> 00:56:37.690
Inderjit Dhillon: for its time for 2,018. This was a pretty large. very large model. Okay, 340 millionparameters. Oh, you might have heard of more, you know, like you might have heard of Gpd. 3 and was.

459
00:56:37.990 --> 00:56:40.500
Inderjit Dhillon: Let's see. Gp. D. 3. Was

460
00:56:41.700 --> 00:56:45.670
Inderjit Dhillon: anybody know how much? How many Gpu 3 is. or walls.

461
00:56:49.240 --> 00:56:50.370
Inderjit Dhillon: and we all wait

462
00:56:52.790 --> 00:56:53.600
Inderjit Dhillon: anybody.

463
00:56:58.680 --> 00:57:09.300
Inderjit Dhillon: When did this appear? Gp: 3 paper do you 175 billionIs that 175 billionparameters.

464
00:57:13.400 --> 00:57:15.530
Inderjit Dhillon: Okay, and board large

465
00:57:20.610 --> 00:57:25.500
Inderjit Dhillon: is 340,340 million

466
00:57:26.550 --> 00:57:28.360
Inderjit Dhillon: right? So this is

467
00:57:28.970 --> 00:57:33.610
Inderjit Dhillon: 175 times 10 to the power 9. This is

468
00:57:33.910 --> 00:57:50.090
Inderjit Dhillon: 340 times 10 to the power. 6. Yeah. So you can see that it is about. you know. 500 times bigger, right? 175 times 10 to the point 9 divided by 3, 40

469
00:57:50.340 --> 00:57:53.390
Inderjit Dhillon: times 10 for 6. This is about

470
00:57:55.570 --> 00:58:01.370
Inderjit Dhillon: one by 2 times 10.3, so about 500 times bigger.

471
00:58:08.410 --> 00:58:11.150
Inderjit Dhillon: Okay. what about G.

472
00:58:12.500 --> 00:58:13.710
Inderjit Dhillon: How big is that

473
00:58:18.260 --> 00:58:19.100
Inderjit Dhillon: anybody

474
00:58:23.230 --> 00:58:30.710
Inderjit Dhillon: was released? I think. What during while we have I've been teaching right? So things are happening quite fast.

475
00:58:31.190 --> 00:58:36.910
Inderjit Dhillon: Right? It was released. What? 2 3 weeks ago. I guess I think it's a trillion, right?

476
00:58:38.870 --> 00:58:45.080
Inderjit Dhillon: They actually Haven't sent. Okay, so there. Now that there is kind of

477
00:58:46.540 --> 00:58:56.090
Inderjit Dhillon: all these commercial applications people are starting to become a little bit more secretive, and they actually have not revealed the training data

478
00:58:56.450 --> 00:59:05.160
Inderjit Dhillon: size also, and not also the size of the model. Right? I think. I think if you look at the review.

479
00:59:07.140 --> 00:59:16.530
Inderjit Dhillon: sorry if you look at the video. I was actually watching yesterday the video with the Sam Altman. Same person interviewing him

480
00:59:18.590 --> 00:59:25.850
Inderjit Dhillon: Statement: No. Sam Altman Wasn't divided by the same person. And I think.

481
00:59:26.320 --> 00:59:28.910
Inderjit Dhillon: let me just this.

482
00:59:30.530 --> 00:59:33.470
Nilesh Gupta: Okay. Sorry, Lex Friedman

483
00:59:33.480 --> 00:59:45.210
Inderjit Dhillon: right? He was be interviewed and apparently like treatment, said somewhere at that Gpd. For is 100 trillion parameters. But I don't think that's correct. Okay.

484
00:59:45.480 --> 00:59:58.270
Inderjit Dhillon: But but anyway, so you know, we don't know exactly how it is, how big it is. It could be a trillion. Parameters, maybe different. But we know, for example, that you know, for example, the palm model

485
00:59:58.750 --> 01:00:03.400
Inderjit Dhillon: that is out of Google is what 540 billionparameters.

486
01:00:07.270 --> 01:00:10.280
Inderjit Dhillon: Okay, so you can see that this is now.

487
01:00:10.350 --> 01:00:13.330
Inderjit Dhillon: you know. 1,000 times bigger.

488
01:00:13.810 --> 01:00:16.930
Inderjit Dhillon: Maybe you know 1 1.5,

489
01:00:17.070 --> 01:00:23.700
Inderjit Dhillon: 15 100 times 1,500 times bigger then, bird lot.

490
01:00:24.560 --> 01:00:33.520
Inderjit Dhillon: So things move quickly, right. I mean Bert Large was a pretty big language model, so you know we'll see.

491
01:00:33.980 --> 01:00:38.250
Inderjit Dhillon: You know you. You have heard the term Llm. For a large language model.

492
01:00:38.660 --> 01:00:53.460
Inderjit Dhillon: I think board is not really classified as a large language mortal now right, because it's not as it's not as large, right, maybe, and the definition of L. The question is whether it'll keep on changing. But at some point of time it'll saturate right.

493
01:00:53.560 --> 01:00:58.190
Inderjit Dhillon: And then there's also the question of how much training data is used to

494
01:00:58.290 --> 01:01:06.640
Inderjit Dhillon: train these models right? Because the larger the model is. You kind of say that, hey? You know you need more and more training data to train.

495
01:01:07.950 --> 01:01:12.740
Inderjit Dhillon: Okay. So if you look at this.

496
01:01:13.010 --> 01:01:19.070
Inderjit Dhillon: I guess Gp: 3 was also released in what 2,017 or 18. Is that correct?

497
01:01:22.500 --> 01:01:27.500
Nilesh Gupta: I think that's 2020. Okay, what is this? Then?

498
01:01:28.120 --> 01:01:29.520
Inderjit Dhillon: Pretty open?

499
01:01:31.080 --> 01:01:35.340
Inderjit Dhillon: Yeah. I still opening a state of the art.

500
01:01:35.630 --> 01:01:37.830
Inderjit Dhillon: This might be just Gpt.

501
01:01:39.510 --> 01:01:48.480
Inderjit Dhillon: Oh, I see, because there were different versions of Gpt. Right? You could do one and so on. Yeah, okay. So it was an ordinary version of Gpt: okay, thanks. I think that makes a lot of sense.

502
01:01:48.600 --> 01:01:57.710
Nilesh Gupta: Actually, I think they have another.

503
01:01:58.270 --> 01:02:13.020
Chitrank: What do you mean? Before? Jpd: They were like other pretend models, if i'm not wrong, one was like purely multi-layered lsdm based I don't remember its name. It was released in 2,016 by more. This is this one right? The Elmo model

504
01:02:18.990 --> 01:02:22.780
Chitrank: by the time i'm not sure they some you need Lm: as well.

505
01:02:23.380 --> 01:02:25.190
Chitrank: Yeah, okay.

506
01:02:25.520 --> 01:02:27.360
Chitrank: i'm. Not too sure about it. But

507
01:02:28.640 --> 01:02:33.610
Inderjit Dhillon: yeah, I mean. And what's happened? Post birth is that all the language

508
01:02:33.940 --> 01:02:37.730
models are now based on transformers.

509
01:02:39.600 --> 01:02:46.150
Inderjit Dhillon: Previously they were, you know, some of them were based on Lsdms.

510
01:02:46.320 --> 01:02:58.490
Inderjit Dhillon: and so you could see over here that you know this is some natural language Inference tasks to see whether you remember the entailment task, which is whether you know you give it

511
01:02:58.720 --> 01:03:07.650
Inderjit Dhillon: a hypothesis. It's actually become very important now, right because Llm's or large language models can can

512
01:03:09.540 --> 01:03:19.920
Inderjit Dhillon: and really hallucinate, can. They can make up stuff right? I think. The other day, on Chat Gpd, I was just trying find something a nice misspelled.

513
01:03:20.520 --> 01:03:32.360
Inderjit Dhillon: I was asking about. you know, dense, symmetric matrices and I misspelled. So instead of instead of typing this.

514
01:03:36.460 --> 01:03:42.000
Inderjit Dhillon: I was just trying to see what his knowledge was. And then I I ended up doing this dent

515
01:03:42.620 --> 01:03:46.290
Inderjit Dhillon: symmetric matrices.

516
01:03:48.340 --> 01:03:52.550
Inderjit Dhillon: and it went into this hallucination, where it actually made up

517
01:03:52.880 --> 01:04:04.390
Inderjit Dhillon: these things, called, apparently called dent symmetric matrices. and how they come up in various applications. But i'm pretty sure that there is no such thing as a dent symmetric matrix.

518
01:04:04.440 --> 01:04:05.110
Inderjit Dhillon: Okay.

519
01:04:05.380 --> 01:04:14.750
Inderjit Dhillon: So so. So you know, like if a if a language model outputs a hypoth or something, then you can think of it as a hypothesis.

520
01:04:14.900 --> 01:04:19.080
Inderjit Dhillon: And then, if you have a text where you think that there is a premise over here.

521
01:04:19.320 --> 01:04:24.390
Inderjit Dhillon: You know. How do you kind of say that the hypothesis is actually supported by

522
01:04:24.480 --> 01:04:38.040
Inderjit Dhillon: the premise. Right? So you can actually use a model to to to to check that. Assuming that you have a piece of text with hypotheses, right? So that's what is being done, which is kind of trying to add citations

523
01:04:38.280 --> 01:04:41.250
Inderjit Dhillon: to to generate the text.

524
01:04:41.600 --> 01:04:47.360
Inderjit Dhillon: So you can see that you know the state of the art was really advanced by

525
01:04:47.700 --> 01:04:49.570
Inderjit Dhillon: by board.

526
01:04:50.500 --> 01:04:54.410
Inderjit Dhillon: and it became this paradigm right? Which is that you

527
01:04:55.000 --> 01:05:00.100
Inderjit Dhillon: pre-trained some general representation, create this very very large model.

528
01:05:00.910 --> 01:05:13.550
Inderjit Dhillon: Okay? And then at least in the board era. You can then do fine tuning and fine tuning. It does entail a fair amount of work in the sense that

529
01:05:13.770 --> 01:05:22.790
Inderjit Dhillon: you have a task, and you are going to change all the weights. Okay, so that means that if you want to use this particular model. Then, for imprints.

530
01:05:22.810 --> 01:05:33.090
Inderjit Dhillon: you have to have a change to model. So if you have K. Tasks, let's say 20 tasks that you want to use the model for

531
01:05:33.260 --> 01:05:43.000
Inderjit Dhillon: then you will need 20 models, each of which is fine tuned from. But but the nice thing is that you can actually take this

532
01:05:43.850 --> 01:05:49.950
Inderjit Dhillon: base model or the foundation model. and then you can find Okay.

533
01:05:50.060 --> 01:05:55.830
Inderjit Dhillon: Now, a lot has happened since the board paper, and now

534
01:05:55.870 --> 01:06:06.820
Inderjit Dhillon: these language models have these things called oh, prompts. And so, instead of fine-tuning, one other paradigm that is emerged is

535
01:06:06.940 --> 01:06:11.580
Inderjit Dhillon: to give to figure out what prompt to give to a language model.

536
01:06:11.590 --> 01:06:14.800
Inderjit Dhillon: so that they can give you kind of desired results.

537
01:06:16.110 --> 01:06:22.650
Inderjit Dhillon: Right? So that's kind of a different paradigm. Where? When these models become very large.

538
01:06:22.810 --> 01:06:27.840
Inderjit Dhillon: right? So that is really definitely in the kind of Gp. D. 3.

539
01:06:28.150 --> 01:06:31.600
Inderjit Dhillon: And you believe, for well.

540
01:06:31.840 --> 01:06:43.180
Inderjit Dhillon: where, when they get to size big enough, you can actually give them different prompts and still get something still get pretty good output. So in that case you actually don't need to

541
01:06:43.420 --> 01:06:50.060
Inderjit Dhillon: find unit. So what that means is you're not actually changing the weights of the particular model.

542
01:06:50.390 --> 01:06:58.670
Inderjit Dhillon: So and I think there is a there is a a talk, I think, on prompt engineering

543
01:06:58.720 --> 01:07:03.370
Inderjit Dhillon: given by one of the teams among you.

544
01:07:04.740 --> 01:07:11.280
Inderjit Dhillon: So with that, you know, I've come kind of to the conclusion of today's lecture

545
01:07:11.520 --> 01:07:13.190
Inderjit Dhillon: questions.

546
01:07:21.000 --> 01:07:24.710
Inderjit Dhillon: There is going to be a homework coming up

547
01:07:25.120 --> 01:07:30.140
Inderjit Dhillon: hopefully. It will be ready today, and it will actually be

548
01:07:30.310 --> 01:07:35.340
Inderjit Dhillon: something where you can, you know, use transformers.

549
01:07:41.940 --> 01:07:43.960
Inderjit Dhillon: Okay, any questions.

550
01:07:50.110 --> 01:07:53.370
Inderjit Dhillon: Okay, let me stop sharing.

551
01:07:55.150 --> 01:08:11.950
Inderjit Dhillon: Okay. So now we will actually move on to kind of the. you know, almost like the final phase of this class. So far, you know, I've been mostly lecturing. Occasionally Malaysia stepped into lecture when I've been away.

552
01:08:12.620 --> 01:08:26.490
Inderjit Dhillon: but now we'll move on to a different phase where you won't have to listen just to me, hopefully. It'll be much more exciting, and we have, like a pretty interesting set of

553
01:08:26.660 --> 01:08:30.100
Inderjit Dhillon: talks that are coming up.

554
01:08:30.770 --> 01:08:43.609
Inderjit Dhillon: You all are presenting those talks, and we will learn more about our deep learning through those talks. And then, of course, you have your project work coming along, so

555
01:08:44.270 --> 01:08:49.979
Inderjit Dhillon: we'll figure out hopefully, that is coming along. Okay.

556
01:08:52.050 --> 01:08:54.810
Inderjit Dhillon: So do you have any questions about either of these things.

557
01:08:56.800 --> 01:09:00.130
Chitrank: So what has to be the format for the presentation?

558
01:09:01.130 --> 01:09:07.470
Inderjit Dhillon: Yeah. So with English. I was. I was talking about giving you all like a

559
01:09:07.750 --> 01:09:14.779
Inderjit Dhillon: template. I don't know if malaysia's gotten around to doing it. Actually hasn't been well for the past few days, so

560
01:09:15.950 --> 01:09:17.090
Inderjit Dhillon: oh.

561
01:09:17.680 --> 01:09:27.390
Inderjit Dhillon: but we don't want something very elaborate, right it should be just a a few slides, maybe 4, 5 slides, if you want to add

562
01:09:27.410 --> 01:09:30.420
Inderjit Dhillon: more. That's okay.

563
01:09:30.680 --> 01:09:34.370
Inderjit Dhillon: But you can kind of state the

564
01:09:34.620 --> 01:09:47.240
Inderjit Dhillon: problem area goal in the first line. What you're talking about. Go in depth into the approach or the material in the next 2 to 3 slides.

565
01:09:47.420 --> 01:09:48.800
Inderjit Dhillon: and then conflict.

566
01:09:49.600 --> 01:09:54.040
Chitrank: Oh, I was talking about the the presentations that, like these

567
01:09:54.530 --> 01:10:02.750
Chitrank: I the the topic. Oh, yeah, the class presentations, right? That's what i'm talking about. Okay.

568
01:10:03.310 --> 01:10:15.980
Chitrank: And so ner said that the the slot will be of like 10min long. So you get that you have to present for like 5, 6min and keep the rest. 4min of questions.

569
01:10:16.140 --> 01:10:23.150
Inderjit Dhillon: Yeah, yeah. And that's Why, i'm telling you not to make too many slides. Okay, Got it? Yeah, I do. Because I mean, I don't want you to have like

570
01:10:23.710 --> 01:10:37.480
Inderjit Dhillon: 15 slides, and then you will not be able to go through those slides. So try, and you know you don't have to be too kind of complete or so. And then for the presentations it's okay. If you use other mediums right, you don't have to necessarily create slides if it's just.

571
01:10:37.670 --> 01:10:44.220
Inderjit Dhillon: you know a document or so we are pretty flexible in what you do. We will collect all the artifacts that you.

572
01:10:44.380 --> 01:10:54.510
Inderjit Dhillon: you, you have the presentation of whichever style you do it, so that we can put it on the canvas website.

573
01:10:54.550 --> 01:10:59.900
Inderjit Dhillon: and then we will again. The same thing like we'll make the recordings available. I mean, really

574
01:11:01.010 --> 01:11:02.570
Inderjit Dhillon: what what what

575
01:11:02.770 --> 01:11:13.120
Inderjit Dhillon: I've tried to do in this class is trying to give you kind of like a a reasonably decent mathematical foundation to the different topics.

576
01:11:13.310 --> 01:11:19.600
Inderjit Dhillon: even when we have come to transformers and seen you've seen the number of like matrices, and so on, that come up.

577
01:11:19.660 --> 01:11:20.580
Chitrank: My.

578
01:11:20.750 --> 01:11:32.460
Inderjit Dhillon: And now we are basically. Saying, okay, let's now move to talk about the current state of the art. But we won't go in so much depth as we went in. When we were talking the foundational material.

579
01:11:32.620 --> 01:11:40.160
Inderjit Dhillon: But the hope is that since you've learned that foundational material, you can always go back to it if you need to understand something in the current papers

580
01:11:41.930 --> 01:11:43.730
makes sense

581
01:11:44.320 --> 01:11:52.150
Nilesh Gupta: so just one clarification for the presentation class presentation. I think we have enough time to have 8 to 10min, so

582
01:11:52.350 --> 01:11:54.630
Nilesh Gupta: actual presentation. And then 5min of

583
01:11:54.740 --> 01:11:57.420
Nilesh Gupta: discussion today, because, like you are only doing 5.

584
01:11:57.460 --> 01:11:58.980
Nilesh Gupta: Thanks. And what cool?

585
01:11:59.170 --> 01:12:02.080
Inderjit Dhillon: Okay, Good. Thank you for reminding me. So we are doing.

586
01:12:02.890 --> 01:12:15.820
Inderjit Dhillon: We're doing kind of 5 presentations per class. So you know, it comes out to be 15min per presentation. You know there's always a little bit of like, you know, a minute or so that kind of gets wasted sometimes

587
01:12:15.910 --> 01:12:23.260
Inderjit Dhillon: going from one presentation to the other. But you can have like me. They said, about, you know, 10 to 12min for presenting the material.

588
01:12:26.710 --> 01:12:31.710
Inderjit Dhillon: and then it's up to you. I'll leave it up to you. I don't want you to. you know.

589
01:12:32.050 --> 01:12:38.720
Inderjit Dhillon: I just want you to convey, You know, the most important ideas right to everybody else.

590
01:12:38.990 --> 01:12:42.010
Inderjit Dhillon: Remember, this is not.

591
01:12:42.220 --> 01:12:49.090
Inderjit Dhillon: This is not designed to kind of grade you or to judge you right. The main purpose of this is so that

592
01:12:49.260 --> 01:12:55.970
Inderjit Dhillon: you can kind of read some material independently, and be able to present it, and the value to everybody else is

593
01:12:56.000 --> 01:13:01.670
Inderjit Dhillon: that you know they get exposed to some of the latest in the in the literature. So what

594
01:13:01.850 --> 01:13:15.470
Inderjit Dhillon: my hope is that this will help you to learn. Learn how to read a paper, learn, or or material, learn how to present. understand these things, and then through the homeworks and the projects you are learning to.

595
01:13:15.480 --> 01:13:21.360
Inderjit Dhillon: Actually, you know, mostly implement them and see the results for yourself, because that's how

596
01:13:21.390 --> 01:13:28.390
Inderjit Dhillon: You know, modern machine learning jobs are not only in industry but also research and academia.

597
01:13:30.970 --> 01:13:33.560
Inderjit Dhillon: Okay, any other questions.

598
01:13:37.230 --> 01:13:48.750
Inderjit Dhillon: Okay, awesome. So like, I said, next time you'll hear less from me and more from your colleagues. So let's that's so.

599
01:13:49.220 --> 01:14:02.130
Inderjit Dhillon: Look forward to the presentation. So I want everybody to be encouraging towards the people who present. and and you know, in advance. Thank you all for

600
01:14:02.190 --> 01:14:10.660
Inderjit Dhillon: doing the presentation. I really looking forward to them. Okay, thank you so much, and I will see you all on

601
01:14:11.090 --> 01:14:13.220
Inderjit Dhillon: Monday right.

